Direction fa to en
2023-05-09 02:51:33 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]
                     [--log-format {json,none,simple,tqdm}]
                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]
                     [--aim-run-hash AIM_RUN_HASH]
                     [--tensorboard-logdir TENSORBOARD_LOGDIR]
                     [--wandb-project WANDB_PROJECT] [--azureml-logging]
                     [--seed SEED] [--cpu] [--tpu] [--bf16]
                     [--memory-efficient-bf16] [--fp16]
                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]
                     [--fp16-init-scale FP16_INIT_SCALE]
                     [--fp16-scale-window FP16_SCALE_WINDOW]
                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
                     [--on-cpu-convert-precision]
                     [--min-loss-scale MIN_LOSS_SCALE]
                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]
                     [--amp-batch-retries AMP_BATCH_RETRIES]
                     [--amp-init-scale AMP_INIT_SCALE]
                     [--amp-scale-window AMP_SCALE_WINDOW]
                     [--user-dir USER_DIR]
                     [--empty-cache-freq EMPTY_CACHE_FREQ]
                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]
                     [--model-parallel-size MODEL_PARALLEL_SIZE]
                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]
                     [--profile] [--reset-logging] [--suppress-crashes]
                     [--use-plasma-view] [--plasma-path PLASMA_PATH]
                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,speech_dlm_criterion,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]
                     [--tokenizer {moses,nltk,space}]
                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]
                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]
                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]
                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]
                     [--task TASK] [--num-workers NUM_WORKERS]
                     [--skip-invalid-size-inputs-valid-test]
                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]
                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]
                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]
                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]
                     [--data-buffer-size DATA_BUFFER_SIZE]
                     [--train-subset TRAIN_SUBSET]
                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]
                     [--ignore-unused-valid-subsets]
                     [--validate-interval VALIDATE_INTERVAL]
                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]
                     [--validate-after-updates VALIDATE_AFTER_UPDATES]
                     [--fixed-validation-seed FIXED_VALIDATION_SEED]
                     [--disable-validation]
                     [--max-tokens-valid MAX_TOKENS_VALID]
                     [--batch-size-valid BATCH_SIZE_VALID]
                     [--max-valid-steps MAX_VALID_STEPS]
                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]
                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]
                     [--grouped-shuffling]
                     [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]
                     [--update-ordered-indices-seed]
                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]
                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]
                     [--distributed-rank DISTRIBUTED_RANK]
                     [--distributed-backend DISTRIBUTED_BACKEND]
                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]
                     [--distributed-port DISTRIBUTED_PORT]
                     [--device-id DEVICE_ID] [--distributed-no-spawn]
                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]
                     [--ddp-comm-hook {none,fp16}]
                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]
                     [--find-unused-parameters] [--gradient-as-bucket-view]
                     [--fast-stat-sync]
                     [--heartbeat-timeout HEARTBEAT_TIMEOUT]
                     [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]
                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]
                     [--localsgd-frequency LOCALSGD_FREQUENCY]
                     [--nprocs-per-node NPROCS_PER_NODE]
                     [--pipeline-model-parallel]
                     [--pipeline-balance PIPELINE_BALANCE]
                     [--pipeline-devices PIPELINE_DEVICES]
                     [--pipeline-chunks PIPELINE_CHUNKS]
                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]
                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]
                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]
                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]
                     [--pipeline-checkpoint {always,never,except_last}]
                     [--zero-sharding {none,os}] [--no-reshard-after-forward]
                     [--fp32-reduce-scatter] [--cpu-offload]
                     [--use-sharded-state] [--not-fsdp-flatten-parameters]
                     [--arch ARCH] [--max-epoch MAX_EPOCH]
                     [--max-update MAX_UPDATE]
                     [--stop-time-hours STOP_TIME_HOURS]
                     [--clip-norm CLIP_NORM] [--sentence-avg]
                     [--update-freq UPDATE_FREQ] [--lr LR]
                     [--stop-min-lr STOP_MIN_LR] [--use-bmuf]
                     [--skip-remainder-batch] [--debug-param-names]
                     [--save-dir SAVE_DIR] [--restore-file RESTORE_FILE]
                     [--continue-once CONTINUE_ONCE]
                     [--finetune-from-model FINETUNE_FROM_MODEL]
                     [--reset-dataloader] [--reset-lr-scheduler]
                     [--reset-meters] [--reset-optimizer]
                     [--optimizer-overrides OPTIMIZER_OVERRIDES]
                     [--save-interval SAVE_INTERVAL]
                     [--save-interval-updates SAVE_INTERVAL_UPDATES]
                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]
                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]
                     [--keep-last-epochs KEEP_LAST_EPOCHS]
                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]
                     [--no-save] [--no-epoch-checkpoints]
                     [--no-last-checkpoints] [--no-save-optimizer-state]
                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]
                     [--checkpoint-suffix CHECKPOINT_SUFFIX]
                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]
                     [--load-checkpoint-on-all-dp-ranks]
                     [--write-checkpoints-asynchronously] [--store-ema]
                     [--ema-decay EMA_DECAY]
                     [--ema-start-update EMA_START_UPDATE]
                     [--ema-seed-model EMA_SEED_MODEL]
                     [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]
                     [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]
                     [--dropout DROPOUT]
                     [--attention-dropout ATTENTION_DROPOUT]
                     [--activation-dropout ACTIVATION_DROPOUT]
                     [--adaptive-input]
                     [--encoder-embed-path ENCODER_EMBED_PATH]
                     [--encoder-embed-dim ENCODER_EMBED_DIM]
                     [--encoder-ffn-embed-dim ENCODER_FFN_EMBED_DIM]
                     [--encoder-layers ENCODER_LAYERS]
                     [--encoder-attention-heads ENCODER_ATTENTION_HEADS]
                     [--encoder-normalize-before] [--encoder-learned-pos]
                     [--encoder-layerdrop ENCODER_LAYERDROP]
                     [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]
                     [--encoder-xformers-att-config ENCODER_XFORMERS_ATT_CONFIG]
                     [--max-source-positions MAX_SOURCE_POSITIONS]
                     [--decoder-embed-path DECODER_EMBED_PATH]
                     [--decoder-embed-dim DECODER_EMBED_DIM]
                     [--decoder-ffn-embed-dim DECODER_FFN_EMBED_DIM]
                     [--decoder-layers DECODER_LAYERS]
                     [--decoder-attention-heads DECODER_ATTENTION_HEADS]
                     [--decoder-normalize-before] [--decoder-learned-pos]
                     [--decoder-layerdrop DECODER_LAYERDROP]
                     [--decoder-layers-to-keep DECODER_LAYERS_TO_KEEP]
                     [--decoder-xformers-att-config DECODER_XFORMERS_ATT_CONFIG]
                     [--decoder-output-dim DECODER_OUTPUT_DIM]
                     [--max-target-positions MAX_TARGET_POSITIONS]
                     [--share-decoder-input-output-embed]
                     [--share-all-embeddings] [--merge-src-tgt-embed]
                     [--no-token-positional-embeddings]
                     [--adaptive-softmax-cutoff ADAPTIVE_SOFTMAX_CUTOFF]
                     [--adaptive-softmax-dropout ADAPTIVE_SOFTMAX_DROPOUT]
                     [--adaptive-softmax-factor ADAPTIVE_SOFTMAX_FACTOR]
                     [--layernorm-embedding] [--tie-adaptive-weights]
                     [--tie-adaptive-proj] [--no-scale-embedding]
                     [--checkpoint-activations] [--offload-activations]
                     [--no-cross-attention] [--cross-self-attention]
                     [--quant-noise-pq QUANT_NOISE_PQ]
                     [--quant-noise-pq-block-size QUANT_NOISE_PQ_BLOCK_SIZE]
                     [--quant-noise-scalar QUANT_NOISE_SCALAR]
                     [--min-params-to-wrap MIN_PARAMS_TO_WRAP] [--char-inputs]
                     [--relu-dropout RELU_DROPOUT] [--base-layers BASE_LAYERS]
                     [--base-sublayers BASE_SUBLAYERS]
                     [--base-shuffle BASE_SHUFFLE] [--export]
                     [--no-decoder-final-norm] [--source-lang SOURCE_LANG]
                     [--target-lang TARGET_LANG] [--load-alignments]
                     [--left-pad-source] [--left-pad-target]
                     [--upsample-primary UPSAMPLE_PRIMARY] [--truncate-source]
                     [--num-batch-buckets NUM_BATCH_BUCKETS] [--eval-bleu]
                     [--eval-bleu-args EVAL_BLEU_ARGS]
                     [--eval-bleu-detok EVAL_BLEU_DETOK]
                     [--eval-bleu-detok-args EVAL_BLEU_DETOK_ARGS]
                     [--eval-tokenized-bleu]
                     [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]]
                     [--eval-bleu-print-samples] [--alpha ALPHA]
                     [--adaptive-alpha ADAPTIVE_ALPHA]
                     [--max-updates-train MAX_UPDATES_TRAIN]
                     [--temperature-q TEMPERATURE_Q]
                     [--temperature-p TEMPERATURE_P] [--num-iter NUM_ITER]
                     [--div DIV] [--importance-metric IMPORTANCE_METRIC]
                     [--smooth-scores] [--weighted-freezing]
                     [--start-freezing START_FREEZING]
                     [--force-anneal FORCE_ANNEAL] [--lr-shrink LR_SHRINK]
                     [--warmup-updates WARMUP_UPDATES] [--pad PAD] [--eos EOS]
                     [--unk UNK]
                     data
fairseq-train: error: unrecognized arguments:  
/cm/local/apps/slurm/var/spool/job15053526/slurm_script: line 30: --importance-metric: command not found
2023-05-09 02:51:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-05-09 02:51:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/scratch4/cs601/tli104/wf_6000/checkpoints_fa', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_alpha=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, alpha=5.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/data-bin-fa', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, div='X', dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, importance_metric='magnitude', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_updates_train=25000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_iter=2, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch4/cs601/tli104/wf_6000/checkpoints_fa', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, smooth_scores=True, source_lang=None, start_freezing=6000, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation_intra_distillation', temperature_p=2, temperature_q=5, tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weighted_freezing=True, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation_intra_distillation', 'data': 'data/data-bin-fa', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False, 'alpha': 5.0, 'adaptive_alpha': 0, 'max_updates_train': 25000, 'temperature_q': 5.0, 'temperature_p': 2.0, 'num_iter': 2, 'div': 'X', 'importance_metric': 'magnitude', 'smooth_scores': True, 'weighted_freezing': True, 'start_freezing': 6000}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-09 02:51:38 | INFO | fairseq.tasks.translation | [fa] dictionary: 12001 types
2023-05-09 02:51:38 | INFO | fairseq.tasks.translation | [en] dictionary: 12001 types
2023-05-09 02:51:39 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=12001, bias=False)
  )
)
2023-05-09 02:51:39 | INFO | fairseq_cli.train | task: Translation_Intra_Distillation
2023-05-09 02:51:39 | INFO | fairseq_cli.train | model: TransformerModel
2023-05-09 02:51:39 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-09 02:51:39 | INFO | fairseq_cli.train | num. shared model params: 43,832,320 (num. trained: 43,832,320)
2023-05-09 02:51:39 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-09 02:51:39 | INFO | fairseq.data.data_utils | loaded 4,055 examples from: data/data-bin-fa/valid.fa-en.fa
2023-05-09 02:51:39 | INFO | fairseq.data.data_utils | loaded 4,055 examples from: data/data-bin-fa/valid.fa-en.en
2023-05-09 02:51:39 | INFO | fairseq.tasks.translation | data/data-bin-fa valid fa-en 4055 examples
2023-05-09 02:51:40 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-09 02:51:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 02:51:40 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.347 GB ; name = Graphics Device                         
2023-05-09 02:51:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 02:51:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-05-09 02:51:40 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-05-09 02:51:40 | INFO | fairseq.trainer | Preparing to load checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_last.pt
2023-05-09 02:51:40 | INFO | fairseq.trainer | No existing checkpoint found /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_last.pt
2023-05-09 02:51:40 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-09 02:51:40 | INFO | fairseq.data.data_utils | loaded 89,230 examples from: data/data-bin-fa/train.fa-en.fa
2023-05-09 02:51:40 | INFO | fairseq.data.data_utils | loaded 89,230 examples from: data/data-bin-fa/train.fa-en.en
2023-05-09 02:51:40 | INFO | fairseq.tasks.translation | data/data-bin-fa train fa-en 89230 examples
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 02:51:40 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2023-05-09 02:51:40 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 02:51:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 02:51:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 02:51:41 | INFO | fairseq.trainer | begin training epoch 1
2023-05-09 02:51:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 02:51:53 | INFO | train_inner | epoch 001:    100 / 654 loss=12.792, nll_loss=12.623, intra_distillation_loss=0, ppl=6308.02, wps=30905.5, ups=9.44, wpb=3278.9, bsz=138.2, num_updates=100, lr=1.25e-05, gnorm=4.013, train_wall=11, gb_free=77.1, wall=13
2023-05-09 02:52:04 | INFO | train_inner | epoch 001:    200 / 654 loss=10.983, nll_loss=10.596, intra_distillation_loss=0, ppl=1547.77, wps=31564.3, ups=9.26, wpb=3408.7, bsz=137.1, num_updates=200, lr=2.5e-05, gnorm=1.595, train_wall=10, gb_free=76.8, wall=24
2023-05-09 02:52:14 | INFO | train_inner | epoch 001:    300 / 654 loss=9.99, nll_loss=9.451, intra_distillation_loss=0, ppl=699.84, wps=31162.3, ups=9.34, wpb=3335.1, bsz=144.4, num_updates=300, lr=3.75e-05, gnorm=1.77, train_wall=10, gb_free=76.8, wall=34
2023-05-09 02:52:25 | INFO | train_inner | epoch 001:    400 / 654 loss=9.55, nll_loss=8.892, intra_distillation_loss=0, ppl=475.13, wps=31627.1, ups=9.38, wpb=3370.5, bsz=133, num_updates=400, lr=5e-05, gnorm=1.723, train_wall=10, gb_free=76.7, wall=45
2023-05-09 02:52:36 | INFO | train_inner | epoch 001:    500 / 654 loss=9.258, nll_loss=8.533, intra_distillation_loss=0, ppl=370.41, wps=31571.1, ups=9.38, wpb=3366.9, bsz=137, num_updates=500, lr=6.25e-05, gnorm=1.544, train_wall=10, gb_free=76.9, wall=56
2023-05-09 02:52:46 | INFO | train_inner | epoch 001:    600 / 654 loss=9.078, nll_loss=8.321, intra_distillation_loss=0, ppl=319.87, wps=31573.3, ups=9.44, wpb=3344.4, bsz=137.1, num_updates=600, lr=7.5e-05, gnorm=1.892, train_wall=10, gb_free=77.2, wall=66
2023-05-09 02:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 02:52:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:53:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.805 | nll_loss 7.963 | intra_distillation_loss 0 | ppl 249.47 | bleu 0.53 | wps 3483.8 | wpb 2524.5 | bsz 103.8 | num_updates 654
2023-05-09 02:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 654 updates
2023-05-09 02:53:20 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 1 @ 654 updates, score 0.53) (writing took 1.7888828930445015 seconds)
2023-05-09 02:53:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-09 02:53:22 | INFO | train | epoch 001 | loss 10.161 | nll_loss 9.602 | intra_distillation_loss 0 | ppl 777.24 | wps 21974.9 | ups 6.55 | wpb 3353.2 | bsz 136.4 | num_updates 654 | lr 8.175e-05 | gnorm 2.034 | train_wall 64 | gb_free 77 | wall 102
2023-05-09 02:53:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:53:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 02:53:22 | INFO | fairseq.trainer | begin training epoch 2
2023-05-09 02:53:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 02:53:27 | INFO | train_inner | epoch 002:     46 / 654 loss=8.948, nll_loss=8.174, intra_distillation_loss=0, ppl=288.86, wps=8168.7, ups=2.46, wpb=3320.3, bsz=119, num_updates=700, lr=8.75e-05, gnorm=1.461, train_wall=10, gb_free=76.7, wall=107
2023-05-09 02:53:38 | INFO | train_inner | epoch 002:    146 / 654 loss=8.683, nll_loss=7.872, intra_distillation_loss=0, ppl=234.34, wps=30889.1, ups=9.17, wpb=3369.5, bsz=139.4, num_updates=800, lr=0.0001, gnorm=1.709, train_wall=10, gb_free=77.1, wall=118
2023-05-09 02:53:49 | INFO | train_inner | epoch 002:    246 / 654 loss=8.502, nll_loss=7.666, intra_distillation_loss=0, ppl=203.1, wps=31333, ups=9.39, wpb=3338.2, bsz=139.1, num_updates=900, lr=0.0001125, gnorm=1.582, train_wall=10, gb_free=76.9, wall=128
2023-05-09 02:53:59 | INFO | train_inner | epoch 002:    346 / 654 loss=8.4, nll_loss=7.548, intra_distillation_loss=0, ppl=187.13, wps=31428.7, ups=9.4, wpb=3344.7, bsz=129.5, num_updates=1000, lr=0.000125, gnorm=1.724, train_wall=10, gb_free=76.6, wall=139
2023-05-09 02:54:10 | INFO | train_inner | epoch 002:    446 / 654 loss=8.184, nll_loss=7.302, intra_distillation_loss=0, ppl=157.77, wps=31830.7, ups=9.32, wpb=3415.4, bsz=152.7, num_updates=1100, lr=0.0001375, gnorm=1.621, train_wall=10, gb_free=77, wall=150
2023-05-09 02:54:21 | INFO | train_inner | epoch 002:    546 / 654 loss=8.203, nll_loss=7.325, intra_distillation_loss=0, ppl=160.29, wps=30989.2, ups=9.38, wpb=3302.1, bsz=125.8, num_updates=1200, lr=0.00015, gnorm=1.446, train_wall=10, gb_free=76.7, wall=161
2023-05-09 02:54:31 | INFO | train_inner | epoch 002:    646 / 654 loss=8.065, nll_loss=7.164, intra_distillation_loss=0, ppl=143.42, wps=31841.9, ups=9.35, wpb=3406.6, bsz=144.4, num_updates=1300, lr=0.0001625, gnorm=1.724, train_wall=10, gb_free=76.8, wall=171
2023-05-09 02:54:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 02:54:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:54:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.89 | nll_loss 6.932 | intra_distillation_loss 0 | ppl 122.12 | bleu 1.84 | wps 3743.7 | wpb 2524.5 | bsz 103.8 | num_updates 1308 | best_bleu 1.84
2023-05-09 02:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1308 updates
2023-05-09 02:54:58 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:54:59 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 2 @ 1308 updates, score 1.84) (writing took 1.803276987047866 seconds)
2023-05-09 02:55:00 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-09 02:55:00 | INFO | train | epoch 002 | loss 8.375 | nll_loss 7.52 | intra_distillation_loss 0 | ppl 183.59 | wps 22327.2 | ups 6.66 | wpb 3353.2 | bsz 136.4 | num_updates 1308 | lr 0.0001635 | gnorm 1.621 | train_wall 63 | gb_free 76.8 | wall 200
2023-05-09 02:55:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:55:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 02:55:00 | INFO | fairseq.trainer | begin training epoch 3
2023-05-09 02:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 02:55:10 | INFO | train_inner | epoch 003:     92 / 654 loss=7.958, nll_loss=7.044, intra_distillation_loss=0, ppl=131.93, wps=8728.5, ups=2.57, wpb=3401, bsz=132.1, num_updates=1400, lr=0.000175, gnorm=1.416, train_wall=10, gb_free=76.9, wall=210
2023-05-09 02:55:21 | INFO | train_inner | epoch 003:    192 / 654 loss=7.929, nll_loss=7.008, intra_distillation_loss=0, ppl=128.75, wps=30598.9, ups=9.3, wpb=3288.9, bsz=135.7, num_updates=1500, lr=0.0001875, gnorm=1.489, train_wall=10, gb_free=76.9, wall=221
2023-05-09 02:55:32 | INFO | train_inner | epoch 003:    292 / 654 loss=7.811, nll_loss=6.875, intra_distillation_loss=0, ppl=117.34, wps=31260.2, ups=9.39, wpb=3327.6, bsz=136.8, num_updates=1600, lr=0.0002, gnorm=1.387, train_wall=10, gb_free=76.9, wall=232
2023-05-09 02:55:42 | INFO | train_inner | epoch 003:    392 / 654 loss=7.73, nll_loss=6.783, intra_distillation_loss=0, ppl=110.15, wps=31166.1, ups=9.39, wpb=3320.4, bsz=134.6, num_updates=1700, lr=0.0002125, gnorm=1.275, train_wall=10, gb_free=77.2, wall=242
2023-05-09 02:55:53 | INFO | train_inner | epoch 003:    492 / 654 loss=7.711, nll_loss=6.761, intra_distillation_loss=0, ppl=108.44, wps=31571.5, ups=9.34, wpb=3380.2, bsz=132.6, num_updates=1800, lr=0.000225, gnorm=1.27, train_wall=10, gb_free=77.2, wall=253
2023-05-09 02:56:04 | INFO | train_inner | epoch 003:    592 / 654 loss=7.562, nll_loss=6.591, intra_distillation_loss=0, ppl=96.4, wps=31848.2, ups=9.33, wpb=3414.9, bsz=148.9, num_updates=1900, lr=0.0002375, gnorm=1.345, train_wall=10, gb_free=76.7, wall=264
2023-05-09 02:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 02:56:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:56:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.459 | nll_loss 6.427 | intra_distillation_loss 0 | ppl 86.02 | bleu 2.43 | wps 3642.4 | wpb 2524.5 | bsz 103.8 | num_updates 1962 | best_bleu 2.43
2023-05-09 02:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1962 updates
2023-05-09 02:56:37 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:56:38 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 3 @ 1962 updates, score 2.43) (writing took 1.835862915031612 seconds)
2023-05-09 02:56:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-09 02:56:39 | INFO | train | epoch 003 | loss 7.762 | nll_loss 6.819 | intra_distillation_loss 0 | ppl 112.91 | wps 22172.1 | ups 6.61 | wpb 3353.2 | bsz 136.4 | num_updates 1962 | lr 0.00024525 | gnorm 1.369 | train_wall 63 | gb_free 76.8 | wall 299
2023-05-09 02:56:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:56:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 02:56:39 | INFO | fairseq.trainer | begin training epoch 4
2023-05-09 02:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 02:56:43 | INFO | train_inner | epoch 004:     38 / 654 loss=7.645, nll_loss=6.685, intra_distillation_loss=0, ppl=102.92, wps=8383.5, ups=2.52, wpb=3323.5, bsz=123.4, num_updates=2000, lr=0.00025, gnorm=1.363, train_wall=10, gb_free=77.5, wall=303
2023-05-09 02:56:54 | INFO | train_inner | epoch 004:    138 / 654 loss=7.505, nll_loss=6.526, intra_distillation_loss=0, ppl=92.16, wps=31187.2, ups=9.34, wpb=3340.5, bsz=127.9, num_updates=2100, lr=0.0002625, gnorm=1.192, train_wall=10, gb_free=77.1, wall=314
2023-05-09 02:57:05 | INFO | train_inner | epoch 004:    238 / 654 loss=7.482, nll_loss=6.499, intra_distillation_loss=0, ppl=90.42, wps=31267.4, ups=9.47, wpb=3302.5, bsz=123, num_updates=2200, lr=0.000275, gnorm=1.256, train_wall=10, gb_free=77, wall=325
2023-05-09 02:57:15 | INFO | train_inner | epoch 004:    338 / 654 loss=7.419, nll_loss=6.427, intra_distillation_loss=0, ppl=86.04, wps=31530.1, ups=9.37, wpb=3363.7, bsz=132.6, num_updates=2300, lr=0.0002875, gnorm=1.374, train_wall=10, gb_free=76.8, wall=335
2023-05-09 02:57:26 | INFO | train_inner | epoch 004:    438 / 654 loss=7.317, nll_loss=6.311, intra_distillation_loss=0, ppl=79.39, wps=31236, ups=9.42, wpb=3317, bsz=142.4, num_updates=2400, lr=0.0003, gnorm=1.222, train_wall=10, gb_free=77, wall=346
2023-05-09 02:57:37 | INFO | train_inner | epoch 004:    538 / 654 loss=7.264, nll_loss=6.25, intra_distillation_loss=0, ppl=76.09, wps=31086.2, ups=9.42, wpb=3300.9, bsz=144.2, num_updates=2500, lr=0.0003125, gnorm=1.244, train_wall=10, gb_free=78, wall=356
2023-05-09 02:57:47 | INFO | train_inner | epoch 004:    638 / 654 loss=7.205, nll_loss=6.183, intra_distillation_loss=0, ppl=72.65, wps=31794.9, ups=9.14, wpb=3478, bsz=146.2, num_updates=2600, lr=0.000325, gnorm=1.173, train_wall=10, gb_free=76.9, wall=367
2023-05-09 02:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 02:57:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:58:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.086 | nll_loss 6.012 | intra_distillation_loss 0 | ppl 64.52 | bleu 3.5 | wps 3660 | wpb 2524.5 | bsz 103.8 | num_updates 2616 | best_bleu 3.5
2023-05-09 02:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2616 updates
2023-05-09 02:58:16 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:58:17 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:58:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 4 @ 2616 updates, score 3.5) (writing took 1.8150404799962416 seconds)
2023-05-09 02:58:18 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-09 02:58:18 | INFO | train | epoch 004 | loss 7.37 | nll_loss 6.371 | intra_distillation_loss 0 | ppl 82.78 | wps 22220.3 | ups 6.63 | wpb 3353.2 | bsz 136.4 | num_updates 2616 | lr 0.000327 | gnorm 1.244 | train_wall 63 | gb_free 76.7 | wall 398
2023-05-09 02:58:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:58:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 02:58:18 | INFO | fairseq.trainer | begin training epoch 5
2023-05-09 02:58:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 02:58:27 | INFO | train_inner | epoch 005:     84 / 654 loss=7.067, nll_loss=6.027, intra_distillation_loss=0, ppl=65.23, wps=8560.2, ups=2.52, wpb=3390.3, bsz=151.1, num_updates=2700, lr=0.0003375, gnorm=1.187, train_wall=10, gb_free=76.8, wall=407
2023-05-09 02:58:38 | INFO | train_inner | epoch 005:    184 / 654 loss=7.019, nll_loss=5.97, intra_distillation_loss=0, ppl=62.67, wps=31252.7, ups=9.42, wpb=3317.3, bsz=149.4, num_updates=2800, lr=0.00035, gnorm=1.185, train_wall=10, gb_free=77, wall=418
2023-05-09 02:58:48 | INFO | train_inner | epoch 005:    284 / 654 loss=7.1, nll_loss=6.062, intra_distillation_loss=0, ppl=66.8, wps=31179.6, ups=9.33, wpb=3342.3, bsz=133.8, num_updates=2900, lr=0.0003625, gnorm=1.145, train_wall=10, gb_free=76.8, wall=428
2023-05-09 02:58:59 | INFO | train_inner | epoch 005:    384 / 654 loss=7.092, nll_loss=6.052, intra_distillation_loss=0, ppl=66.36, wps=31567.4, ups=9.29, wpb=3397.1, bsz=133.6, num_updates=3000, lr=0.000375, gnorm=1.17, train_wall=10, gb_free=76.8, wall=439
2023-05-09 02:59:10 | INFO | train_inner | epoch 005:    484 / 654 loss=7.118, nll_loss=6.079, intra_distillation_loss=0, ppl=67.58, wps=31372.5, ups=9.47, wpb=3312.4, bsz=127.4, num_updates=3100, lr=0.0003875, gnorm=1.242, train_wall=10, gb_free=77, wall=450
2023-05-09 02:59:20 | INFO | train_inner | epoch 005:    584 / 654 loss=7.067, nll_loss=6.023, intra_distillation_loss=0, ppl=65.01, wps=31178.9, ups=9.44, wpb=3302.5, bsz=128.9, num_updates=3200, lr=0.0004, gnorm=1.135, train_wall=10, gb_free=76.9, wall=460
2023-05-09 02:59:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 02:59:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:59:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.814 | nll_loss 5.683 | intra_distillation_loss 0 | ppl 51.37 | bleu 4.53 | wps 3589.1 | wpb 2524.5 | bsz 103.8 | num_updates 3270 | best_bleu 4.53
2023-05-09 02:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3270 updates
2023-05-09 02:59:55 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 02:59:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 5 @ 3270 updates, score 4.53) (writing took 1.798048389959149 seconds)
2023-05-09 02:59:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-09 02:59:57 | INFO | train | epoch 005 | loss 7.068 | nll_loss 6.025 | intra_distillation_loss 0 | ppl 65.11 | wps 22102.5 | ups 6.59 | wpb 3353.2 | bsz 136.4 | num_updates 3270 | lr 0.00040875 | gnorm 1.166 | train_wall 63 | gb_free 77.1 | wall 497
2023-05-09 02:59:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 02:59:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 02:59:57 | INFO | fairseq.trainer | begin training epoch 6
2023-05-09 02:59:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:00:00 | INFO | train_inner | epoch 006:     30 / 654 loss=6.949, nll_loss=5.887, intra_distillation_loss=0, ppl=59.17, wps=8637.7, ups=2.5, wpb=3458, bsz=133, num_updates=3300, lr=0.0004125, gnorm=1.072, train_wall=10, gb_free=76.8, wall=500
2023-05-09 03:00:11 | INFO | train_inner | epoch 006:    130 / 654 loss=6.921, nll_loss=5.857, intra_distillation_loss=0, ppl=57.94, wps=31614, ups=9.33, wpb=3388.6, bsz=132.7, num_updates=3400, lr=0.000425, gnorm=1.211, train_wall=10, gb_free=76.8, wall=511
2023-05-09 03:00:22 | INFO | train_inner | epoch 006:    230 / 654 loss=6.899, nll_loss=5.828, intra_distillation_loss=0, ppl=56.82, wps=30730.4, ups=9.55, wpb=3219.1, bsz=127.4, num_updates=3500, lr=0.0004375, gnorm=1.152, train_wall=9, gb_free=77, wall=522
2023-05-09 03:00:32 | INFO | train_inner | epoch 006:    330 / 654 loss=6.742, nll_loss=5.65, intra_distillation_loss=0, ppl=50.22, wps=31192.5, ups=9.2, wpb=3392.2, bsz=159.9, num_updates=3600, lr=0.00045, gnorm=1.206, train_wall=10, gb_free=77, wall=532
2023-05-09 03:00:43 | INFO | train_inner | epoch 006:    430 / 654 loss=6.859, nll_loss=5.782, intra_distillation_loss=0, ppl=55.02, wps=31372.6, ups=9.27, wpb=3385.5, bsz=128.2, num_updates=3700, lr=0.0004625, gnorm=1.082, train_wall=10, gb_free=76.7, wall=543
2023-05-09 03:00:54 | INFO | train_inner | epoch 006:    530 / 654 loss=6.85, nll_loss=5.77, intra_distillation_loss=0, ppl=54.58, wps=31220.7, ups=9.44, wpb=3308.2, bsz=139, num_updates=3800, lr=0.000475, gnorm=1.232, train_wall=10, gb_free=76.8, wall=554
2023-05-09 03:01:05 | INFO | train_inner | epoch 006:    630 / 654 loss=6.765, nll_loss=5.674, intra_distillation_loss=0, ppl=51.06, wps=31632, ups=9.26, wpb=3415.9, bsz=144.2, num_updates=3900, lr=0.0004875, gnorm=1.043, train_wall=10, gb_free=76.9, wall=565
2023-05-09 03:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:01:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:01:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.663 | nll_loss 5.502 | intra_distillation_loss 0 | ppl 45.32 | bleu 3.98 | wps 3693.7 | wpb 2524.5 | bsz 103.8 | num_updates 3924 | best_bleu 4.53
2023-05-09 03:01:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 3924 updates
2023-05-09 03:01:34 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_last.pt
2023-05-09 03:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_last.pt
2023-05-09 03:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_last.pt (epoch 6 @ 3924 updates, score 3.98) (writing took 0.7419219949515536 seconds)
2023-05-09 03:01:34 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-09 03:01:34 | INFO | train | epoch 006 | loss 6.846 | nll_loss 5.768 | intra_distillation_loss 0 | ppl 54.48 | wps 22517.7 | ups 6.72 | wpb 3353.2 | bsz 136.4 | num_updates 3924 | lr 0.0004905 | gnorm 1.15 | train_wall 63 | gb_free 76.9 | wall 594
2023-05-09 03:01:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:01:35 | INFO | fairseq.trainer | begin training epoch 7
2023-05-09 03:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:01:43 | INFO | train_inner | epoch 007:     76 / 654 loss=6.716, nll_loss=5.619, intra_distillation_loss=0, ppl=49.15, wps=8906.1, ups=2.62, wpb=3400.8, bsz=129.1, num_updates=4000, lr=0.0005, gnorm=1.087, train_wall=10, gb_free=77, wall=603
2023-05-09 03:01:53 | INFO | train_inner | epoch 007:    176 / 654 loss=6.711, nll_loss=5.612, intra_distillation_loss=0, ppl=48.9, wps=30981.4, ups=9.39, wpb=3300.4, bsz=133.9, num_updates=4100, lr=0.000493865, gnorm=1.146, train_wall=10, gb_free=76.7, wall=613
2023-05-09 03:02:04 | INFO | train_inner | epoch 007:    276 / 654 loss=6.616, nll_loss=5.503, intra_distillation_loss=0, ppl=45.36, wps=31106.1, ups=9.22, wpb=3372.8, bsz=142.3, num_updates=4200, lr=0.00048795, gnorm=1.077, train_wall=10, gb_free=76.8, wall=624
2023-05-09 03:02:15 | INFO | train_inner | epoch 007:    376 / 654 loss=6.644, nll_loss=5.533, intra_distillation_loss=0, ppl=46.3, wps=31989.3, ups=9.39, wpb=3406, bsz=134.1, num_updates=4300, lr=0.000482243, gnorm=1.052, train_wall=10, gb_free=76.8, wall=635
2023-05-09 03:02:26 | INFO | train_inner | epoch 007:    476 / 654 loss=6.518, nll_loss=5.39, intra_distillation_loss=0, ppl=41.94, wps=31088.5, ups=9.38, wpb=3313, bsz=140.8, num_updates=4400, lr=0.000476731, gnorm=1.093, train_wall=10, gb_free=76.9, wall=646
2023-05-09 03:02:36 | INFO | train_inner | epoch 007:    576 / 654 loss=6.605, nll_loss=5.488, intra_distillation_loss=0, ppl=44.87, wps=31661.2, ups=9.4, wpb=3368.5, bsz=121.3, num_updates=4500, lr=0.000471405, gnorm=1.041, train_wall=10, gb_free=77.1, wall=656
2023-05-09 03:02:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:02:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:03:11 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.338 | nll_loss 5.119 | intra_distillation_loss 0 | ppl 34.76 | bleu 5.76 | wps 3737.9 | wpb 2524.5 | bsz 103.8 | num_updates 4578 | best_bleu 5.76
2023-05-09 03:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4578 updates
2023-05-09 03:03:11 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:03:12 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:03:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 7 @ 4578 updates, score 5.76) (writing took 1.6347286329837516 seconds)
2023-05-09 03:03:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-09 03:03:12 | INFO | train | epoch 007 | loss 6.596 | nll_loss 5.479 | intra_distillation_loss 0 | ppl 44.62 | wps 22375.9 | ups 6.67 | wpb 3353.2 | bsz 136.4 | num_updates 4578 | lr 0.000467371 | gnorm 1.082 | train_wall 63 | gb_free 77.1 | wall 692
2023-05-09 03:03:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:03:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:03:13 | INFO | fairseq.trainer | begin training epoch 8
2023-05-09 03:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:03:15 | INFO | train_inner | epoch 008:     22 / 654 loss=6.349, nll_loss=5.198, intra_distillation_loss=0, ppl=36.71, wps=8393.6, ups=2.58, wpb=3247.5, bsz=151, num_updates=4600, lr=0.000466252, gnorm=1.099, train_wall=10, gb_free=77.1, wall=695
2023-05-09 03:03:26 | INFO | train_inner | epoch 008:    122 / 654 loss=6.248, nll_loss=5.08, intra_distillation_loss=0, ppl=33.83, wps=32275.1, ups=9.31, wpb=3467.8, bsz=152.1, num_updates=4700, lr=0.000461266, gnorm=1.111, train_wall=10, gb_free=76.8, wall=706
2023-05-09 03:03:37 | INFO | train_inner | epoch 008:    222 / 654 loss=6.314, nll_loss=5.155, intra_distillation_loss=0, ppl=35.64, wps=31090.8, ups=9.21, wpb=3376, bsz=136.2, num_updates=4800, lr=0.000456435, gnorm=1.112, train_wall=10, gb_free=76.9, wall=716
2023-05-09 03:03:47 | INFO | train_inner | epoch 008:    322 / 654 loss=6.234, nll_loss=5.061, intra_distillation_loss=0, ppl=33.39, wps=32213.7, ups=9.27, wpb=3474.5, bsz=141.2, num_updates=4900, lr=0.000451754, gnorm=1.123, train_wall=10, gb_free=76.9, wall=727
2023-05-09 03:03:58 | INFO | train_inner | epoch 008:    422 / 654 loss=6.317, nll_loss=5.154, intra_distillation_loss=0, ppl=35.61, wps=31143.7, ups=9.51, wpb=3276.3, bsz=120.2, num_updates=5000, lr=0.000447214, gnorm=1.105, train_wall=10, gb_free=76.9, wall=738
2023-05-09 03:04:08 | INFO | train_inner | epoch 008:    522 / 654 loss=6.161, nll_loss=4.978, intra_distillation_loss=0, ppl=31.52, wps=30977.2, ups=9.4, wpb=3296, bsz=141.4, num_updates=5100, lr=0.000442807, gnorm=1.13, train_wall=10, gb_free=76.9, wall=748
2023-05-09 03:04:19 | INFO | train_inner | epoch 008:    622 / 654 loss=6.232, nll_loss=5.058, intra_distillation_loss=0, ppl=33.3, wps=30844, ups=9.48, wpb=3252.5, bsz=127.3, num_updates=5200, lr=0.000438529, gnorm=1.217, train_wall=10, gb_free=76.9, wall=759
2023-05-09 03:04:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:04:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:04:50 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.023 | nll_loss 4.738 | intra_distillation_loss 0 | ppl 26.69 | bleu 8.5 | wps 3614.7 | wpb 2524.5 | bsz 103.8 | num_updates 5232 | best_bleu 8.5
2023-05-09 03:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5232 updates
2023-05-09 03:04:50 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 8 @ 5232 updates, score 8.5) (writing took 1.7880396860418841 seconds)
2023-05-09 03:04:51 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-09 03:04:51 | INFO | train | epoch 008 | loss 6.249 | nll_loss 5.079 | intra_distillation_loss 0 | ppl 33.81 | wps 22165.6 | ups 6.61 | wpb 3353.2 | bsz 136.4 | num_updates 5232 | lr 0.000437186 | gnorm 1.134 | train_wall 63 | gb_free 77.1 | wall 791
2023-05-09 03:04:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:04:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:04:51 | INFO | fairseq.trainer | begin training epoch 9
2023-05-09 03:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:04:59 | INFO | train_inner | epoch 009:     68 / 654 loss=6.04, nll_loss=4.84, intra_distillation_loss=0, ppl=28.64, wps=8591, ups=2.51, wpb=3425.1, bsz=137.9, num_updates=5300, lr=0.000434372, gnorm=1.117, train_wall=10, gb_free=76.9, wall=799
2023-05-09 03:05:10 | INFO | train_inner | epoch 009:    168 / 654 loss=5.976, nll_loss=4.766, intra_distillation_loss=0, ppl=27.21, wps=31405.6, ups=9.38, wpb=3348.8, bsz=135, num_updates=5400, lr=0.000430331, gnorm=1.106, train_wall=10, gb_free=77.1, wall=810
2023-05-09 03:05:20 | INFO | train_inner | epoch 009:    268 / 654 loss=5.935, nll_loss=4.717, intra_distillation_loss=0, ppl=26.31, wps=31811.6, ups=9.31, wpb=3418.4, bsz=137.8, num_updates=5500, lr=0.000426401, gnorm=1.139, train_wall=10, gb_free=77, wall=820
2023-05-09 03:05:31 | INFO | train_inner | epoch 009:    368 / 654 loss=5.961, nll_loss=4.746, intra_distillation_loss=0, ppl=26.84, wps=30395.7, ups=9.49, wpb=3203.2, bsz=133.6, num_updates=5600, lr=0.000422577, gnorm=1.128, train_wall=9, gb_free=76.8, wall=831
2023-05-09 03:05:41 | INFO | train_inner | epoch 009:    468 / 654 loss=5.885, nll_loss=4.659, intra_distillation_loss=0, ppl=25.27, wps=32008.2, ups=9.4, wpb=3404.3, bsz=145.6, num_updates=5700, lr=0.000418854, gnorm=1.155, train_wall=10, gb_free=76.9, wall=841
2023-05-09 03:05:52 | INFO | train_inner | epoch 009:    568 / 654 loss=5.946, nll_loss=4.729, intra_distillation_loss=0, ppl=26.51, wps=31071.1, ups=9.37, wpb=3314.5, bsz=126.2, num_updates=5800, lr=0.000415227, gnorm=1.127, train_wall=10, gb_free=76.8, wall=852
2023-05-09 03:06:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:06:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:06:28 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.785 | nll_loss 4.435 | intra_distillation_loss 0 | ppl 21.64 | bleu 10.78 | wps 3686.4 | wpb 2524.5 | bsz 103.8 | num_updates 5886 | best_bleu 10.78
2023-05-09 03:06:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 5886 updates
2023-05-09 03:06:28 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:06:29 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 9 @ 5886 updates, score 10.78) (writing took 1.6803039840888232 seconds)
2023-05-09 03:06:30 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-09 03:06:30 | INFO | train | epoch 009 | loss 5.933 | nll_loss 4.715 | intra_distillation_loss 0 | ppl 26.27 | wps 22322.5 | ups 6.66 | wpb 3353.2 | bsz 136.4 | num_updates 5886 | lr 0.000412183 | gnorm 1.126 | train_wall 63 | gb_free 76.9 | wall 890
2023-05-09 03:06:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:06:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:06:30 | INFO | fairseq.trainer | begin training epoch 10
2023-05-09 03:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:06:31 | INFO | train_inner | epoch 010:     14 / 654 loss=5.827, nll_loss=4.593, intra_distillation_loss=0, ppl=24.14, wps=8627.9, ups=2.56, wpb=3375.5, bsz=140.1, num_updates=5900, lr=0.000411693, gnorm=1.119, train_wall=10, gb_free=76.8, wall=891
2023-05-09 03:06:42 | INFO | train_inner | epoch 010:    114 / 654 loss=5.771, nll_loss=4.529, intra_distillation_loss=0, ppl=23.08, wps=31647.1, ups=9.37, wpb=3377.6, bsz=131.8, num_updates=6000, lr=0.000408248, gnorm=1.134, train_wall=10, gb_free=76.9, wall=902
2023-05-09 03:06:55 | INFO | train_inner | epoch 010:    214 / 654 loss=5.707, nll_loss=4.461, intra_distillation_loss=0, ppl=22.03, wps=26448, ups=7.94, wpb=3329.3, bsz=141.4, num_updates=6100, lr=0.000404888, gnorm=0, train_wall=12, gb_free=76.9, wall=914
2023-05-09 03:07:07 | INFO | train_inner | epoch 010:    314 / 654 loss=5.71, nll_loss=4.465, intra_distillation_loss=0, ppl=22.09, wps=26699.6, ups=7.92, wpb=3372.5, bsz=120, num_updates=6200, lr=0.00040161, gnorm=0, train_wall=12, gb_free=77.1, wall=927
2023-05-09 03:07:20 | INFO | train_inner | epoch 010:    414 / 654 loss=5.589, nll_loss=4.327, intra_distillation_loss=0, ppl=20.07, wps=27243.9, ups=7.84, wpb=3473.9, bsz=143, num_updates=6300, lr=0.00039841, gnorm=0, train_wall=12, gb_free=76.9, wall=940
2023-05-09 03:07:32 | INFO | train_inner | epoch 010:    514 / 654 loss=5.604, nll_loss=4.341, intra_distillation_loss=0, ppl=20.27, wps=25842.8, ups=8.01, wpb=3226.7, bsz=131.9, num_updates=6400, lr=0.000395285, gnorm=0, train_wall=11, gb_free=76.7, wall=952
2023-05-09 03:07:45 | INFO | train_inner | epoch 010:    614 / 654 loss=5.549, nll_loss=4.278, intra_distillation_loss=0, ppl=19.4, wps=26207.2, ups=7.96, wpb=3292.2, bsz=146.4, num_updates=6500, lr=0.000392232, gnorm=0, train_wall=11, gb_free=76.9, wall=965
2023-05-09 03:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:07:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:08:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.559 | nll_loss 4.173 | intra_distillation_loss 0 | ppl 18.04 | bleu 13.37 | wps 3742.1 | wpb 2524.5 | bsz 103.8 | num_updates 6540 | best_bleu 13.37
2023-05-09 03:08:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6540 updates
2023-05-09 03:08:16 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:08:17 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:08:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 10 @ 6540 updates, score 13.37) (writing took 1.6819522100267932 seconds)
2023-05-09 03:08:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-09 03:08:18 | INFO | train | epoch 010 | loss 5.648 | nll_loss 4.392 | intra_distillation_loss 0 | ppl 20.99 | wps 20247.9 | ups 6.04 | wpb 3353.2 | bsz 136.4 | num_updates 6540 | lr 0.000391031 | gnorm 0.197 | train_wall 73 | gb_free 76.8 | wall 998
2023-05-09 03:08:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:08:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:08:18 | INFO | fairseq.trainer | begin training epoch 11
2023-05-09 03:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:08:26 | INFO | train_inner | epoch 011:     60 / 654 loss=5.466, nll_loss=4.184, intra_distillation_loss=0, ppl=18.17, wps=8562.7, ups=2.44, wpb=3506.1, bsz=142.8, num_updates=6600, lr=0.000389249, gnorm=0, train_wall=12, gb_free=76.8, wall=1006
2023-05-09 03:08:38 | INFO | train_inner | epoch 011:    160 / 654 loss=5.575, nll_loss=4.304, intra_distillation_loss=0, ppl=19.75, wps=26369.6, ups=8.08, wpb=3261.7, bsz=114.9, num_updates=6700, lr=0.000386334, gnorm=0, train_wall=11, gb_free=77, wall=1018
2023-05-09 03:08:51 | INFO | train_inner | epoch 011:    260 / 654 loss=5.422, nll_loss=4.13, intra_distillation_loss=0, ppl=17.5, wps=25940.1, ups=7.92, wpb=3277, bsz=138.6, num_updates=6800, lr=0.000383482, gnorm=0, train_wall=12, gb_free=76.9, wall=1031
2023-05-09 03:09:04 | INFO | train_inner | epoch 011:    360 / 654 loss=5.415, nll_loss=4.119, intra_distillation_loss=0, ppl=17.37, wps=26754, ups=7.89, wpb=3391.2, bsz=134.9, num_updates=6900, lr=0.000380693, gnorm=0, train_wall=12, gb_free=76.8, wall=1044
2023-05-09 03:09:16 | INFO | train_inner | epoch 011:    460 / 654 loss=5.422, nll_loss=4.127, intra_distillation_loss=0, ppl=17.47, wps=26404.9, ups=7.98, wpb=3307.7, bsz=144.6, num_updates=7000, lr=0.000377964, gnorm=0, train_wall=12, gb_free=77, wall=1056
2023-05-09 03:09:29 | INFO | train_inner | epoch 011:    560 / 654 loss=5.359, nll_loss=4.055, intra_distillation_loss=0, ppl=16.62, wps=27062.2, ups=7.85, wpb=3447.1, bsz=147.9, num_updates=7100, lr=0.000375293, gnorm=0, train_wall=12, gb_free=76.9, wall=1069
2023-05-09 03:09:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:09:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:10:08 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.456 | nll_loss 4.034 | intra_distillation_loss 0 | ppl 16.38 | bleu 14.05 | wps 3613 | wpb 2524.5 | bsz 103.8 | num_updates 7194 | best_bleu 14.05
2023-05-09 03:10:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 7194 updates
2023-05-09 03:10:08 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 11 @ 7194 updates, score 14.05) (writing took 1.7265263890149072 seconds)
2023-05-09 03:10:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-09 03:10:10 | INFO | train | epoch 011 | loss 5.436 | nll_loss 4.144 | intra_distillation_loss 0 | ppl 17.68 | wps 19661.1 | ups 5.86 | wpb 3353.2 | bsz 136.4 | num_updates 7194 | lr 0.000372833 | gnorm 0 | train_wall 76 | gb_free 77 | wall 1109
2023-05-09 03:10:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:10:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:10:10 | INFO | fairseq.trainer | begin training epoch 12
2023-05-09 03:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:10:10 | INFO | train_inner | epoch 012:      6 / 654 loss=5.422, nll_loss=4.125, intra_distillation_loss=0, ppl=17.45, wps=8002.3, ups=2.41, wpb=3326.7, bsz=135.1, num_updates=7200, lr=0.000372678, gnorm=0, train_wall=12, gb_free=77.1, wall=1110
2023-05-09 03:10:23 | INFO | train_inner | epoch 012:    106 / 654 loss=5.267, nll_loss=3.949, intra_distillation_loss=0, ppl=15.45, wps=26694.2, ups=7.9, wpb=3379, bsz=139.2, num_updates=7300, lr=0.000370117, gnorm=0, train_wall=12, gb_free=77.1, wall=1123
2023-05-09 03:10:36 | INFO | train_inner | epoch 012:    206 / 654 loss=5.313, nll_loss=3.999, intra_distillation_loss=0, ppl=15.99, wps=26757.7, ups=7.94, wpb=3369.4, bsz=131.8, num_updates=7400, lr=0.000367607, gnorm=0, train_wall=12, gb_free=77, wall=1136
2023-05-09 03:10:48 | INFO | train_inner | epoch 012:    306 / 654 loss=5.312, nll_loss=3.998, intra_distillation_loss=0, ppl=15.97, wps=26657.7, ups=7.91, wpb=3370.2, bsz=136.1, num_updates=7500, lr=0.000365148, gnorm=0, train_wall=12, gb_free=76.4, wall=1148
2023-05-09 03:11:01 | INFO | train_inner | epoch 012:    406 / 654 loss=5.33, nll_loss=4.017, intra_distillation_loss=0, ppl=16.19, wps=26527, ups=8, wpb=3314.6, bsz=130.6, num_updates=7600, lr=0.000362738, gnorm=0, train_wall=11, gb_free=76.9, wall=1161
2023-05-09 03:11:13 | INFO | train_inner | epoch 012:    506 / 654 loss=5.248, nll_loss=3.924, intra_distillation_loss=0, ppl=15.18, wps=26703.7, ups=7.91, wpb=3374.7, bsz=153, num_updates=7700, lr=0.000360375, gnorm=0, train_wall=12, gb_free=76.9, wall=1173
2023-05-09 03:11:26 | INFO | train_inner | epoch 012:    606 / 654 loss=5.413, nll_loss=4.111, intra_distillation_loss=0, ppl=17.28, wps=26741.4, ups=8, wpb=3342.5, bsz=124.8, num_updates=7800, lr=0.000358057, gnorm=0, train_wall=12, gb_free=77, wall=1186
2023-05-09 03:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:11:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:11:57 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.33 | nll_loss 3.886 | intra_distillation_loss 0 | ppl 14.78 | bleu 15.95 | wps 3877.1 | wpb 2524.5 | bsz 103.8 | num_updates 7848 | best_bleu 15.95
2023-05-09 03:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 7848 updates
2023-05-09 03:11:57 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 12 @ 7848 updates, score 15.95) (writing took 1.6928800899768248 seconds)
2023-05-09 03:11:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-09 03:11:59 | INFO | train | epoch 012 | loss 5.307 | nll_loss 3.991 | intra_distillation_loss 0 | ppl 15.9 | wps 20012.4 | ups 5.97 | wpb 3353.2 | bsz 136.4 | num_updates 7848 | lr 0.000356961 | gnorm 0 | train_wall 76 | gb_free 76.9 | wall 1219
2023-05-09 03:11:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:11:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:11:59 | INFO | fairseq.trainer | begin training epoch 13
2023-05-09 03:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:12:06 | INFO | train_inner | epoch 013:     52 / 654 loss=5.216, nll_loss=3.888, intra_distillation_loss=0, ppl=14.81, wps=8268.3, ups=2.52, wpb=3286.3, bsz=139.5, num_updates=7900, lr=0.000355784, gnorm=0, train_wall=12, gb_free=76.9, wall=1226
2023-05-09 03:12:18 | INFO | train_inner | epoch 013:    152 / 654 loss=5.142, nll_loss=3.802, intra_distillation_loss=0, ppl=13.95, wps=26974.3, ups=7.9, wpb=3414.8, bsz=144.8, num_updates=8000, lr=0.000353553, gnorm=0, train_wall=12, gb_free=76.8, wall=1238
2023-05-09 03:12:31 | INFO | train_inner | epoch 013:    252 / 654 loss=5.265, nll_loss=3.941, intra_distillation_loss=0, ppl=15.35, wps=26980, ups=7.95, wpb=3395.6, bsz=125.6, num_updates=8100, lr=0.000351364, gnorm=0, train_wall=12, gb_free=77.2, wall=1251
2023-05-09 03:12:44 | INFO | train_inner | epoch 013:    352 / 654 loss=5.148, nll_loss=3.807, intra_distillation_loss=0, ppl=14, wps=26308.3, ups=7.9, wpb=3329.8, bsz=145.9, num_updates=8200, lr=0.000349215, gnorm=0, train_wall=12, gb_free=76.9, wall=1264
2023-05-09 03:12:56 | INFO | train_inner | epoch 013:    452 / 654 loss=5.223, nll_loss=3.892, intra_distillation_loss=0, ppl=14.85, wps=26288.8, ups=7.99, wpb=3290.3, bsz=127.8, num_updates=8300, lr=0.000347105, gnorm=0, train_wall=11, gb_free=77.1, wall=1276
2023-05-09 03:13:09 | INFO | train_inner | epoch 013:    552 / 654 loss=5.233, nll_loss=3.904, intra_distillation_loss=0, ppl=14.97, wps=26638.8, ups=7.91, wpb=3367, bsz=136.3, num_updates=8400, lr=0.000345033, gnorm=0, train_wall=12, gb_free=77.1, wall=1289
2023-05-09 03:13:21 | INFO | train_inner | epoch 013:    652 / 654 loss=5.224, nll_loss=3.894, intra_distillation_loss=0, ppl=14.86, wps=26887.8, ups=7.97, wpb=3372.1, bsz=136.2, num_updates=8500, lr=0.000342997, gnorm=0, train_wall=12, gb_free=77, wall=1301
2023-05-09 03:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:13:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:13:45 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.268 | nll_loss 3.811 | intra_distillation_loss 0 | ppl 14.04 | bleu 16.67 | wps 4148.2 | wpb 2524.5 | bsz 103.8 | num_updates 8502 | best_bleu 16.67
2023-05-09 03:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 8502 updates
2023-05-09 03:13:45 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:13:46 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:13:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 13 @ 8502 updates, score 16.67) (writing took 1.7003168310038745 seconds)
2023-05-09 03:13:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-09 03:13:47 | INFO | train | epoch 013 | loss 5.204 | nll_loss 3.871 | intra_distillation_loss 0 | ppl 14.63 | wps 20310.6 | ups 6.06 | wpb 3353.2 | bsz 136.4 | num_updates 8502 | lr 0.000342957 | gnorm 0 | train_wall 76 | gb_free 77 | wall 1327
2023-05-09 03:13:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:13:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:13:47 | INFO | fairseq.trainer | begin training epoch 14
2023-05-09 03:13:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:14:00 | INFO | train_inner | epoch 014:     98 / 654 loss=5.168, nll_loss=3.829, intra_distillation_loss=0, ppl=14.21, wps=8940.6, ups=2.61, wpb=3419, bsz=118.6, num_updates=8600, lr=0.000340997, gnorm=0, train_wall=12, gb_free=76.8, wall=1339
2023-05-09 03:14:12 | INFO | train_inner | epoch 014:    198 / 654 loss=5.114, nll_loss=3.767, intra_distillation_loss=0, ppl=13.61, wps=26482.2, ups=7.99, wpb=3314.5, bsz=139.3, num_updates=8700, lr=0.000339032, gnorm=0, train_wall=11, gb_free=76.9, wall=1352
2023-05-09 03:14:25 | INFO | train_inner | epoch 014:    298 / 654 loss=5.095, nll_loss=3.745, intra_distillation_loss=0, ppl=13.41, wps=26870.9, ups=7.91, wpb=3398.9, bsz=144.6, num_updates=8800, lr=0.0003371, gnorm=0, train_wall=12, gb_free=76.8, wall=1365
2023-05-09 03:14:37 | INFO | train_inner | epoch 014:    398 / 654 loss=5.129, nll_loss=3.783, intra_distillation_loss=0, ppl=13.76, wps=26299.7, ups=7.9, wpb=3330, bsz=131.9, num_updates=8900, lr=0.000335201, gnorm=0, train_wall=12, gb_free=77, wall=1377
2023-05-09 03:14:50 | INFO | train_inner | epoch 014:    498 / 654 loss=5.14, nll_loss=3.795, intra_distillation_loss=0, ppl=13.88, wps=26633.5, ups=7.95, wpb=3349.9, bsz=137.5, num_updates=9000, lr=0.000333333, gnorm=0, train_wall=12, gb_free=77, wall=1390
2023-05-09 03:15:02 | INFO | train_inner | epoch 014:    598 / 654 loss=5.101, nll_loss=3.751, intra_distillation_loss=0, ppl=13.46, wps=26280.6, ups=7.97, wpb=3295.4, bsz=136.4, num_updates=9100, lr=0.000331497, gnorm=0, train_wall=11, gb_free=76.8, wall=1402
2023-05-09 03:15:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:15:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:15:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.207 | nll_loss 3.738 | intra_distillation_loss 0 | ppl 13.34 | bleu 17.67 | wps 3791.7 | wpb 2524.5 | bsz 103.8 | num_updates 9156 | best_bleu 17.67
2023-05-09 03:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 9156 updates
2023-05-09 03:15:36 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt
2023-05-09 03:15:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_fa/checkpoint_best.pt (epoch 14 @ 9156 updates, score 17.67) (writing took 1.6965239470591769 seconds)
2023-05-09 03:15:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-09 03:15:37 | INFO | train | epoch 014 | loss 5.117 | nll_loss 3.77 | intra_distillation_loss 0 | ppl 13.64 | wps 19883.2 | ups 5.93 | wpb 3353.2 | bsz 136.4 | num_updates 9156 | lr 0.000330481 | gnorm 0 | train_wall 76 | gb_free 76.7 | wall 1437
2023-05-09 03:15:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:15:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 654
2023-05-09 03:15:37 | INFO | fairseq.trainer | begin training epoch 15
2023-05-09 03:15:37 | INFO | fairseq_cli.train | Start iterating over samples
slurmstepd: error: *** JOB 15053526 ON icgpu04 CANCELLED AT 2023-05-09T03:15:39 ***
