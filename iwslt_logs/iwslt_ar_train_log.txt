Direction ar to en
2023-05-09 03:17:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
Traceback (most recent call last):
  File "/home/tli104/.conda/envs/fairseqenv/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/tli104/my_fairseq/fairseq_cli/train.py", line 574, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/tli104/my_fairseq/fairseq/distributed/utils.py", line 404, in call_main
    main(cfg, **kwargs)
  File "/home/tli104/my_fairseq/fairseq_cli/train.py", line 58, in main
    assert (
AssertionError: Must specify batch size either with --max-tokens or --batch-size
/cm/local/apps/slurm/var/spool/job15056575/slurm_script: line 28: --weighted-freezing: command not found
/cm/local/apps/slurm/var/spool/job15056575/slurm_script: line 30: --importance-metric: command not found
2023-05-09 03:17:35 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-05-09 03:17:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/scratch4/cs601/tli104/wf_6000/checkpoints_ar', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_alpha=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, alpha=5.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/data-bin-ar', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, div='X', dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, importance_metric='magnitude', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_updates_train=25000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_iter=2, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch4/cs601/tli104/wf_6000/checkpoints_ar', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, smooth_scores=True, source_lang=None, start_freezing=6000, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation_intra_distillation', temperature_p=2, temperature_q=5, tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weighted_freezing=True, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation_intra_distillation', 'data': 'data/data-bin-ar', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False, 'alpha': 5.0, 'adaptive_alpha': 0, 'max_updates_train': 25000, 'temperature_q': 5.0, 'temperature_p': 2.0, 'num_iter': 2, 'div': 'X', 'importance_metric': 'magnitude', 'smooth_scores': True, 'weighted_freezing': True, 'start_freezing': 6000}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-09 03:17:37 | INFO | fairseq.tasks.translation | [ar] dictionary: 12001 types
2023-05-09 03:17:37 | INFO | fairseq.tasks.translation | [en] dictionary: 12001 types
2023-05-09 03:17:38 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=12001, bias=False)
  )
)
2023-05-09 03:17:38 | INFO | fairseq_cli.train | task: Translation_Intra_Distillation
2023-05-09 03:17:38 | INFO | fairseq_cli.train | model: TransformerModel
2023-05-09 03:17:38 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-09 03:17:38 | INFO | fairseq_cli.train | num. shared model params: 43,832,320 (num. trained: 43,832,320)
2023-05-09 03:17:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-09 03:17:38 | INFO | fairseq.data.data_utils | loaded 6,352 examples from: data/data-bin-ar/valid.ar-en.ar
2023-05-09 03:17:38 | INFO | fairseq.data.data_utils | loaded 6,352 examples from: data/data-bin-ar/valid.ar-en.en
2023-05-09 03:17:38 | INFO | fairseq.tasks.translation | data/data-bin-ar valid ar-en 6352 examples
2023-05-09 03:17:39 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-09 03:17:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 03:17:39 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.347 GB ; name = Graphics Device                         
2023-05-09 03:17:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 03:17:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-05-09 03:17:39 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-05-09 03:17:39 | INFO | fairseq.trainer | Preparing to load checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 03:17:39 | INFO | fairseq.trainer | No existing checkpoint found /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 03:17:39 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-09 03:17:39 | INFO | fairseq.data.data_utils | loaded 139,748 examples from: data/data-bin-ar/train.ar-en.ar
2023-05-09 03:17:39 | INFO | fairseq.data.data_utils | loaded 139,748 examples from: data/data-bin-ar/train.ar-en.en
2023-05-09 03:17:39 | INFO | fairseq.tasks.translation | data/data-bin-ar train ar-en 139748 examples
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 03:17:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2023-05-09 03:17:39 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 03:17:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:17:40 | INFO | fairseq.trainer | begin training epoch 1
2023-05-09 03:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:17:52 | INFO | train_inner | epoch 001:    100 / 1001 loss=12.84, nll_loss=12.677, intra_distillation_loss=0, ppl=6547.1, wps=33302.6, ups=9.29, wpb=3585.9, bsz=131, num_updates=100, lr=1.25e-05, gnorm=3.709, train_wall=11, gb_free=76.9, wall=13
2023-05-09 03:18:03 | INFO | train_inner | epoch 001:    200 / 1001 loss=11.129, nll_loss=10.759, intra_distillation_loss=0, ppl=1733.26, wps=33558.1, ups=9.27, wpb=3619.1, bsz=131.5, num_updates=200, lr=2.5e-05, gnorm=1.705, train_wall=10, gb_free=76.7, wall=24
2023-05-09 03:18:14 | INFO | train_inner | epoch 001:    300 / 1001 loss=10.139, nll_loss=9.619, intra_distillation_loss=0, ppl=786.1, wps=32923.1, ups=9.21, wpb=3575, bsz=142.8, num_updates=300, lr=3.75e-05, gnorm=1.701, train_wall=10, gb_free=76.9, wall=35
2023-05-09 03:18:25 | INFO | train_inner | epoch 001:    400 / 1001 loss=9.641, nll_loss=8.991, intra_distillation_loss=0, ppl=508.73, wps=33130.6, ups=9.24, wpb=3587.4, bsz=140.2, num_updates=400, lr=5e-05, gnorm=1.416, train_wall=10, gb_free=76.9, wall=46
2023-05-09 03:18:36 | INFO | train_inner | epoch 001:    500 / 1001 loss=9.484, nll_loss=8.782, intra_distillation_loss=0, ppl=440.17, wps=31946.4, ups=9.15, wpb=3491.5, bsz=143, num_updates=500, lr=6.25e-05, gnorm=1.573, train_wall=10, gb_free=76.8, wall=57
2023-05-09 03:18:46 | INFO | train_inner | epoch 001:    600 / 1001 loss=9.18, nll_loss=8.43, intra_distillation_loss=0, ppl=344.99, wps=33302.7, ups=9.28, wpb=3589.2, bsz=149.3, num_updates=600, lr=7.5e-05, gnorm=1.671, train_wall=10, gb_free=77, wall=67
2023-05-09 03:18:57 | INFO | train_inner | epoch 001:    700 / 1001 loss=9.064, nll_loss=8.3, intra_distillation_loss=0, ppl=315.2, wps=33411.5, ups=9.09, wpb=3675.2, bsz=141.1, num_updates=700, lr=8.75e-05, gnorm=1.493, train_wall=10, gb_free=76.8, wall=78
2023-05-09 03:19:08 | INFO | train_inner | epoch 001:    800 / 1001 loss=8.782, nll_loss=7.981, intra_distillation_loss=0, ppl=252.61, wps=33412.9, ups=9.26, wpb=3608.8, bsz=149, num_updates=800, lr=0.0001, gnorm=1.567, train_wall=10, gb_free=76.9, wall=89
2023-05-09 03:19:19 | INFO | train_inner | epoch 001:    900 / 1001 loss=8.786, nll_loss=7.981, intra_distillation_loss=0, ppl=252.59, wps=33187.1, ups=9.37, wpb=3543.2, bsz=129.4, num_updates=900, lr=0.0001125, gnorm=1.795, train_wall=10, gb_free=76.8, wall=100
2023-05-09 03:19:29 | INFO | train_inner | epoch 001:   1000 / 1001 loss=8.554, nll_loss=7.72, intra_distillation_loss=0, ppl=210.81, wps=33774.8, ups=9.37, wpb=3602.9, bsz=138, num_updates=1000, lr=0.000125, gnorm=1.458, train_wall=10, gb_free=76.9, wall=111
2023-05-09 03:19:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:19:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:20:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.302 | nll_loss 7.397 | intra_distillation_loss 0 | ppl 168.52 | bleu 1.63 | wps 3812.6 | wpb 2727.5 | bsz 105.9 | num_updates 1001
2023-05-09 03:20:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1001 updates
2023-05-09 03:20:13 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:20:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 1 @ 1001 updates, score 1.63) (writing took 1.8403571880189702 seconds)
2023-05-09 03:20:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-09 03:20:15 | INFO | train | epoch 001 | loss 9.758 | nll_loss 9.122 | intra_distillation_loss 0 | ppl 557.11 | wps 23441.6 | ups 6.53 | wpb 3588.2 | bsz 139.6 | num_updates 1001 | lr 0.000125125 | gnorm 1.808 | train_wall 98 | gb_free 76.9 | wall 156
2023-05-09 03:20:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:20:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:20:15 | INFO | fairseq.trainer | begin training epoch 2
2023-05-09 03:20:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:20:25 | INFO | train_inner | epoch 002:     99 / 1001 loss=8.393, nll_loss=7.536, intra_distillation_loss=0, ppl=185.62, wps=6439.4, ups=1.78, wpb=3607.8, bsz=142.6, num_updates=1100, lr=0.0001375, gnorm=1.506, train_wall=10, gb_free=76.7, wall=167
2023-05-09 03:20:36 | INFO | train_inner | epoch 002:    199 / 1001 loss=8.283, nll_loss=7.408, intra_distillation_loss=0, ppl=169.78, wps=33218.3, ups=9.18, wpb=3617.1, bsz=151.4, num_updates=1200, lr=0.00015, gnorm=1.545, train_wall=10, gb_free=76.7, wall=177
2023-05-09 03:20:47 | INFO | train_inner | epoch 002:    299 / 1001 loss=8.179, nll_loss=7.29, intra_distillation_loss=0, ppl=156.5, wps=32924.8, ups=9.1, wpb=3616.7, bsz=142.2, num_updates=1300, lr=0.0001625, gnorm=1.328, train_wall=10, gb_free=77, wall=188
2023-05-09 03:20:58 | INFO | train_inner | epoch 002:    399 / 1001 loss=8.111, nll_loss=7.212, intra_distillation_loss=0, ppl=148.28, wps=33108.2, ups=9.34, wpb=3545.3, bsz=138, num_updates=1400, lr=0.000175, gnorm=1.35, train_wall=10, gb_free=76.8, wall=199
2023-05-09 03:21:09 | INFO | train_inner | epoch 002:    499 / 1001 loss=8.076, nll_loss=7.172, intra_distillation_loss=0, ppl=144.18, wps=32791.6, ups=9.14, wpb=3589.3, bsz=135, num_updates=1500, lr=0.0001875, gnorm=1.374, train_wall=10, gb_free=76.7, wall=210
2023-05-09 03:21:20 | INFO | train_inner | epoch 002:    599 / 1001 loss=7.946, nll_loss=7.025, intra_distillation_loss=0, ppl=130.21, wps=32944, ups=9.21, wpb=3575.1, bsz=148.1, num_updates=1600, lr=0.0002, gnorm=1.289, train_wall=10, gb_free=76.8, wall=221
2023-05-09 03:21:31 | INFO | train_inner | epoch 002:    699 / 1001 loss=7.883, nll_loss=6.952, intra_distillation_loss=0, ppl=123.85, wps=33273.7, ups=9.3, wpb=3578.8, bsz=138.9, num_updates=1700, lr=0.0002125, gnorm=1.331, train_wall=10, gb_free=76.8, wall=232
2023-05-09 03:21:41 | INFO | train_inner | epoch 002:    799 / 1001 loss=7.838, nll_loss=6.901, intra_distillation_loss=0, ppl=119.53, wps=33200.4, ups=9.25, wpb=3587.6, bsz=137.4, num_updates=1800, lr=0.000225, gnorm=1.271, train_wall=10, gb_free=76.9, wall=242
2023-05-09 03:21:52 | INFO | train_inner | epoch 002:    899 / 1001 loss=7.797, nll_loss=6.853, intra_distillation_loss=0, ppl=115.61, wps=33568.6, ups=9.22, wpb=3639.8, bsz=137.8, num_updates=1900, lr=0.0002375, gnorm=1.268, train_wall=10, gb_free=76.9, wall=253
2023-05-09 03:22:03 | INFO | train_inner | epoch 002:    999 / 1001 loss=7.847, nll_loss=6.909, intra_distillation_loss=0, ppl=120.18, wps=33682.9, ups=9.52, wpb=3538.6, bsz=126.8, num_updates=2000, lr=0.00025, gnorm=1.35, train_wall=10, gb_free=76.9, wall=264
2023-05-09 03:22:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:22:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:22:46 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.552 | nll_loss 6.513 | intra_distillation_loss 0 | ppl 91.34 | bleu 2.46 | wps 3850.7 | wpb 2727.5 | bsz 105.9 | num_updates 2002 | best_bleu 2.46
2023-05-09 03:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2002 updates
2023-05-09 03:22:46 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:22:47 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:22:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 2 @ 2002 updates, score 2.46) (writing took 1.8893118529813364 seconds)
2023-05-09 03:22:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-09 03:22:47 | INFO | train | epoch 002 | loss 8.036 | nll_loss 7.126 | intra_distillation_loss 0 | ppl 139.71 | wps 23480.6 | ups 6.54 | wpb 3588.2 | bsz 139.6 | num_updates 2002 | lr 0.00025025 | gnorm 1.361 | train_wall 98 | gb_free 77 | wall 309
2023-05-09 03:22:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:22:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:22:48 | INFO | fairseq.trainer | begin training epoch 3
2023-05-09 03:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:22:58 | INFO | train_inner | epoch 003:     98 / 1001 loss=7.707, nll_loss=6.751, intra_distillation_loss=0, ppl=107.74, wps=6256.8, ups=1.81, wpb=3460.7, bsz=132.6, num_updates=2100, lr=0.0002625, gnorm=1.261, train_wall=10, gb_free=76.8, wall=319
2023-05-09 03:23:09 | INFO | train_inner | epoch 003:    198 / 1001 loss=7.578, nll_loss=6.605, intra_distillation_loss=0, ppl=97.37, wps=33276.8, ups=9.23, wpb=3605.5, bsz=136.2, num_updates=2200, lr=0.000275, gnorm=1.129, train_wall=10, gb_free=76.8, wall=330
2023-05-09 03:23:20 | INFO | train_inner | epoch 003:    298 / 1001 loss=7.504, nll_loss=6.52, intra_distillation_loss=0, ppl=91.79, wps=33510.8, ups=9.14, wpb=3664.7, bsz=141.1, num_updates=2300, lr=0.0002875, gnorm=1.204, train_wall=10, gb_free=76.9, wall=341
2023-05-09 03:23:31 | INFO | train_inner | epoch 003:    398 / 1001 loss=7.584, nll_loss=6.611, intra_distillation_loss=0, ppl=97.75, wps=32966.6, ups=9.29, wpb=3550.5, bsz=126.9, num_updates=2400, lr=0.0003, gnorm=1.169, train_wall=10, gb_free=77, wall=352
2023-05-09 03:23:41 | INFO | train_inner | epoch 003:    498 / 1001 loss=7.508, nll_loss=6.525, intra_distillation_loss=0, ppl=92.07, wps=33304.6, ups=9.27, wpb=3594.5, bsz=139.8, num_updates=2500, lr=0.0003125, gnorm=1.245, train_wall=10, gb_free=76.9, wall=362
2023-05-09 03:23:52 | INFO | train_inner | epoch 003:    598 / 1001 loss=7.354, nll_loss=6.35, intra_distillation_loss=0, ppl=81.54, wps=33414.1, ups=9.12, wpb=3663.7, bsz=156.2, num_updates=2600, lr=0.000325, gnorm=1.264, train_wall=10, gb_free=76.9, wall=373
2023-05-09 03:24:03 | INFO | train_inner | epoch 003:    698 / 1001 loss=7.364, nll_loss=6.362, intra_distillation_loss=0, ppl=82.25, wps=33298.2, ups=9.22, wpb=3612.8, bsz=143.6, num_updates=2700, lr=0.0003375, gnorm=1.138, train_wall=10, gb_free=76.8, wall=384
2023-05-09 03:24:14 | INFO | train_inner | epoch 003:    798 / 1001 loss=7.389, nll_loss=6.388, intra_distillation_loss=0, ppl=83.78, wps=33192.6, ups=9.19, wpb=3610.6, bsz=133.1, num_updates=2800, lr=0.00035, gnorm=1.138, train_wall=10, gb_free=76.9, wall=395
2023-05-09 03:24:25 | INFO | train_inner | epoch 003:    898 / 1001 loss=7.305, nll_loss=6.293, intra_distillation_loss=0, ppl=78.43, wps=32906.3, ups=9.33, wpb=3525.5, bsz=143.5, num_updates=2900, lr=0.0003625, gnorm=1.214, train_wall=10, gb_free=77.1, wall=406
2023-05-09 03:24:36 | INFO | train_inner | epoch 003:    998 / 1001 loss=7.279, nll_loss=6.263, intra_distillation_loss=0, ppl=76.8, wps=33282.8, ups=9.32, wpb=3569.7, bsz=140.2, num_updates=3000, lr=0.000375, gnorm=1.105, train_wall=10, gb_free=76.8, wall=417
2023-05-09 03:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:24:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:25:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.105 | nll_loss 6.008 | intra_distillation_loss 0 | ppl 64.36 | bleu 3.42 | wps 3785 | wpb 2727.5 | bsz 105.9 | num_updates 3003 | best_bleu 3.42
2023-05-09 03:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3003 updates
2023-05-09 03:25:19 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:25:20 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:25:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 3 @ 3003 updates, score 3.42) (writing took 1.9744271569652483 seconds)
2023-05-09 03:25:21 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-09 03:25:21 | INFO | train | epoch 003 | loss 7.453 | nll_loss 6.463 | intra_distillation_loss 0 | ppl 88.2 | wps 23356.8 | ups 6.51 | wpb 3588.2 | bsz 139.6 | num_updates 3003 | lr 0.000375375 | gnorm 1.186 | train_wall 98 | gb_free 76.8 | wall 462
2023-05-09 03:25:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:25:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:25:21 | INFO | fairseq.trainer | begin training epoch 4
2023-05-09 03:25:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:25:32 | INFO | train_inner | epoch 004:     97 / 1001 loss=7.212, nll_loss=6.187, intra_distillation_loss=0, ppl=72.85, wps=6350.3, ups=1.78, wpb=3568.1, bsz=129.8, num_updates=3100, lr=0.0003875, gnorm=1.14, train_wall=10, gb_free=77.2, wall=473
2023-05-09 03:25:43 | INFO | train_inner | epoch 004:    197 / 1001 loss=7.169, nll_loss=6.139, intra_distillation_loss=0, ppl=70.49, wps=33281.8, ups=9.23, wpb=3604.1, bsz=133.4, num_updates=3200, lr=0.0004, gnorm=1.063, train_wall=10, gb_free=76.9, wall=484
2023-05-09 03:25:53 | INFO | train_inner | epoch 004:    297 / 1001 loss=7.216, nll_loss=6.189, intra_distillation_loss=0, ppl=72.96, wps=33093.6, ups=9.39, wpb=3523.1, bsz=127.2, num_updates=3300, lr=0.0004125, gnorm=1.165, train_wall=10, gb_free=76.7, wall=494
2023-05-09 03:26:04 | INFO | train_inner | epoch 004:    397 / 1001 loss=7.103, nll_loss=6.062, intra_distillation_loss=0, ppl=66.79, wps=33280.6, ups=9.23, wpb=3604.4, bsz=136.2, num_updates=3400, lr=0.000425, gnorm=1.101, train_wall=10, gb_free=77.1, wall=505
2023-05-09 03:26:15 | INFO | train_inner | epoch 004:    497 / 1001 loss=7.073, nll_loss=6.028, intra_distillation_loss=0, ppl=65.25, wps=33297.9, ups=9.22, wpb=3610.1, bsz=137.8, num_updates=3500, lr=0.0004375, gnorm=1.061, train_wall=10, gb_free=76.9, wall=516
2023-05-09 03:26:26 | INFO | train_inner | epoch 004:    597 / 1001 loss=7.097, nll_loss=6.054, intra_distillation_loss=0, ppl=66.43, wps=32939.2, ups=9.34, wpb=3525.9, bsz=135.8, num_updates=3600, lr=0.00045, gnorm=1.108, train_wall=10, gb_free=76.9, wall=527
2023-05-09 03:26:37 | INFO | train_inner | epoch 004:    697 / 1001 loss=6.958, nll_loss=5.896, intra_distillation_loss=0, ppl=59.56, wps=32753.7, ups=9.06, wpb=3615.9, bsz=156.2, num_updates=3700, lr=0.0004625, gnorm=1.153, train_wall=10, gb_free=76.8, wall=538
2023-05-09 03:26:47 | INFO | train_inner | epoch 004:    797 / 1001 loss=6.945, nll_loss=5.881, intra_distillation_loss=0, ppl=58.92, wps=33462, ups=9.25, wpb=3617.8, bsz=151.4, num_updates=3800, lr=0.000475, gnorm=1.181, train_wall=10, gb_free=76.9, wall=548
2023-05-09 03:26:58 | INFO | train_inner | epoch 004:    897 / 1001 loss=7.01, nll_loss=5.955, intra_distillation_loss=0, ppl=62.02, wps=33253.5, ups=9.19, wpb=3620.2, bsz=135.2, num_updates=3900, lr=0.0004875, gnorm=1.039, train_wall=10, gb_free=76.7, wall=559
2023-05-09 03:27:09 | INFO | train_inner | epoch 004:    997 / 1001 loss=6.866, nll_loss=5.791, intra_distillation_loss=0, ppl=55.35, wps=33290.9, ups=9.24, wpb=3603.5, bsz=153.5, num_updates=4000, lr=0.0005, gnorm=1.088, train_wall=10, gb_free=76.9, wall=570
2023-05-09 03:27:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:27:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:27:52 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.77 | nll_loss 5.637 | intra_distillation_loss 0 | ppl 49.75 | bleu 4.2 | wps 3855 | wpb 2727.5 | bsz 105.9 | num_updates 4004 | best_bleu 4.2
2023-05-09 03:27:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4004 updates
2023-05-09 03:27:52 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:27:53 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 4 @ 4004 updates, score 4.2) (writing took 1.7546141309430823 seconds)
2023-05-09 03:27:54 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-09 03:27:54 | INFO | train | epoch 004 | loss 7.064 | nll_loss 6.017 | intra_distillation_loss 0 | ppl 64.77 | wps 23518.9 | ups 6.55 | wpb 3588.2 | bsz 139.6 | num_updates 4004 | lr 0.00049975 | gnorm 1.11 | train_wall 98 | gb_free 77 | wall 615
2023-05-09 03:27:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:27:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:27:54 | INFO | fairseq.trainer | begin training epoch 5
2023-05-09 03:27:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:28:05 | INFO | train_inner | epoch 005:     96 / 1001 loss=6.746, nll_loss=5.655, intra_distillation_loss=0, ppl=50.38, wps=6582.9, ups=1.8, wpb=3653.5, bsz=161.6, num_updates=4100, lr=0.000493865, gnorm=1.064, train_wall=10, gb_free=76.9, wall=626
2023-05-09 03:28:15 | INFO | train_inner | epoch 005:    196 / 1001 loss=6.931, nll_loss=5.862, intra_distillation_loss=0, ppl=58.15, wps=32988.4, ups=9.42, wpb=3501.2, bsz=127, num_updates=4200, lr=0.00048795, gnorm=1.096, train_wall=10, gb_free=76.9, wall=636
2023-05-09 03:28:26 | INFO | train_inner | epoch 005:    296 / 1001 loss=6.761, nll_loss=5.67, intra_distillation_loss=0, ppl=50.9, wps=33499.1, ups=9.3, wpb=3601.7, bsz=139.6, num_updates=4300, lr=0.000482243, gnorm=1.018, train_wall=10, gb_free=76.8, wall=647
2023-05-09 03:28:37 | INFO | train_inner | epoch 005:    396 / 1001 loss=6.768, nll_loss=5.677, intra_distillation_loss=0, ppl=51.16, wps=33396.6, ups=9.13, wpb=3656.1, bsz=130.9, num_updates=4400, lr=0.000476731, gnorm=1.059, train_wall=10, gb_free=76.9, wall=658
2023-05-09 03:28:48 | INFO | train_inner | epoch 005:    496 / 1001 loss=6.721, nll_loss=5.623, intra_distillation_loss=0, ppl=49.29, wps=32880.1, ups=9.23, wpb=3564.1, bsz=134.6, num_updates=4500, lr=0.000471405, gnorm=1.095, train_wall=10, gb_free=76.8, wall=669
2023-05-09 03:28:59 | INFO | train_inner | epoch 005:    596 / 1001 loss=6.564, nll_loss=5.443, intra_distillation_loss=0, ppl=43.5, wps=33220.2, ups=9.27, wpb=3585.2, bsz=150.6, num_updates=4600, lr=0.000466252, gnorm=1.05, train_wall=10, gb_free=76.8, wall=680
2023-05-09 03:29:09 | INFO | train_inner | epoch 005:    696 / 1001 loss=6.598, nll_loss=5.483, intra_distillation_loss=0, ppl=44.72, wps=32634.1, ups=9.19, wpb=3552.9, bsz=139.4, num_updates=4700, lr=0.000461266, gnorm=1.112, train_wall=10, gb_free=76.9, wall=691
2023-05-09 03:29:20 | INFO | train_inner | epoch 005:    796 / 1001 loss=6.558, nll_loss=5.436, intra_distillation_loss=0, ppl=43.29, wps=32662.7, ups=9.17, wpb=3560.6, bsz=141, num_updates=4800, lr=0.000456435, gnorm=1.081, train_wall=10, gb_free=76.8, wall=701
2023-05-09 03:29:31 | INFO | train_inner | epoch 005:    896 / 1001 loss=6.605, nll_loss=5.489, intra_distillation_loss=0, ppl=44.9, wps=33390.3, ups=9.34, wpb=3575.6, bsz=128.8, num_updates=4900, lr=0.000451754, gnorm=1.061, train_wall=10, gb_free=77, wall=712
2023-05-09 03:29:42 | INFO | train_inner | epoch 005:    996 / 1001 loss=6.382, nll_loss=5.235, intra_distillation_loss=0, ppl=37.67, wps=33574.3, ups=9.21, wpb=3647.1, bsz=145.8, num_updates=5000, lr=0.000447214, gnorm=1.114, train_wall=10, gb_free=76.8, wall=723
2023-05-09 03:29:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:29:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:30:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.213 | nll_loss 4.984 | intra_distillation_loss 0 | ppl 31.65 | bleu 7.44 | wps 3819.6 | wpb 2727.5 | bsz 105.9 | num_updates 5005 | best_bleu 7.44
2023-05-09 03:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5005 updates
2023-05-09 03:30:25 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:30:26 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 5 @ 5005 updates, score 7.44) (writing took 1.853097100974992 seconds)
2023-05-09 03:30:27 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-09 03:30:27 | INFO | train | epoch 005 | loss 6.663 | nll_loss 5.557 | intra_distillation_loss 0 | ppl 47.07 | wps 23430.4 | ups 6.53 | wpb 3588.2 | bsz 139.6 | num_updates 5005 | lr 0.00044699 | gnorm 1.075 | train_wall 98 | gb_free 76.8 | wall 768
2023-05-09 03:30:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:30:27 | INFO | fairseq.trainer | begin training epoch 6
2023-05-09 03:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:30:38 | INFO | train_inner | epoch 006:     95 / 1001 loss=6.398, nll_loss=5.254, intra_distillation_loss=0, ppl=38.15, wps=6344.2, ups=1.8, wpb=3529.7, bsz=127.1, num_updates=5100, lr=0.000442807, gnorm=1.078, train_wall=10, gb_free=76.9, wall=779
2023-05-09 03:30:48 | INFO | train_inner | epoch 006:    195 / 1001 loss=6.323, nll_loss=5.167, intra_distillation_loss=0, ppl=35.91, wps=33524.8, ups=9.36, wpb=3579.9, bsz=128.9, num_updates=5200, lr=0.000438529, gnorm=1.136, train_wall=10, gb_free=76.9, wall=789
2023-05-09 03:30:59 | INFO | train_inner | epoch 006:    295 / 1001 loss=6.24, nll_loss=5.072, intra_distillation_loss=0, ppl=33.63, wps=33911.3, ups=9.19, wpb=3689.8, bsz=140.3, num_updates=5300, lr=0.000434372, gnorm=1.122, train_wall=10, gb_free=76.8, wall=800
2023-05-09 03:31:10 | INFO | train_inner | epoch 006:    395 / 1001 loss=6.252, nll_loss=5.083, intra_distillation_loss=0, ppl=33.9, wps=33285.1, ups=9.26, wpb=3596.4, bsz=131, num_updates=5400, lr=0.000430331, gnorm=1.206, train_wall=10, gb_free=76.9, wall=811
2023-05-09 03:31:21 | INFO | train_inner | epoch 006:    495 / 1001 loss=6.224, nll_loss=5.052, intra_distillation_loss=0, ppl=33.16, wps=32516.3, ups=9.22, wpb=3527.3, bsz=148.4, num_updates=5500, lr=0.000426401, gnorm=1.165, train_wall=10, gb_free=76.8, wall=822
2023-05-09 03:31:32 | INFO | train_inner | epoch 006:    595 / 1001 loss=6.125, nll_loss=4.938, intra_distillation_loss=0, ppl=30.65, wps=32569.7, ups=9.24, wpb=3523.7, bsz=143.4, num_updates=5600, lr=0.000422577, gnorm=1.174, train_wall=10, gb_free=77, wall=833
2023-05-09 03:31:43 | INFO | train_inner | epoch 006:    695 / 1001 loss=5.982, nll_loss=4.776, intra_distillation_loss=0, ppl=27.41, wps=32682.7, ups=9.14, wpb=3575.3, bsz=161.7, num_updates=5700, lr=0.000418854, gnorm=1.123, train_wall=10, gb_free=77, wall=844
2023-05-09 03:31:53 | INFO | train_inner | epoch 006:    795 / 1001 loss=6.16, nll_loss=4.977, intra_distillation_loss=0, ppl=31.49, wps=33406.3, ups=9.32, wpb=3585.3, bsz=132.7, num_updates=5800, lr=0.000415227, gnorm=1.195, train_wall=10, gb_free=76.8, wall=854
2023-05-09 03:32:04 | INFO | train_inner | epoch 006:    895 / 1001 loss=6.081, nll_loss=4.886, intra_distillation_loss=0, ppl=29.58, wps=33311.9, ups=9.16, wpb=3638, bsz=136.7, num_updates=5900, lr=0.000411693, gnorm=1.162, train_wall=10, gb_free=76.8, wall=865
2023-05-09 03:32:15 | INFO | train_inner | epoch 006:    995 / 1001 loss=5.944, nll_loss=4.731, intra_distillation_loss=0, ppl=26.56, wps=33425.4, ups=9.2, wpb=3633.7, bsz=144.1, num_updates=6000, lr=0.000408248, gnorm=1.123, train_wall=10, gb_free=77.1, wall=876
2023-05-09 03:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:32:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:33:00 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.776 | nll_loss 4.434 | intra_distillation_loss 0 | ppl 21.61 | bleu 11.91 | wps 3736.9 | wpb 2727.5 | bsz 105.9 | num_updates 6006 | best_bleu 11.91
2023-05-09 03:33:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6006 updates
2023-05-09 03:33:00 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:33:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 6 @ 6006 updates, score 11.91) (writing took 1.7627551309997216 seconds)
2023-05-09 03:33:01 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-09 03:33:01 | INFO | train | epoch 006 | loss 6.167 | nll_loss 4.987 | intra_distillation_loss 0 | ppl 31.71 | wps 23299.6 | ups 6.49 | wpb 3588.2 | bsz 139.6 | num_updates 6006 | lr 0.000408044 | gnorm 1.142 | train_wall 98 | gb_free 77.1 | wall 923
2023-05-09 03:33:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:33:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:33:01 | INFO | fairseq.trainer | begin training epoch 7
2023-05-09 03:33:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:33:14 | INFO | train_inner | epoch 007:     94 / 1001 loss=5.828, nll_loss=4.605, intra_distillation_loss=0, ppl=24.33, wps=6203.2, ups=1.71, wpb=3631.1, bsz=139.8, num_updates=6100, lr=0.000404888, gnorm=0, train_wall=12, gb_free=77, wall=935
2023-05-09 03:33:26 | INFO | train_inner | epoch 007:    194 / 1001 loss=5.764, nll_loss=4.532, intra_distillation_loss=0, ppl=23.14, wps=27625.2, ups=7.78, wpb=3551, bsz=139, num_updates=6200, lr=0.00040161, gnorm=0, train_wall=12, gb_free=77.1, wall=948
2023-05-09 03:33:39 | INFO | train_inner | epoch 007:    294 / 1001 loss=5.721, nll_loss=4.48, intra_distillation_loss=0, ppl=22.31, wps=28385.1, ups=7.85, wpb=3614.6, bsz=135.7, num_updates=6300, lr=0.00039841, gnorm=0, train_wall=12, gb_free=76.8, wall=960
2023-05-09 03:33:52 | INFO | train_inner | epoch 007:    394 / 1001 loss=5.75, nll_loss=4.511, intra_distillation_loss=0, ppl=22.8, wps=28095.2, ups=7.76, wpb=3619.4, bsz=136.5, num_updates=6400, lr=0.000395285, gnorm=0, train_wall=12, gb_free=76.8, wall=973
2023-05-09 03:34:05 | INFO | train_inner | epoch 007:    494 / 1001 loss=5.74, nll_loss=4.499, intra_distillation_loss=0, ppl=22.61, wps=27964.5, ups=7.89, wpb=3543.6, bsz=138.9, num_updates=6500, lr=0.000392232, gnorm=0, train_wall=12, gb_free=76.7, wall=986
2023-05-09 03:34:17 | INFO | train_inner | epoch 007:    594 / 1001 loss=5.69, nll_loss=4.44, intra_distillation_loss=0, ppl=21.7, wps=28490.7, ups=7.95, wpb=3582.4, bsz=136.8, num_updates=6600, lr=0.000389249, gnorm=0, train_wall=12, gb_free=77.2, wall=998
2023-05-09 03:34:30 | INFO | train_inner | epoch 007:    694 / 1001 loss=5.812, nll_loss=4.579, intra_distillation_loss=0, ppl=23.9, wps=27750.9, ups=7.9, wpb=3512.9, bsz=125.6, num_updates=6700, lr=0.000386334, gnorm=0, train_wall=12, gb_free=76.8, wall=1011
2023-05-09 03:34:43 | INFO | train_inner | epoch 007:    794 / 1001 loss=5.58, nll_loss=4.314, intra_distillation_loss=0, ppl=19.89, wps=27825.7, ups=7.79, wpb=3572.4, bsz=151.3, num_updates=6800, lr=0.000383482, gnorm=0, train_wall=12, gb_free=76.8, wall=1024
2023-05-09 03:34:56 | INFO | train_inner | epoch 007:    894 / 1001 loss=5.645, nll_loss=4.387, intra_distillation_loss=0, ppl=20.92, wps=28403.3, ups=7.8, wpb=3643.6, bsz=136.2, num_updates=6900, lr=0.000380693, gnorm=0, train_wall=12, gb_free=76.9, wall=1037
2023-05-09 03:35:09 | INFO | train_inner | epoch 007:    994 / 1001 loss=5.555, nll_loss=4.285, intra_distillation_loss=0, ppl=19.49, wps=28122.6, ups=7.78, wpb=3615, bsz=154.3, num_updates=7000, lr=0.000377964, gnorm=0, train_wall=12, gb_free=77.1, wall=1050
2023-05-09 03:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:35:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:35:49 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.549 | nll_loss 4.168 | intra_distillation_loss 0 | ppl 17.97 | bleu 14.92 | wps 4175.2 | wpb 2727.5 | bsz 105.9 | num_updates 7007 | best_bleu 14.92
2023-05-09 03:35:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7007 updates
2023-05-09 03:35:49 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:35:50 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:35:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 7 @ 7007 updates, score 14.92) (writing took 1.8481104000238702 seconds)
2023-05-09 03:35:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-09 03:35:51 | INFO | train | epoch 007 | loss 5.708 | nll_loss 4.463 | intra_distillation_loss 0 | ppl 22.05 | wps 21242.3 | ups 5.92 | wpb 3588.2 | bsz 139.6 | num_updates 7007 | lr 0.000377776 | gnorm 0 | train_wall 117 | gb_free 76.9 | wall 1092
2023-05-09 03:35:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:35:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:35:51 | INFO | fairseq.trainer | begin training epoch 8
2023-05-09 03:35:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:36:03 | INFO | train_inner | epoch 008:     93 / 1001 loss=5.555, nll_loss=4.285, intra_distillation_loss=0, ppl=19.49, wps=6697.6, ups=1.85, wpb=3624.3, bsz=142.6, num_updates=7100, lr=0.000375293, gnorm=0, train_wall=12, gb_free=76.9, wall=1104
2023-05-09 03:36:15 | INFO | train_inner | epoch 008:    193 / 1001 loss=5.425, nll_loss=4.135, intra_distillation_loss=0, ppl=17.57, wps=28199.1, ups=7.86, wpb=3588.2, bsz=156, num_updates=7200, lr=0.000372678, gnorm=0, train_wall=12, gb_free=76.9, wall=1116
2023-05-09 03:36:28 | INFO | train_inner | epoch 008:    293 / 1001 loss=5.513, nll_loss=4.234, intra_distillation_loss=0, ppl=18.81, wps=27807.4, ups=7.86, wpb=3539.6, bsz=139.3, num_updates=7300, lr=0.000370117, gnorm=0, train_wall=12, gb_free=76.9, wall=1129
2023-05-09 03:36:41 | INFO | train_inner | epoch 008:    393 / 1001 loss=5.496, nll_loss=4.214, intra_distillation_loss=0, ppl=18.55, wps=28545.6, ups=7.92, wpb=3602.4, bsz=134, num_updates=7400, lr=0.000367607, gnorm=0, train_wall=12, gb_free=77, wall=1142
2023-05-09 03:36:53 | INFO | train_inner | epoch 008:    493 / 1001 loss=5.492, nll_loss=4.209, intra_distillation_loss=0, ppl=18.5, wps=27945.6, ups=7.85, wpb=3558.3, bsz=133.9, num_updates=7500, lr=0.000365148, gnorm=0, train_wall=12, gb_free=76.9, wall=1155
2023-05-09 03:37:06 | INFO | train_inner | epoch 008:    593 / 1001 loss=5.413, nll_loss=4.12, intra_distillation_loss=0, ppl=17.38, wps=28077.1, ups=7.76, wpb=3619.7, bsz=150.3, num_updates=7600, lr=0.000362738, gnorm=0, train_wall=12, gb_free=76.8, wall=1167
2023-05-09 03:37:19 | INFO | train_inner | epoch 008:    693 / 1001 loss=5.536, nll_loss=4.257, intra_distillation_loss=0, ppl=19.12, wps=28267, ups=7.91, wpb=3575.4, bsz=127.4, num_updates=7700, lr=0.000360375, gnorm=0, train_wall=12, gb_free=77.1, wall=1180
2023-05-09 03:37:32 | INFO | train_inner | epoch 008:    793 / 1001 loss=5.486, nll_loss=4.202, intra_distillation_loss=0, ppl=18.4, wps=27860.6, ups=7.78, wpb=3579.8, bsz=140.8, num_updates=7800, lr=0.000358057, gnorm=0, train_wall=12, gb_free=77, wall=1193
2023-05-09 03:37:44 | INFO | train_inner | epoch 008:    893 / 1001 loss=5.497, nll_loss=4.213, intra_distillation_loss=0, ppl=18.55, wps=28155.2, ups=7.91, wpb=3557.9, bsz=135.4, num_updates=7900, lr=0.000355784, gnorm=0, train_wall=12, gb_free=77.1, wall=1206
2023-05-09 03:37:57 | INFO | train_inner | epoch 008:    993 / 1001 loss=5.397, nll_loss=4.1, intra_distillation_loss=0, ppl=17.15, wps=28200.2, ups=7.77, wpb=3628.7, bsz=138.5, num_updates=8000, lr=0.000353553, gnorm=0, train_wall=12, gb_free=76.9, wall=1218
2023-05-09 03:37:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:37:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:38:39 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.28 | nll_loss 3.832 | intra_distillation_loss 0 | ppl 14.24 | bleu 17.1 | wps 4082.9 | wpb 2727.5 | bsz 105.9 | num_updates 8008 | best_bleu 17.1
2023-05-09 03:38:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8008 updates
2023-05-09 03:38:39 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:38:39 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:38:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 8 @ 8008 updates, score 17.1) (writing took 1.7850000070175156 seconds)
2023-05-09 03:38:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-09 03:38:40 | INFO | train | epoch 008 | loss 5.48 | nll_loss 4.195 | intra_distillation_loss 0 | ppl 18.32 | wps 21147.7 | ups 5.89 | wpb 3588.2 | bsz 139.6 | num_updates 8008 | lr 0.000353377 | gnorm 0 | train_wall 117 | gb_free 77.1 | wall 1261
2023-05-09 03:38:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:38:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:38:40 | INFO | fairseq.trainer | begin training epoch 9
2023-05-09 03:38:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:38:52 | INFO | train_inner | epoch 009:     92 / 1001 loss=5.399, nll_loss=4.102, intra_distillation_loss=0, ppl=17.18, wps=6434.9, ups=1.82, wpb=3528.7, bsz=125.9, num_updates=8100, lr=0.000351364, gnorm=0, train_wall=12, gb_free=76.9, wall=1273
2023-05-09 03:39:05 | INFO | train_inner | epoch 009:    192 / 1001 loss=5.285, nll_loss=3.972, intra_distillation_loss=0, ppl=15.7, wps=27869.6, ups=7.74, wpb=3598.7, bsz=149, num_updates=8200, lr=0.000349215, gnorm=0, train_wall=12, gb_free=76.9, wall=1286
2023-05-09 03:39:18 | INFO | train_inner | epoch 009:    292 / 1001 loss=5.318, nll_loss=4.008, intra_distillation_loss=0, ppl=16.09, wps=28443.2, ups=7.81, wpb=3641.8, bsz=137.6, num_updates=8300, lr=0.000347105, gnorm=0, train_wall=12, gb_free=76.8, wall=1299
2023-05-09 03:39:31 | INFO | train_inner | epoch 009:    392 / 1001 loss=5.177, nll_loss=3.848, intra_distillation_loss=0, ppl=14.4, wps=28244.1, ups=7.71, wpb=3662.9, bsz=159.8, num_updates=8400, lr=0.000345033, gnorm=0, train_wall=12, gb_free=77.1, wall=1312
2023-05-09 03:39:44 | INFO | train_inner | epoch 009:    492 / 1001 loss=5.394, nll_loss=4.094, intra_distillation_loss=0, ppl=17.08, wps=27844.3, ups=7.82, wpb=3560.3, bsz=137.9, num_updates=8500, lr=0.000342997, gnorm=0, train_wall=12, gb_free=76.9, wall=1325
2023-05-09 03:39:56 | INFO | train_inner | epoch 009:    592 / 1001 loss=5.313, nll_loss=4.002, intra_distillation_loss=0, ppl=16.02, wps=28289.7, ups=7.82, wpb=3616.8, bsz=140.2, num_updates=8600, lr=0.000340997, gnorm=0, train_wall=12, gb_free=77.1, wall=1337
2023-05-09 03:40:09 | INFO | train_inner | epoch 009:    692 / 1001 loss=5.312, nll_loss=4, intra_distillation_loss=0, ppl=16.01, wps=28262.3, ups=7.81, wpb=3619, bsz=139.3, num_updates=8700, lr=0.000339032, gnorm=0, train_wall=12, gb_free=77, wall=1350
2023-05-09 03:40:22 | INFO | train_inner | epoch 009:    792 / 1001 loss=5.347, nll_loss=4.041, intra_distillation_loss=0, ppl=16.46, wps=27690.8, ups=7.88, wpb=3516.2, bsz=138, num_updates=8800, lr=0.0003371, gnorm=0, train_wall=12, gb_free=76.9, wall=1363
2023-05-09 03:40:35 | INFO | train_inner | epoch 009:    892 / 1001 loss=5.358, nll_loss=4.052, intra_distillation_loss=0, ppl=16.59, wps=28291.2, ups=7.88, wpb=3589, bsz=123.1, num_updates=8900, lr=0.000335201, gnorm=0, train_wall=12, gb_free=77, wall=1376
2023-05-09 03:40:47 | INFO | train_inner | epoch 009:    992 / 1001 loss=5.309, nll_loss=3.998, intra_distillation_loss=0, ppl=15.98, wps=27845.1, ups=7.85, wpb=3545.8, bsz=143, num_updates=9000, lr=0.000333333, gnorm=0, train_wall=12, gb_free=76.9, wall=1388
2023-05-09 03:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:40:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:41:29 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.159 | nll_loss 3.694 | intra_distillation_loss 0 | ppl 12.94 | bleu 18.59 | wps 4054.7 | wpb 2727.5 | bsz 105.9 | num_updates 9009 | best_bleu 18.59
2023-05-09 03:41:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9009 updates
2023-05-09 03:41:29 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:41:30 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:41:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 9 @ 9009 updates, score 18.59) (writing took 1.7765570040792227 seconds)
2023-05-09 03:41:31 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-09 03:41:31 | INFO | train | epoch 009 | loss 5.318 | nll_loss 4.009 | intra_distillation_loss 0 | ppl 16.09 | wps 21090.6 | ups 5.88 | wpb 3588.2 | bsz 139.6 | num_updates 9009 | lr 0.000333167 | gnorm 0 | train_wall 117 | gb_free 77 | wall 1432
2023-05-09 03:41:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:41:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:41:31 | INFO | fairseq.trainer | begin training epoch 10
2023-05-09 03:41:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:41:42 | INFO | train_inner | epoch 010:     91 / 1001 loss=5.159, nll_loss=3.827, intra_distillation_loss=0, ppl=14.19, wps=6651.6, ups=1.82, wpb=3663.1, bsz=140.3, num_updates=9100, lr=0.000331497, gnorm=0, train_wall=12, gb_free=76.9, wall=1443
2023-05-09 03:41:55 | INFO | train_inner | epoch 010:    191 / 1001 loss=5.207, nll_loss=3.88, intra_distillation_loss=0, ppl=14.73, wps=28300.7, ups=7.78, wpb=3636.4, bsz=136.8, num_updates=9200, lr=0.00032969, gnorm=0, train_wall=12, gb_free=76.8, wall=1456
2023-05-09 03:42:08 | INFO | train_inner | epoch 010:    291 / 1001 loss=5.177, nll_loss=3.845, intra_distillation_loss=0, ppl=14.37, wps=28095.1, ups=7.74, wpb=3629.2, bsz=135.6, num_updates=9300, lr=0.000327913, gnorm=0, train_wall=12, gb_free=76.8, wall=1469
2023-05-09 03:42:21 | INFO | train_inner | epoch 010:    391 / 1001 loss=5.172, nll_loss=3.84, intra_distillation_loss=0, ppl=14.32, wps=27974.4, ups=7.82, wpb=3575.4, bsz=141, num_updates=9400, lr=0.000326164, gnorm=0, train_wall=12, gb_free=76.9, wall=1482
2023-05-09 03:42:34 | INFO | train_inner | epoch 010:    491 / 1001 loss=5.228, nll_loss=3.903, intra_distillation_loss=0, ppl=14.96, wps=28321.6, ups=7.93, wpb=3573.2, bsz=132.5, num_updates=9500, lr=0.000324443, gnorm=0, train_wall=12, gb_free=76.9, wall=1495
2023-05-09 03:42:46 | INFO | train_inner | epoch 010:    591 / 1001 loss=5.162, nll_loss=3.828, intra_distillation_loss=0, ppl=14.2, wps=28634.8, ups=7.85, wpb=3648.8, bsz=141.1, num_updates=9600, lr=0.000322749, gnorm=0, train_wall=12, gb_free=77, wall=1507
2023-05-09 03:42:59 | INFO | train_inner | epoch 010:    691 / 1001 loss=5.189, nll_loss=3.859, intra_distillation_loss=0, ppl=14.51, wps=28004.2, ups=7.91, wpb=3540.8, bsz=133.6, num_updates=9700, lr=0.000321081, gnorm=0, train_wall=12, gb_free=77, wall=1520
2023-05-09 03:43:12 | INFO | train_inner | epoch 010:    791 / 1001 loss=5.167, nll_loss=3.834, intra_distillation_loss=0, ppl=14.26, wps=28055.1, ups=7.83, wpb=3582.4, bsz=146.9, num_updates=9800, lr=0.000319438, gnorm=0, train_wall=12, gb_free=76.9, wall=1533
2023-05-09 03:43:24 | INFO | train_inner | epoch 010:    891 / 1001 loss=5.311, nll_loss=3.998, intra_distillation_loss=0, ppl=15.98, wps=27426.8, ups=7.9, wpb=3471.3, bsz=129.5, num_updates=9900, lr=0.000317821, gnorm=0, train_wall=12, gb_free=77.1, wall=1545
2023-05-09 03:43:37 | INFO | train_inner | epoch 010:    991 / 1001 loss=5.119, nll_loss=3.78, intra_distillation_loss=0, ppl=13.74, wps=27836.6, ups=7.78, wpb=3579.9, bsz=159.8, num_updates=10000, lr=0.000316228, gnorm=0, train_wall=12, gb_free=76.8, wall=1558
2023-05-09 03:43:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:43:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:44:18 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.086 | nll_loss 3.613 | intra_distillation_loss 0 | ppl 12.24 | bleu 19.24 | wps 4152.1 | wpb 2727.5 | bsz 105.9 | num_updates 10010 | best_bleu 19.24
2023-05-09 03:44:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 10010 updates
2023-05-09 03:44:18 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:44:19 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 10 @ 10010 updates, score 19.24) (writing took 1.95317558106035 seconds)
2023-05-09 03:44:20 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-09 03:44:20 | INFO | train | epoch 010 | loss 5.19 | nll_loss 3.86 | intra_distillation_loss 0 | ppl 14.52 | wps 21219.6 | ups 5.91 | wpb 3588.2 | bsz 139.6 | num_updates 10010 | lr 0.00031607 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 1601
2023-05-09 03:44:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:44:20 | INFO | fairseq.trainer | begin training epoch 11
2023-05-09 03:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:44:32 | INFO | train_inner | epoch 011:     90 / 1001 loss=5.131, nll_loss=3.794, intra_distillation_loss=0, ppl=13.87, wps=6562.4, ups=1.84, wpb=3559.8, bsz=134.3, num_updates=10100, lr=0.000314658, gnorm=0, train_wall=12, gb_free=76.8, wall=1613
2023-05-09 03:44:44 | INFO | train_inner | epoch 011:    190 / 1001 loss=5.127, nll_loss=3.788, intra_distillation_loss=0, ppl=13.81, wps=28139, ups=7.92, wpb=3553, bsz=134.6, num_updates=10200, lr=0.000313112, gnorm=0, train_wall=12, gb_free=76.9, wall=1625
2023-05-09 03:44:57 | INFO | train_inner | epoch 011:    290 / 1001 loss=5.028, nll_loss=3.677, intra_distillation_loss=0, ppl=12.79, wps=28242.3, ups=7.79, wpb=3623.5, bsz=156.6, num_updates=10300, lr=0.000311588, gnorm=0, train_wall=12, gb_free=77.1, wall=1638
2023-05-09 03:45:10 | INFO | train_inner | epoch 011:    390 / 1001 loss=5.075, nll_loss=3.728, intra_distillation_loss=0, ppl=13.25, wps=28018.1, ups=7.79, wpb=3596.8, bsz=142.5, num_updates=10400, lr=0.000310087, gnorm=0, train_wall=12, gb_free=77, wall=1651
2023-05-09 03:45:22 | INFO | train_inner | epoch 011:    490 / 1001 loss=5.12, nll_loss=3.78, intra_distillation_loss=0, ppl=13.74, wps=27843.3, ups=7.88, wpb=3535.2, bsz=135.6, num_updates=10500, lr=0.000308607, gnorm=0, train_wall=12, gb_free=76.9, wall=1664
2023-05-09 03:45:35 | INFO | train_inner | epoch 011:    590 / 1001 loss=5.006, nll_loss=3.649, intra_distillation_loss=0, ppl=12.55, wps=28455.5, ups=7.84, wpb=3631.4, bsz=148.8, num_updates=10600, lr=0.000307148, gnorm=0, train_wall=12, gb_free=76.9, wall=1676
2023-05-09 03:45:48 | INFO | train_inner | epoch 011:    690 / 1001 loss=5.098, nll_loss=3.755, intra_distillation_loss=0, ppl=13.5, wps=27860.9, ups=7.77, wpb=3587.7, bsz=139.8, num_updates=10700, lr=0.000305709, gnorm=0, train_wall=12, gb_free=76.9, wall=1689
2023-05-09 03:46:01 | INFO | train_inner | epoch 011:    790 / 1001 loss=5.117, nll_loss=3.775, intra_distillation_loss=0, ppl=13.69, wps=28389.4, ups=7.82, wpb=3629.5, bsz=129.2, num_updates=10800, lr=0.00030429, gnorm=0, train_wall=12, gb_free=77, wall=1702
2023-05-09 03:46:14 | INFO | train_inner | epoch 011:    890 / 1001 loss=5.118, nll_loss=3.777, intra_distillation_loss=0, ppl=13.71, wps=27891.8, ups=7.8, wpb=3574.5, bsz=139, num_updates=10900, lr=0.000302891, gnorm=0, train_wall=12, gb_free=76.8, wall=1715
2023-05-09 03:46:26 | INFO | train_inner | epoch 011:    990 / 1001 loss=5.074, nll_loss=3.726, intra_distillation_loss=0, ppl=13.24, wps=27872.6, ups=7.85, wpb=3549.1, bsz=138.1, num_updates=11000, lr=0.000301511, gnorm=0, train_wall=12, gb_free=76.9, wall=1728
2023-05-09 03:46:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:46:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:47:10 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.975 | nll_loss 3.481 | intra_distillation_loss 0 | ppl 11.17 | bleu 21.27 | wps 3874 | wpb 2727.5 | bsz 105.9 | num_updates 11011 | best_bleu 21.27
2023-05-09 03:47:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 11011 updates
2023-05-09 03:47:10 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:47:11 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:47:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 11 @ 11011 updates, score 21.27) (writing took 1.9369962239870802 seconds)
2023-05-09 03:47:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-09 03:47:12 | INFO | train | epoch 011 | loss 5.087 | nll_loss 3.742 | intra_distillation_loss 0 | ppl 13.38 | wps 20863 | ups 5.81 | wpb 3588.2 | bsz 139.6 | num_updates 11011 | lr 0.000301361 | gnorm 0 | train_wall 117 | gb_free 76.9 | wall 1773
2023-05-09 03:47:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:47:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:47:12 | INFO | fairseq.trainer | begin training epoch 12
2023-05-09 03:47:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:47:23 | INFO | train_inner | epoch 012:     89 / 1001 loss=5.038, nll_loss=3.686, intra_distillation_loss=0, ppl=12.87, wps=6296.3, ups=1.75, wpb=3590.8, bsz=120, num_updates=11100, lr=0.00030015, gnorm=0, train_wall=12, gb_free=76.8, wall=1785
2023-05-09 03:47:36 | INFO | train_inner | epoch 012:    189 / 1001 loss=4.946, nll_loss=3.582, intra_distillation_loss=0, ppl=11.98, wps=28195.2, ups=7.84, wpb=3595.4, bsz=151.9, num_updates=11200, lr=0.000298807, gnorm=0, train_wall=12, gb_free=76.9, wall=1797
2023-05-09 03:47:49 | INFO | train_inner | epoch 012:    289 / 1001 loss=4.9, nll_loss=3.529, intra_distillation_loss=0, ppl=11.54, wps=28509.8, ups=7.74, wpb=3682.5, bsz=153.6, num_updates=11300, lr=0.000297482, gnorm=0, train_wall=12, gb_free=76.8, wall=1810
2023-05-09 03:48:02 | INFO | train_inner | epoch 012:    389 / 1001 loss=5.024, nll_loss=3.67, intra_distillation_loss=0, ppl=12.73, wps=28051.1, ups=7.77, wpb=3611.8, bsz=142.5, num_updates=11400, lr=0.000296174, gnorm=0, train_wall=12, gb_free=77.1, wall=1823
2023-05-09 03:48:15 | INFO | train_inner | epoch 012:    489 / 1001 loss=5.006, nll_loss=3.65, intra_distillation_loss=0, ppl=12.55, wps=27978.6, ups=7.75, wpb=3610.2, bsz=143.3, num_updates=11500, lr=0.000294884, gnorm=0, train_wall=12, gb_free=76.8, wall=1836
2023-05-09 03:48:28 | INFO | train_inner | epoch 012:    589 / 1001 loss=5.029, nll_loss=3.675, intra_distillation_loss=0, ppl=12.78, wps=27992.7, ups=7.85, wpb=3564.8, bsz=138.6, num_updates=11600, lr=0.00029361, gnorm=0, train_wall=12, gb_free=76.9, wall=1849
2023-05-09 03:48:40 | INFO | train_inner | epoch 012:    689 / 1001 loss=5.02, nll_loss=3.665, intra_distillation_loss=0, ppl=12.69, wps=28097.4, ups=7.86, wpb=3575.8, bsz=136, num_updates=11700, lr=0.000292353, gnorm=0, train_wall=12, gb_free=76.8, wall=1861
2023-05-09 03:48:53 | INFO | train_inner | epoch 012:    789 / 1001 loss=4.962, nll_loss=3.6, intra_distillation_loss=0, ppl=12.12, wps=28165.2, ups=7.87, wpb=3580.2, bsz=144.1, num_updates=11800, lr=0.000291111, gnorm=0, train_wall=12, gb_free=77.1, wall=1874
2023-05-09 03:49:06 | INFO | train_inner | epoch 012:    889 / 1001 loss=5.065, nll_loss=3.717, intra_distillation_loss=0, ppl=13.15, wps=28104.9, ups=7.86, wpb=3575.4, bsz=130.5, num_updates=11900, lr=0.000289886, gnorm=0, train_wall=12, gb_free=76.7, wall=1887
2023-05-09 03:49:19 | INFO | train_inner | epoch 012:    989 / 1001 loss=4.999, nll_loss=3.641, intra_distillation_loss=0, ppl=12.47, wps=27874.8, ups=7.84, wpb=3554.4, bsz=135.8, num_updates=12000, lr=0.000288675, gnorm=0, train_wall=12, gb_free=76.9, wall=1900
2023-05-09 03:49:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:49:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:49:58 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.949 | nll_loss 3.454 | intra_distillation_loss 0 | ppl 10.96 | bleu 20.74 | wps 4373.8 | wpb 2727.5 | bsz 105.9 | num_updates 12012 | best_bleu 21.27
2023-05-09 03:49:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 12012 updates
2023-05-09 03:49:58 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 03:49:58 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 03:49:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt (epoch 12 @ 12012 updates, score 20.74) (writing took 0.9228342139394954 seconds)
2023-05-09 03:49:58 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-09 03:49:58 | INFO | train | epoch 012 | loss 5 | nll_loss 3.642 | intra_distillation_loss 0 | ppl 12.49 | wps 21591.2 | ups 6.02 | wpb 3588.2 | bsz 139.6 | num_updates 12012 | lr 0.000288531 | gnorm 0 | train_wall 117 | gb_free 77.1 | wall 1940
2023-05-09 03:49:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:49:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:49:59 | INFO | fairseq.trainer | begin training epoch 13
2023-05-09 03:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:50:10 | INFO | train_inner | epoch 013:     88 / 1001 loss=4.971, nll_loss=3.611, intra_distillation_loss=0, ppl=12.22, wps=6906.1, ups=1.95, wpb=3541.3, bsz=131, num_updates=12100, lr=0.00028748, gnorm=0, train_wall=12, gb_free=76.9, wall=1951
2023-05-09 03:50:23 | INFO | train_inner | epoch 013:    188 / 1001 loss=4.95, nll_loss=3.586, intra_distillation_loss=0, ppl=12.01, wps=27875, ups=7.88, wpb=3539.5, bsz=138, num_updates=12200, lr=0.000286299, gnorm=0, train_wall=12, gb_free=76.9, wall=1964
2023-05-09 03:50:35 | INFO | train_inner | epoch 013:    288 / 1001 loss=4.918, nll_loss=3.548, intra_distillation_loss=0, ppl=11.69, wps=28079.7, ups=7.88, wpb=3563.9, bsz=142, num_updates=12300, lr=0.000285133, gnorm=0, train_wall=12, gb_free=76.9, wall=1976
2023-05-09 03:50:48 | INFO | train_inner | epoch 013:    388 / 1001 loss=4.848, nll_loss=3.469, intra_distillation_loss=0, ppl=11.08, wps=28271.7, ups=7.77, wpb=3636.5, bsz=148, num_updates=12400, lr=0.000283981, gnorm=0, train_wall=12, gb_free=76.8, wall=1989
2023-05-09 03:51:01 | INFO | train_inner | epoch 013:    488 / 1001 loss=4.906, nll_loss=3.535, intra_distillation_loss=0, ppl=11.59, wps=28277.3, ups=7.9, wpb=3577.2, bsz=138.2, num_updates=12500, lr=0.000282843, gnorm=0, train_wall=12, gb_free=76.7, wall=2002
2023-05-09 03:51:14 | INFO | train_inner | epoch 013:    588 / 1001 loss=4.91, nll_loss=3.54, intra_distillation_loss=0, ppl=11.63, wps=28091.3, ups=7.74, wpb=3628.8, bsz=146.1, num_updates=12600, lr=0.000281718, gnorm=0, train_wall=12, gb_free=77, wall=2015
2023-05-09 03:51:27 | INFO | train_inner | epoch 013:    688 / 1001 loss=4.937, nll_loss=3.571, intra_distillation_loss=0, ppl=11.88, wps=27876.2, ups=7.79, wpb=3577.2, bsz=138.6, num_updates=12700, lr=0.000280607, gnorm=0, train_wall=12, gb_free=76.8, wall=2028
2023-05-09 03:51:39 | INFO | train_inner | epoch 013:    788 / 1001 loss=4.923, nll_loss=3.554, intra_distillation_loss=0, ppl=11.74, wps=28021.7, ups=7.83, wpb=3577, bsz=138, num_updates=12800, lr=0.000279508, gnorm=0, train_wall=12, gb_free=76.8, wall=2040
2023-05-09 03:51:52 | INFO | train_inner | epoch 013:    888 / 1001 loss=5.024, nll_loss=3.67, intra_distillation_loss=0, ppl=12.73, wps=27922.9, ups=7.91, wpb=3528.8, bsz=131.4, num_updates=12900, lr=0.000278423, gnorm=0, train_wall=12, gb_free=77.3, wall=2053
2023-05-09 03:52:05 | INFO | train_inner | epoch 013:    988 / 1001 loss=4.914, nll_loss=3.545, intra_distillation_loss=0, ppl=11.68, wps=28460.2, ups=7.76, wpb=3665.8, bsz=145, num_updates=13000, lr=0.00027735, gnorm=0, train_wall=12, gb_free=76.7, wall=2066
2023-05-09 03:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:52:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:52:45 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.867 | nll_loss 3.359 | intra_distillation_loss 0 | ppl 10.26 | bleu 22.05 | wps 4270.7 | wpb 2727.5 | bsz 105.9 | num_updates 13013 | best_bleu 22.05
2023-05-09 03:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 13013 updates
2023-05-09 03:52:45 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 13 @ 13013 updates, score 22.05) (writing took 1.8958475129911676 seconds)
2023-05-09 03:52:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-09 03:52:47 | INFO | train | epoch 013 | loss 4.927 | nll_loss 3.559 | intra_distillation_loss 0 | ppl 11.79 | wps 21341.6 | ups 5.95 | wpb 3588.2 | bsz 139.6 | num_updates 13013 | lr 0.000277212 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2108
2023-05-09 03:52:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:52:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:52:47 | INFO | fairseq.trainer | begin training epoch 14
2023-05-09 03:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:52:58 | INFO | train_inner | epoch 014:     87 / 1001 loss=4.826, nll_loss=3.444, intra_distillation_loss=0, ppl=10.88, wps=6793.9, ups=1.88, wpb=3620.1, bsz=142, num_updates=13100, lr=0.000276289, gnorm=0, train_wall=12, gb_free=77.1, wall=2119
2023-05-09 03:53:11 | INFO | train_inner | epoch 014:    187 / 1001 loss=4.869, nll_loss=3.494, intra_distillation_loss=0, ppl=11.27, wps=27739.2, ups=7.86, wpb=3527.1, bsz=139.1, num_updates=13200, lr=0.000275241, gnorm=0, train_wall=12, gb_free=76.8, wall=2132
2023-05-09 03:53:24 | INFO | train_inner | epoch 014:    287 / 1001 loss=4.92, nll_loss=3.551, intra_distillation_loss=0, ppl=11.72, wps=27972.5, ups=7.77, wpb=3599, bsz=136.2, num_updates=13300, lr=0.000274204, gnorm=0, train_wall=12, gb_free=76.8, wall=2145
2023-05-09 03:53:36 | INFO | train_inner | epoch 014:    387 / 1001 loss=4.93, nll_loss=3.561, intra_distillation_loss=0, ppl=11.8, wps=28336.7, ups=7.93, wpb=3573.4, bsz=113.6, num_updates=13400, lr=0.000273179, gnorm=0, train_wall=12, gb_free=77, wall=2157
2023-05-09 03:53:49 | INFO | train_inner | epoch 014:    487 / 1001 loss=4.83, nll_loss=3.449, intra_distillation_loss=0, ppl=10.92, wps=28224.7, ups=7.85, wpb=3594.3, bsz=144.7, num_updates=13500, lr=0.000272166, gnorm=0, train_wall=12, gb_free=77.1, wall=2170
2023-05-09 03:54:02 | INFO | train_inner | epoch 014:    587 / 1001 loss=4.793, nll_loss=3.406, intra_distillation_loss=0, ppl=10.6, wps=28231.3, ups=7.77, wpb=3631.7, bsz=152.4, num_updates=13600, lr=0.000271163, gnorm=0, train_wall=12, gb_free=76.9, wall=2183
2023-05-09 03:54:15 | INFO | train_inner | epoch 014:    687 / 1001 loss=4.857, nll_loss=3.48, intra_distillation_loss=0, ppl=11.16, wps=27935.9, ups=7.85, wpb=3557.3, bsz=141, num_updates=13700, lr=0.000270172, gnorm=0, train_wall=12, gb_free=77, wall=2196
2023-05-09 03:54:27 | INFO | train_inner | epoch 014:    787 / 1001 loss=4.873, nll_loss=3.497, intra_distillation_loss=0, ppl=11.29, wps=28078.5, ups=7.81, wpb=3593.9, bsz=138.8, num_updates=13800, lr=0.000269191, gnorm=0, train_wall=12, gb_free=76.9, wall=2208
2023-05-09 03:54:40 | INFO | train_inner | epoch 014:    887 / 1001 loss=4.845, nll_loss=3.466, intra_distillation_loss=0, ppl=11.05, wps=28179.1, ups=7.87, wpb=3580.9, bsz=140.2, num_updates=13900, lr=0.000268221, gnorm=0, train_wall=12, gb_free=76.9, wall=2221
2023-05-09 03:54:53 | INFO | train_inner | epoch 014:    987 / 1001 loss=4.82, nll_loss=3.437, intra_distillation_loss=0, ppl=10.83, wps=28325, ups=7.8, wpb=3631.1, bsz=147.9, num_updates=14000, lr=0.000267261, gnorm=0, train_wall=12, gb_free=76.9, wall=2234
2023-05-09 03:54:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:54:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:55:34 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.837 | nll_loss 3.316 | intra_distillation_loss 0 | ppl 9.96 | bleu 23 | wps 4155.6 | wpb 2727.5 | bsz 105.9 | num_updates 14014 | best_bleu 23
2023-05-09 03:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 14014 updates
2023-05-09 03:55:34 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:55:35 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:55:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 14 @ 14014 updates, score 23.0) (writing took 1.8333630169508979 seconds)
2023-05-09 03:55:36 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-09 03:55:36 | INFO | train | epoch 014 | loss 4.858 | nll_loss 3.48 | intra_distillation_loss 0 | ppl 11.16 | wps 21229.8 | ups 5.92 | wpb 3588.2 | bsz 139.6 | num_updates 14014 | lr 0.000267128 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2277
2023-05-09 03:55:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:55:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:55:36 | INFO | fairseq.trainer | begin training epoch 15
2023-05-09 03:55:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:55:47 | INFO | train_inner | epoch 015:     86 / 1001 loss=4.777, nll_loss=3.39, intra_distillation_loss=0, ppl=10.48, wps=6551.2, ups=1.85, wpb=3548.6, bsz=143.3, num_updates=14100, lr=0.000266312, gnorm=0, train_wall=12, gb_free=76.8, wall=2288
2023-05-09 03:56:00 | INFO | train_inner | epoch 015:    186 / 1001 loss=4.845, nll_loss=3.465, intra_distillation_loss=0, ppl=11.05, wps=27822.2, ups=7.92, wpb=3514.9, bsz=127.6, num_updates=14200, lr=0.000265372, gnorm=0, train_wall=12, gb_free=76.8, wall=2301
2023-05-09 03:56:12 | INFO | train_inner | epoch 015:    286 / 1001 loss=4.81, nll_loss=3.426, intra_distillation_loss=0, ppl=10.75, wps=27748.2, ups=7.9, wpb=3511.1, bsz=138.2, num_updates=14300, lr=0.000264443, gnorm=0, train_wall=12, gb_free=76.8, wall=2313
2023-05-09 03:56:25 | INFO | train_inner | epoch 015:    386 / 1001 loss=4.822, nll_loss=3.438, intra_distillation_loss=0, ppl=10.84, wps=28243.1, ups=7.88, wpb=3584.1, bsz=128.1, num_updates=14400, lr=0.000263523, gnorm=0, train_wall=12, gb_free=76.8, wall=2326
2023-05-09 03:56:38 | INFO | train_inner | epoch 015:    486 / 1001 loss=4.766, nll_loss=3.375, intra_distillation_loss=0, ppl=10.38, wps=28095.9, ups=7.76, wpb=3621.8, bsz=149.5, num_updates=14500, lr=0.000262613, gnorm=0, train_wall=12, gb_free=76.9, wall=2339
2023-05-09 03:56:51 | INFO | train_inner | epoch 015:    586 / 1001 loss=4.848, nll_loss=3.469, intra_distillation_loss=0, ppl=11.07, wps=27766.7, ups=7.83, wpb=3547.5, bsz=132.4, num_updates=14600, lr=0.000261712, gnorm=0, train_wall=12, gb_free=76.8, wall=2352
2023-05-09 03:57:04 | INFO | train_inner | epoch 015:    686 / 1001 loss=4.833, nll_loss=3.451, intra_distillation_loss=0, ppl=10.93, wps=28409.5, ups=7.84, wpb=3623.5, bsz=131, num_updates=14700, lr=0.00026082, gnorm=0, train_wall=12, gb_free=76.9, wall=2365
2023-05-09 03:57:16 | INFO | train_inner | epoch 015:    786 / 1001 loss=4.754, nll_loss=3.363, intra_distillation_loss=0, ppl=10.29, wps=28433.6, ups=7.77, wpb=3658.3, bsz=155.8, num_updates=14800, lr=0.000259938, gnorm=0, train_wall=12, gb_free=76.8, wall=2377
2023-05-09 03:57:29 | INFO | train_inner | epoch 015:    886 / 1001 loss=4.782, nll_loss=3.394, intra_distillation_loss=0, ppl=10.51, wps=28046.9, ups=7.75, wpb=3619.1, bsz=149.8, num_updates=14900, lr=0.000259064, gnorm=0, train_wall=12, gb_free=77.1, wall=2390
2023-05-09 03:57:42 | INFO | train_inner | epoch 015:    986 / 1001 loss=4.802, nll_loss=3.417, intra_distillation_loss=0, ppl=10.68, wps=28319.7, ups=7.85, wpb=3607.5, bsz=140, num_updates=15000, lr=0.000258199, gnorm=0, train_wall=12, gb_free=77.1, wall=2403
2023-05-09 03:57:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:57:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:58:22 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.805 | nll_loss 3.286 | intra_distillation_loss 0 | ppl 9.75 | bleu 23.22 | wps 4263.8 | wpb 2727.5 | bsz 105.9 | num_updates 15015 | best_bleu 23.22
2023-05-09 03:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 15015 updates
2023-05-09 03:58:22 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 03:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 15 @ 15015 updates, score 23.22) (writing took 1.9151932860258967 seconds)
2023-05-09 03:58:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-09 03:58:24 | INFO | train | epoch 015 | loss 4.8 | nll_loss 3.414 | intra_distillation_loss 0 | ppl 10.66 | wps 21332.5 | ups 5.95 | wpb 3588.2 | bsz 139.6 | num_updates 15015 | lr 0.00025807 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2445
2023-05-09 03:58:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:58:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 03:58:24 | INFO | fairseq.trainer | begin training epoch 16
2023-05-09 03:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:58:35 | INFO | train_inner | epoch 016:     85 / 1001 loss=4.712, nll_loss=3.314, intra_distillation_loss=0, ppl=9.94, wps=6893.2, ups=1.88, wpb=3669.7, bsz=139.8, num_updates=15100, lr=0.000257343, gnorm=0, train_wall=12, gb_free=76.9, wall=2456
2023-05-09 03:58:48 | INFO | train_inner | epoch 016:    185 / 1001 loss=4.835, nll_loss=3.452, intra_distillation_loss=0, ppl=10.95, wps=28244.3, ups=7.9, wpb=3574.1, bsz=112.3, num_updates=15200, lr=0.000256495, gnorm=0, train_wall=12, gb_free=77.1, wall=2469
2023-05-09 03:59:01 | INFO | train_inner | epoch 016:    285 / 1001 loss=4.739, nll_loss=3.345, intra_distillation_loss=0, ppl=10.16, wps=28196.1, ups=7.84, wpb=3598.3, bsz=138.6, num_updates=15300, lr=0.000255655, gnorm=0, train_wall=12, gb_free=77.1, wall=2482
2023-05-09 03:59:14 | INFO | train_inner | epoch 016:    385 / 1001 loss=4.664, nll_loss=3.26, intra_distillation_loss=0, ppl=9.58, wps=27995.7, ups=7.76, wpb=3608, bsz=155, num_updates=15400, lr=0.000254824, gnorm=0, train_wall=12, gb_free=77, wall=2495
2023-05-09 03:59:26 | INFO | train_inner | epoch 016:    485 / 1001 loss=4.755, nll_loss=3.363, intra_distillation_loss=0, ppl=10.29, wps=28061, ups=7.84, wpb=3577.1, bsz=137, num_updates=15500, lr=0.000254, gnorm=0, train_wall=12, gb_free=77.1, wall=2507
2023-05-09 03:59:39 | INFO | train_inner | epoch 016:    585 / 1001 loss=4.719, nll_loss=3.322, intra_distillation_loss=0, ppl=10, wps=28139.5, ups=7.84, wpb=3589.7, bsz=151.4, num_updates=15600, lr=0.000253185, gnorm=0, train_wall=12, gb_free=77.4, wall=2520
2023-05-09 03:59:52 | INFO | train_inner | epoch 016:    685 / 1001 loss=4.816, nll_loss=3.431, intra_distillation_loss=0, ppl=10.79, wps=28072.7, ups=7.84, wpb=3578.7, bsz=128.2, num_updates=15700, lr=0.000252377, gnorm=0, train_wall=12, gb_free=76.9, wall=2533
2023-05-09 04:00:05 | INFO | train_inner | epoch 016:    785 / 1001 loss=4.794, nll_loss=3.408, intra_distillation_loss=0, ppl=10.61, wps=28114.6, ups=7.86, wpb=3577.3, bsz=138.3, num_updates=15800, lr=0.000251577, gnorm=0, train_wall=12, gb_free=77.1, wall=2546
2023-05-09 04:00:17 | INFO | train_inner | epoch 016:    885 / 1001 loss=4.775, nll_loss=3.386, intra_distillation_loss=0, ppl=10.45, wps=27628.9, ups=7.79, wpb=3544.9, bsz=135.8, num_updates=15900, lr=0.000250785, gnorm=0, train_wall=12, gb_free=76.9, wall=2558
2023-05-09 04:00:30 | INFO | train_inner | epoch 016:    985 / 1001 loss=4.709, nll_loss=3.312, intra_distillation_loss=0, ppl=9.93, wps=27695.8, ups=7.75, wpb=3575.3, bsz=158.2, num_updates=16000, lr=0.00025, gnorm=0, train_wall=12, gb_free=77, wall=2571
2023-05-09 04:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:00:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:01:12 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.746 | nll_loss 3.209 | intra_distillation_loss 0 | ppl 9.25 | bleu 24.08 | wps 4076.5 | wpb 2727.5 | bsz 105.9 | num_updates 16016 | best_bleu 24.08
2023-05-09 04:01:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 16016 updates
2023-05-09 04:01:12 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 16 @ 16016 updates, score 24.08) (writing took 1.7801992089953274 seconds)
2023-05-09 04:01:14 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-09 04:01:14 | INFO | train | epoch 016 | loss 4.751 | nll_loss 3.358 | intra_distillation_loss 0 | ppl 10.25 | wps 21133 | ups 5.89 | wpb 3588.2 | bsz 139.6 | num_updates 16016 | lr 0.000249875 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2615
2023-05-09 04:01:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:01:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:01:14 | INFO | fairseq.trainer | begin training epoch 17
2023-05-09 04:01:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:01:25 | INFO | train_inner | epoch 017:     84 / 1001 loss=4.728, nll_loss=3.333, intra_distillation_loss=0, ppl=10.08, wps=6481.1, ups=1.82, wpb=3554.2, bsz=130.2, num_updates=16100, lr=0.000249222, gnorm=0, train_wall=12, gb_free=76.8, wall=2626
2023-05-09 04:01:38 | INFO | train_inner | epoch 017:    184 / 1001 loss=4.562, nll_loss=3.144, intra_distillation_loss=0, ppl=8.84, wps=28318.7, ups=7.84, wpb=3611.2, bsz=157.2, num_updates=16200, lr=0.000248452, gnorm=0, train_wall=12, gb_free=76.9, wall=2639
2023-05-09 04:01:51 | INFO | train_inner | epoch 017:    284 / 1001 loss=4.641, nll_loss=3.234, intra_distillation_loss=0, ppl=9.41, wps=27579, ups=7.78, wpb=3545.9, bsz=148.9, num_updates=16300, lr=0.000247689, gnorm=0, train_wall=12, gb_free=76.8, wall=2652
2023-05-09 04:02:04 | INFO | train_inner | epoch 017:    384 / 1001 loss=4.678, nll_loss=3.274, intra_distillation_loss=0, ppl=9.68, wps=28205.6, ups=7.76, wpb=3635.9, bsz=143.9, num_updates=16400, lr=0.000246932, gnorm=0, train_wall=12, gb_free=76.8, wall=2665
2023-05-09 04:02:16 | INFO | train_inner | epoch 017:    484 / 1001 loss=4.753, nll_loss=3.361, intra_distillation_loss=0, ppl=10.27, wps=27901.6, ups=7.95, wpb=3510.8, bsz=135.8, num_updates=16500, lr=0.000246183, gnorm=0, train_wall=12, gb_free=76.9, wall=2677
2023-05-09 04:02:29 | INFO | train_inner | epoch 017:    584 / 1001 loss=4.705, nll_loss=3.305, intra_distillation_loss=0, ppl=9.88, wps=28351.8, ups=7.8, wpb=3632.9, bsz=139.8, num_updates=16600, lr=0.00024544, gnorm=0, train_wall=12, gb_free=76.7, wall=2690
2023-05-09 04:02:42 | INFO | train_inner | epoch 017:    684 / 1001 loss=4.7, nll_loss=3.299, intra_distillation_loss=0, ppl=9.84, wps=28418.9, ups=7.84, wpb=3625.1, bsz=138.2, num_updates=16700, lr=0.000244704, gnorm=0, train_wall=12, gb_free=76.7, wall=2703
2023-05-09 04:02:55 | INFO | train_inner | epoch 017:    784 / 1001 loss=4.723, nll_loss=3.327, intra_distillation_loss=0, ppl=10.03, wps=28154, ups=7.74, wpb=3636.9, bsz=133.5, num_updates=16800, lr=0.000243975, gnorm=0, train_wall=12, gb_free=76.9, wall=2716
2023-05-09 04:03:08 | INFO | train_inner | epoch 017:    884 / 1001 loss=4.692, nll_loss=3.292, intra_distillation_loss=0, ppl=9.79, wps=28121, ups=7.79, wpb=3609.2, bsz=154.6, num_updates=16900, lr=0.000243252, gnorm=0, train_wall=12, gb_free=76.8, wall=2729
2023-05-09 04:03:20 | INFO | train_inner | epoch 017:    984 / 1001 loss=4.821, nll_loss=3.438, intra_distillation_loss=0, ppl=10.84, wps=28089.1, ups=7.93, wpb=3544.1, bsz=116.8, num_updates=17000, lr=0.000242536, gnorm=0, train_wall=12, gb_free=76.8, wall=2741
2023-05-09 04:03:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:03:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:04:03 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.723 | nll_loss 3.183 | intra_distillation_loss 0 | ppl 9.08 | bleu 23.96 | wps 4050 | wpb 2727.5 | bsz 105.9 | num_updates 17017 | best_bleu 24.08
2023-05-09 04:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 17017 updates
2023-05-09 04:04:03 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:04:04 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:04:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt (epoch 17 @ 17017 updates, score 23.96) (writing took 0.9107472019968554 seconds)
2023-05-09 04:04:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-09 04:04:04 | INFO | train | epoch 017 | loss 4.702 | nll_loss 3.303 | intra_distillation_loss 0 | ppl 9.87 | wps 21213 | ups 5.91 | wpb 3588.2 | bsz 139.6 | num_updates 17017 | lr 0.000242414 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2785
2023-05-09 04:04:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:04:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:04:04 | INFO | fairseq.trainer | begin training epoch 18
2023-05-09 04:04:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:04:14 | INFO | train_inner | epoch 018:     83 / 1001 loss=4.74, nll_loss=3.346, intra_distillation_loss=0, ppl=10.17, wps=6503.9, ups=1.85, wpb=3514.6, bsz=120.2, num_updates=17100, lr=0.000241825, gnorm=0, train_wall=12, gb_free=76.7, wall=2795
2023-05-09 04:04:27 | INFO | train_inner | epoch 018:    183 / 1001 loss=4.667, nll_loss=3.262, intra_distillation_loss=0, ppl=9.59, wps=28182.9, ups=7.9, wpb=3569.4, bsz=125, num_updates=17200, lr=0.000241121, gnorm=0, train_wall=12, gb_free=77, wall=2808
2023-05-09 04:04:40 | INFO | train_inner | epoch 018:    283 / 1001 loss=4.715, nll_loss=3.317, intra_distillation_loss=0, ppl=9.97, wps=27845.2, ups=7.9, wpb=3526.7, bsz=131.2, num_updates=17300, lr=0.000240424, gnorm=0, train_wall=12, gb_free=76.8, wall=2821
2023-05-09 04:04:52 | INFO | train_inner | epoch 018:    383 / 1001 loss=4.618, nll_loss=3.206, intra_distillation_loss=0, ppl=9.23, wps=28364.8, ups=7.81, wpb=3632.2, bsz=149.7, num_updates=17400, lr=0.000239732, gnorm=0, train_wall=12, gb_free=76.9, wall=2833
2023-05-09 04:05:05 | INFO | train_inner | epoch 018:    483 / 1001 loss=4.606, nll_loss=3.194, intra_distillation_loss=0, ppl=9.15, wps=28242.2, ups=7.71, wpb=3661.4, bsz=154.7, num_updates=17500, lr=0.000239046, gnorm=0, train_wall=12, gb_free=76.8, wall=2846
2023-05-09 04:05:18 | INFO | train_inner | epoch 018:    583 / 1001 loss=4.643, nll_loss=3.235, intra_distillation_loss=0, ppl=9.41, wps=28351.4, ups=7.8, wpb=3634, bsz=144.2, num_updates=17600, lr=0.000238366, gnorm=0, train_wall=12, gb_free=76.9, wall=2859
2023-05-09 04:05:31 | INFO | train_inner | epoch 018:    683 / 1001 loss=4.551, nll_loss=3.131, intra_distillation_loss=0, ppl=8.76, wps=28374.4, ups=7.72, wpb=3675.1, bsz=166.5, num_updates=17700, lr=0.000237691, gnorm=0, train_wall=12, gb_free=77, wall=2872
2023-05-09 04:05:44 | INFO | train_inner | epoch 018:    783 / 1001 loss=4.732, nll_loss=3.336, intra_distillation_loss=0, ppl=10.1, wps=27787.8, ups=7.93, wpb=3503.3, bsz=132.9, num_updates=17800, lr=0.000237023, gnorm=0, train_wall=12, gb_free=76.9, wall=2885
2023-05-09 04:05:56 | INFO | train_inner | epoch 018:    883 / 1001 loss=4.681, nll_loss=3.279, intra_distillation_loss=0, ppl=9.71, wps=27961.7, ups=7.85, wpb=3560.1, bsz=144.2, num_updates=17900, lr=0.00023636, gnorm=0, train_wall=12, gb_free=76.9, wall=2897
2023-05-09 04:06:09 | INFO | train_inner | epoch 018:    983 / 1001 loss=4.721, nll_loss=3.324, intra_distillation_loss=0, ppl=10.01, wps=27557.7, ups=7.78, wpb=3543.7, bsz=125.5, num_updates=18000, lr=0.000235702, gnorm=0, train_wall=12, gb_free=76.9, wall=2910
2023-05-09 04:06:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:06:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:06:51 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.682 | nll_loss 3.139 | intra_distillation_loss 0 | ppl 8.81 | bleu 25 | wps 4123.9 | wpb 2727.5 | bsz 105.9 | num_updates 18018 | best_bleu 25
2023-05-09 04:06:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 18018 updates
2023-05-09 04:06:51 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:06:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 18 @ 18018 updates, score 25.0) (writing took 1.7724483161000535 seconds)
2023-05-09 04:06:53 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-09 04:06:53 | INFO | train | epoch 018 | loss 4.661 | nll_loss 3.256 | intra_distillation_loss 0 | ppl 9.55 | wps 21197.4 | ups 5.91 | wpb 3588.2 | bsz 139.6 | num_updates 18018 | lr 0.000235584 | gnorm 0 | train_wall 117 | gb_free 76.7 | wall 2954
2023-05-09 04:06:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:06:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:06:53 | INFO | fairseq.trainer | begin training epoch 19
2023-05-09 04:06:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:07:04 | INFO | train_inner | epoch 019:     82 / 1001 loss=4.549, nll_loss=3.128, intra_distillation_loss=0, ppl=8.74, wps=6797.7, ups=1.84, wpb=3701.7, bsz=138.6, num_updates=18100, lr=0.00023505, gnorm=0, train_wall=12, gb_free=76.8, wall=2965
2023-05-09 04:07:17 | INFO | train_inner | epoch 019:    182 / 1001 loss=4.608, nll_loss=3.195, intra_distillation_loss=0, ppl=9.16, wps=27685.3, ups=7.78, wpb=3559.4, bsz=145.2, num_updates=18200, lr=0.000234404, gnorm=0, train_wall=12, gb_free=76.9, wall=2978
2023-05-09 04:07:29 | INFO | train_inner | epoch 019:    282 / 1001 loss=4.558, nll_loss=3.138, intra_distillation_loss=0, ppl=8.8, wps=28171.9, ups=7.78, wpb=3621.3, bsz=142.6, num_updates=18300, lr=0.000233762, gnorm=0, train_wall=12, gb_free=76.9, wall=2990
2023-05-09 04:07:42 | INFO | train_inner | epoch 019:    382 / 1001 loss=4.651, nll_loss=3.244, intra_distillation_loss=0, ppl=9.48, wps=27477.7, ups=7.88, wpb=3486.9, bsz=137.1, num_updates=18400, lr=0.000233126, gnorm=0, train_wall=12, gb_free=76.9, wall=3003
2023-05-09 04:07:55 | INFO | train_inner | epoch 019:    482 / 1001 loss=4.604, nll_loss=3.192, intra_distillation_loss=0, ppl=9.14, wps=28294.3, ups=7.77, wpb=3639.2, bsz=147.3, num_updates=18500, lr=0.000232495, gnorm=0, train_wall=12, gb_free=77, wall=3016
2023-05-09 04:08:08 | INFO | train_inner | epoch 019:    582 / 1001 loss=4.661, nll_loss=3.255, intra_distillation_loss=0, ppl=9.55, wps=27922.5, ups=7.93, wpb=3522.9, bsz=134.5, num_updates=18600, lr=0.000231869, gnorm=0, train_wall=12, gb_free=77.1, wall=3029
2023-05-09 04:08:20 | INFO | train_inner | epoch 019:    682 / 1001 loss=4.676, nll_loss=3.272, intra_distillation_loss=0, ppl=9.66, wps=28285.8, ups=7.86, wpb=3597.6, bsz=130.2, num_updates=18700, lr=0.000231249, gnorm=0, train_wall=12, gb_free=76.9, wall=3041
2023-05-09 04:08:33 | INFO | train_inner | epoch 019:    782 / 1001 loss=4.565, nll_loss=3.147, intra_distillation_loss=0, ppl=8.86, wps=27659.9, ups=7.72, wpb=3582.4, bsz=155.5, num_updates=18800, lr=0.000230633, gnorm=0, train_wall=12, gb_free=76.7, wall=3054
2023-05-09 04:08:46 | INFO | train_inner | epoch 019:    882 / 1001 loss=4.602, nll_loss=3.19, intra_distillation_loss=0, ppl=9.12, wps=28134.1, ups=7.84, wpb=3588.1, bsz=145, num_updates=18900, lr=0.000230022, gnorm=0, train_wall=12, gb_free=76.9, wall=3067
2023-05-09 04:08:59 | INFO | train_inner | epoch 019:    982 / 1001 loss=4.702, nll_loss=3.301, intra_distillation_loss=0, ppl=9.86, wps=28599.2, ups=7.87, wpb=3635.8, bsz=121.9, num_updates=19000, lr=0.000229416, gnorm=0, train_wall=12, gb_free=76.8, wall=3080
2023-05-09 04:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:09:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:09:40 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.66 | nll_loss 3.117 | intra_distillation_loss 0 | ppl 8.68 | bleu 24.89 | wps 4207.4 | wpb 2727.5 | bsz 105.9 | num_updates 19019 | best_bleu 25
2023-05-09 04:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 19019 updates
2023-05-09 04:09:40 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:09:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_last.pt (epoch 19 @ 19019 updates, score 24.89) (writing took 1.029620170011185 seconds)
2023-05-09 04:09:41 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-09 04:09:41 | INFO | train | epoch 019 | loss 4.621 | nll_loss 3.211 | intra_distillation_loss 0 | ppl 9.26 | wps 21369.9 | ups 5.96 | wpb 3588.2 | bsz 139.6 | num_updates 19019 | lr 0.000229301 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 3122
2023-05-09 04:09:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:09:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:09:41 | INFO | fairseq.trainer | begin training epoch 20
2023-05-09 04:09:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:09:52 | INFO | train_inner | epoch 020:     81 / 1001 loss=4.619, nll_loss=3.208, intra_distillation_loss=0, ppl=9.24, wps=6802.5, ups=1.89, wpb=3595.2, bsz=128.4, num_updates=19100, lr=0.000228814, gnorm=0, train_wall=12, gb_free=76.8, wall=3133
2023-05-09 04:10:04 | INFO | train_inner | epoch 020:    181 / 1001 loss=4.563, nll_loss=3.144, intra_distillation_loss=0, ppl=8.84, wps=27886.7, ups=7.91, wpb=3526.9, bsz=136, num_updates=19200, lr=0.000228218, gnorm=0, train_wall=12, gb_free=76.8, wall=3145
2023-05-09 04:10:17 | INFO | train_inner | epoch 020:    281 / 1001 loss=4.545, nll_loss=3.124, intra_distillation_loss=0, ppl=8.72, wps=28217.6, ups=7.83, wpb=3605, bsz=142.3, num_updates=19300, lr=0.000227626, gnorm=0, train_wall=12, gb_free=77.1, wall=3158
2023-05-09 04:10:30 | INFO | train_inner | epoch 020:    381 / 1001 loss=4.547, nll_loss=3.126, intra_distillation_loss=0, ppl=8.73, wps=27965.5, ups=7.81, wpb=3582.4, bsz=150.6, num_updates=19400, lr=0.000227038, gnorm=0, train_wall=12, gb_free=76.8, wall=3171
2023-05-09 04:10:43 | INFO | train_inner | epoch 020:    481 / 1001 loss=4.626, nll_loss=3.216, intra_distillation_loss=0, ppl=9.29, wps=28075.5, ups=7.78, wpb=3610.6, bsz=139, num_updates=19500, lr=0.000226455, gnorm=0, train_wall=12, gb_free=76.8, wall=3184
2023-05-09 04:10:55 | INFO | train_inner | epoch 020:    581 / 1001 loss=4.548, nll_loss=3.127, intra_distillation_loss=0, ppl=8.74, wps=28908.1, ups=7.84, wpb=3685, bsz=142.6, num_updates=19600, lr=0.000225877, gnorm=0, train_wall=12, gb_free=76.8, wall=3196
2023-05-09 04:11:08 | INFO | train_inner | epoch 020:    681 / 1001 loss=4.591, nll_loss=3.176, intra_distillation_loss=0, ppl=9.04, wps=28137.8, ups=7.78, wpb=3617.3, bsz=134.6, num_updates=19700, lr=0.000225303, gnorm=0, train_wall=12, gb_free=76.8, wall=3209
2023-05-09 04:11:21 | INFO | train_inner | epoch 020:    781 / 1001 loss=4.574, nll_loss=3.157, intra_distillation_loss=0, ppl=8.92, wps=28214, ups=7.81, wpb=3613.3, bsz=142.7, num_updates=19800, lr=0.000224733, gnorm=0, train_wall=12, gb_free=77.1, wall=3222
2023-05-09 04:11:34 | INFO | train_inner | epoch 020:    881 / 1001 loss=4.641, nll_loss=3.233, intra_distillation_loss=0, ppl=9.4, wps=27719.5, ups=7.98, wpb=3474.7, bsz=139.2, num_updates=19900, lr=0.000224168, gnorm=0, train_wall=11, gb_free=77.1, wall=3235
2023-05-09 04:11:46 | INFO | train_inner | epoch 020:    981 / 1001 loss=4.621, nll_loss=3.211, intra_distillation_loss=0, ppl=9.26, wps=27724.9, ups=7.8, wpb=3553.6, bsz=138.3, num_updates=20000, lr=0.000223607, gnorm=0, train_wall=12, gb_free=76.9, wall=3248
2023-05-09 04:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:11:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:12:29 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.643 | nll_loss 3.093 | intra_distillation_loss 0 | ppl 8.53 | bleu 25.29 | wps 4102.6 | wpb 2727.5 | bsz 105.9 | num_updates 20020 | best_bleu 25.29
2023-05-09 04:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 20020 updates
2023-05-09 04:12:29 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:12:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_ar/checkpoint_best.pt (epoch 20 @ 20020 updates, score 25.29) (writing took 1.853107167989947 seconds)
2023-05-09 04:12:31 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-09 04:12:31 | INFO | train | epoch 020 | loss 4.584 | nll_loss 3.169 | intra_distillation_loss 0 | ppl 8.99 | wps 21168.1 | ups 5.9 | wpb 3588.2 | bsz 139.6 | num_updates 20020 | lr 0.000223495 | gnorm 0 | train_wall 117 | gb_free 76.9 | wall 3292
2023-05-09 04:12:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:12:31 | INFO | fairseq_cli.train | done training in 3290.5 seconds
/home/tli104/my_fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-05-09 04:12:43 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-05-09 04:12:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/scratch4/cs601/tli104/wf_8000/checkpoints_ar', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_alpha=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, alpha=5.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/data-bin-ar', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, div='X', dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, importance_metric='magnitude', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_updates_train=25000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_iter=2, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch4/cs601/tli104/wf_8000/checkpoints_ar', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, smooth_scores=True, source_lang=None, start_freezing=8000, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation_intra_distillation', temperature_p=2, temperature_q=5, tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weighted_freezing=True, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation_intra_distillation', 'data': 'data/data-bin-ar', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False, 'alpha': 5.0, 'adaptive_alpha': 0, 'max_updates_train': 25000, 'temperature_q': 5.0, 'temperature_p': 2.0, 'num_iter': 2, 'div': 'X', 'importance_metric': 'magnitude', 'smooth_scores': True, 'weighted_freezing': True, 'start_freezing': 8000}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-09 04:12:46 | INFO | fairseq.tasks.translation | [ar] dictionary: 12001 types
2023-05-09 04:12:46 | INFO | fairseq.tasks.translation | [en] dictionary: 12001 types
2023-05-09 04:12:46 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=12001, bias=False)
  )
)
2023-05-09 04:12:46 | INFO | fairseq_cli.train | task: Translation_Intra_Distillation
2023-05-09 04:12:46 | INFO | fairseq_cli.train | model: TransformerModel
2023-05-09 04:12:46 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-09 04:12:46 | INFO | fairseq_cli.train | num. shared model params: 43,832,320 (num. trained: 43,832,320)
2023-05-09 04:12:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-09 04:12:46 | INFO | fairseq.data.data_utils | loaded 6,352 examples from: data/data-bin-ar/valid.ar-en.ar
2023-05-09 04:12:46 | INFO | fairseq.data.data_utils | loaded 6,352 examples from: data/data-bin-ar/valid.ar-en.en
2023-05-09 04:12:46 | INFO | fairseq.tasks.translation | data/data-bin-ar valid ar-en 6352 examples
2023-05-09 04:12:47 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-09 04:12:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 04:12:47 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.347 GB ; name = Graphics Device                         
2023-05-09 04:12:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 04:12:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-05-09 04:12:47 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-05-09 04:12:47 | INFO | fairseq.trainer | Preparing to load checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:12:47 | INFO | fairseq.trainer | No existing checkpoint found /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:12:47 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-09 04:12:47 | INFO | fairseq.data.data_utils | loaded 139,748 examples from: data/data-bin-ar/train.ar-en.ar
2023-05-09 04:12:47 | INFO | fairseq.data.data_utils | loaded 139,748 examples from: data/data-bin-ar/train.ar-en.en
2023-05-09 04:12:47 | INFO | fairseq.tasks.translation | data/data-bin-ar train ar-en 139748 examples
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 04:12:47 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2023-05-09 04:12:47 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 04:12:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 04:12:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:12:49 | INFO | fairseq.trainer | begin training epoch 1
2023-05-09 04:12:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:13:01 | INFO | train_inner | epoch 001:    100 / 1001 loss=12.84, nll_loss=12.677, intra_distillation_loss=0, ppl=6547.1, wps=33306.7, ups=9.29, wpb=3585.9, bsz=131, num_updates=100, lr=1.25e-05, gnorm=3.709, train_wall=11, gb_free=76.9, wall=13
2023-05-09 04:13:12 | INFO | train_inner | epoch 001:    200 / 1001 loss=11.129, nll_loss=10.759, intra_distillation_loss=0, ppl=1733.26, wps=33536.9, ups=9.27, wpb=3619.1, bsz=131.5, num_updates=200, lr=2.5e-05, gnorm=1.705, train_wall=10, gb_free=76.7, wall=24
2023-05-09 04:13:22 | INFO | train_inner | epoch 001:    300 / 1001 loss=10.139, nll_loss=9.619, intra_distillation_loss=0, ppl=786.1, wps=32969.9, ups=9.22, wpb=3575, bsz=142.8, num_updates=300, lr=3.75e-05, gnorm=1.701, train_wall=10, gb_free=76.9, wall=35
2023-05-09 04:13:33 | INFO | train_inner | epoch 001:    400 / 1001 loss=9.641, nll_loss=8.991, intra_distillation_loss=0, ppl=508.73, wps=33096.3, ups=9.23, wpb=3587.4, bsz=140.2, num_updates=400, lr=5e-05, gnorm=1.416, train_wall=10, gb_free=76.9, wall=46
2023-05-09 04:13:44 | INFO | train_inner | epoch 001:    500 / 1001 loss=9.484, nll_loss=8.782, intra_distillation_loss=0, ppl=440.17, wps=31968.8, ups=9.16, wpb=3491.5, bsz=143, num_updates=500, lr=6.25e-05, gnorm=1.573, train_wall=10, gb_free=76.8, wall=57
2023-05-09 04:13:55 | INFO | train_inner | epoch 001:    600 / 1001 loss=9.18, nll_loss=8.43, intra_distillation_loss=0, ppl=344.99, wps=33311.4, ups=9.28, wpb=3589.2, bsz=149.3, num_updates=600, lr=7.5e-05, gnorm=1.671, train_wall=10, gb_free=77, wall=67
2023-05-09 04:14:06 | INFO | train_inner | epoch 001:    700 / 1001 loss=9.064, nll_loss=8.3, intra_distillation_loss=0, ppl=315.2, wps=33384.8, ups=9.08, wpb=3675.2, bsz=141.1, num_updates=700, lr=8.75e-05, gnorm=1.493, train_wall=10, gb_free=76.8, wall=79
2023-05-09 04:14:17 | INFO | train_inner | epoch 001:    800 / 1001 loss=8.782, nll_loss=7.981, intra_distillation_loss=0, ppl=252.61, wps=33436.5, ups=9.27, wpb=3608.8, bsz=149, num_updates=800, lr=0.0001, gnorm=1.567, train_wall=10, gb_free=76.9, wall=89
2023-05-09 04:14:27 | INFO | train_inner | epoch 001:    900 / 1001 loss=8.786, nll_loss=7.981, intra_distillation_loss=0, ppl=252.59, wps=33164.2, ups=9.36, wpb=3543.2, bsz=129.4, num_updates=900, lr=0.0001125, gnorm=1.795, train_wall=10, gb_free=76.8, wall=100
2023-05-09 04:14:38 | INFO | train_inner | epoch 001:   1000 / 1001 loss=8.554, nll_loss=7.72, intra_distillation_loss=0, ppl=210.81, wps=33723.6, ups=9.36, wpb=3602.9, bsz=138, num_updates=1000, lr=0.000125, gnorm=1.458, train_wall=10, gb_free=76.9, wall=111
2023-05-09 04:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:14:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:15:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.302 | nll_loss 7.397 | intra_distillation_loss 0 | ppl 168.52 | bleu 1.63 | wps 3805.3 | wpb 2727.5 | bsz 105.9 | num_updates 1001
2023-05-09 04:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1001 updates
2023-05-09 04:15:21 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:15:22 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:15:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 1 @ 1001 updates, score 1.63) (writing took 1.8532736239721999 seconds)
2023-05-09 04:15:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-09 04:15:23 | INFO | train | epoch 001 | loss 9.758 | nll_loss 9.122 | intra_distillation_loss 0 | ppl 557.11 | wps 23424.3 | ups 6.53 | wpb 3588.2 | bsz 139.6 | num_updates 1001 | lr 0.000125125 | gnorm 1.808 | train_wall 98 | gb_free 76.9 | wall 156
2023-05-09 04:15:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:15:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:15:23 | INFO | fairseq.trainer | begin training epoch 2
2023-05-09 04:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:15:34 | INFO | train_inner | epoch 002:     99 / 1001 loss=8.393, nll_loss=7.536, intra_distillation_loss=0, ppl=185.62, wps=6438.9, ups=1.78, wpb=3607.8, bsz=142.6, num_updates=1100, lr=0.0001375, gnorm=1.506, train_wall=10, gb_free=76.7, wall=167
2023-05-09 04:15:45 | INFO | train_inner | epoch 002:    199 / 1001 loss=8.283, nll_loss=7.408, intra_distillation_loss=0, ppl=169.78, wps=33247.4, ups=9.19, wpb=3617.1, bsz=151.4, num_updates=1200, lr=0.00015, gnorm=1.545, train_wall=10, gb_free=76.7, wall=178
2023-05-09 04:15:56 | INFO | train_inner | epoch 002:    299 / 1001 loss=8.179, nll_loss=7.29, intra_distillation_loss=0, ppl=156.5, wps=32916.4, ups=9.1, wpb=3616.7, bsz=142.2, num_updates=1300, lr=0.0001625, gnorm=1.328, train_wall=10, gb_free=77, wall=189
2023-05-09 04:16:07 | INFO | train_inner | epoch 002:    399 / 1001 loss=8.111, nll_loss=7.212, intra_distillation_loss=0, ppl=148.28, wps=33105.7, ups=9.34, wpb=3545.3, bsz=138, num_updates=1400, lr=0.000175, gnorm=1.35, train_wall=10, gb_free=76.8, wall=199
2023-05-09 04:16:18 | INFO | train_inner | epoch 002:    499 / 1001 loss=8.076, nll_loss=7.172, intra_distillation_loss=0, ppl=144.18, wps=32831.1, ups=9.15, wpb=3589.3, bsz=135, num_updates=1500, lr=0.0001875, gnorm=1.374, train_wall=10, gb_free=76.7, wall=210
2023-05-09 04:16:28 | INFO | train_inner | epoch 002:    599 / 1001 loss=7.946, nll_loss=7.025, intra_distillation_loss=0, ppl=130.21, wps=32926.9, ups=9.21, wpb=3575.1, bsz=148.1, num_updates=1600, lr=0.0002, gnorm=1.289, train_wall=10, gb_free=76.8, wall=221
2023-05-09 04:16:39 | INFO | train_inner | epoch 002:    699 / 1001 loss=7.883, nll_loss=6.952, intra_distillation_loss=0, ppl=123.85, wps=33288.8, ups=9.3, wpb=3578.8, bsz=138.9, num_updates=1700, lr=0.0002125, gnorm=1.331, train_wall=10, gb_free=76.8, wall=232
2023-05-09 04:16:50 | INFO | train_inner | epoch 002:    799 / 1001 loss=7.838, nll_loss=6.901, intra_distillation_loss=0, ppl=119.53, wps=33199, ups=9.25, wpb=3587.6, bsz=137.4, num_updates=1800, lr=0.000225, gnorm=1.271, train_wall=10, gb_free=76.9, wall=243
2023-05-09 04:17:01 | INFO | train_inner | epoch 002:    899 / 1001 loss=7.797, nll_loss=6.853, intra_distillation_loss=0, ppl=115.61, wps=33559.1, ups=9.22, wpb=3639.8, bsz=137.8, num_updates=1900, lr=0.0002375, gnorm=1.268, train_wall=10, gb_free=76.9, wall=253
2023-05-09 04:17:11 | INFO | train_inner | epoch 002:    999 / 1001 loss=7.847, nll_loss=6.909, intra_distillation_loss=0, ppl=120.18, wps=33641.2, ups=9.51, wpb=3538.6, bsz=126.8, num_updates=2000, lr=0.00025, gnorm=1.35, train_wall=10, gb_free=76.9, wall=264
2023-05-09 04:17:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:17:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:17:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.552 | nll_loss 6.513 | intra_distillation_loss 0 | ppl 91.34 | bleu 2.46 | wps 3848 | wpb 2727.5 | bsz 105.9 | num_updates 2002 | best_bleu 2.46
2023-05-09 04:17:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2002 updates
2023-05-09 04:17:54 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:17:55 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:17:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 2 @ 2002 updates, score 2.46) (writing took 1.8252845159731805 seconds)
2023-05-09 04:17:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-09 04:17:56 | INFO | train | epoch 002 | loss 8.036 | nll_loss 7.126 | intra_distillation_loss 0 | ppl 139.71 | wps 23497.5 | ups 6.55 | wpb 3588.2 | bsz 139.6 | num_updates 2002 | lr 0.00025025 | gnorm 1.361 | train_wall 98 | gb_free 77 | wall 309
2023-05-09 04:17:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:17:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:17:56 | INFO | fairseq.trainer | begin training epoch 3
2023-05-09 04:17:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:18:07 | INFO | train_inner | epoch 003:     98 / 1001 loss=7.707, nll_loss=6.751, intra_distillation_loss=0, ppl=107.74, wps=6257.8, ups=1.81, wpb=3460.7, bsz=132.6, num_updates=2100, lr=0.0002625, gnorm=1.261, train_wall=10, gb_free=76.8, wall=319
2023-05-09 04:18:18 | INFO | train_inner | epoch 003:    198 / 1001 loss=7.578, nll_loss=6.605, intra_distillation_loss=0, ppl=97.37, wps=33256.1, ups=9.22, wpb=3605.5, bsz=136.2, num_updates=2200, lr=0.000275, gnorm=1.129, train_wall=10, gb_free=76.8, wall=330
2023-05-09 04:18:28 | INFO | train_inner | epoch 003:    298 / 1001 loss=7.504, nll_loss=6.52, intra_distillation_loss=0, ppl=91.79, wps=33770, ups=9.21, wpb=3664.7, bsz=141.1, num_updates=2300, lr=0.0002875, gnorm=1.204, train_wall=10, gb_free=76.9, wall=341
2023-05-09 04:18:39 | INFO | train_inner | epoch 003:    398 / 1001 loss=7.584, nll_loss=6.611, intra_distillation_loss=0, ppl=97.75, wps=32680.4, ups=9.2, wpb=3550.5, bsz=126.9, num_updates=2400, lr=0.0003, gnorm=1.169, train_wall=10, gb_free=77, wall=352
2023-05-09 04:18:50 | INFO | train_inner | epoch 003:    498 / 1001 loss=7.508, nll_loss=6.525, intra_distillation_loss=0, ppl=92.07, wps=33357.4, ups=9.28, wpb=3594.5, bsz=139.8, num_updates=2500, lr=0.0003125, gnorm=1.245, train_wall=10, gb_free=76.9, wall=363
2023-05-09 04:19:01 | INFO | train_inner | epoch 003:    598 / 1001 loss=7.354, nll_loss=6.35, intra_distillation_loss=0, ppl=81.54, wps=33364.9, ups=9.11, wpb=3663.7, bsz=156.2, num_updates=2600, lr=0.000325, gnorm=1.264, train_wall=10, gb_free=76.9, wall=374
2023-05-09 04:19:12 | INFO | train_inner | epoch 003:    698 / 1001 loss=7.364, nll_loss=6.362, intra_distillation_loss=0, ppl=82.25, wps=33266, ups=9.21, wpb=3612.8, bsz=143.6, num_updates=2700, lr=0.0003375, gnorm=1.138, train_wall=10, gb_free=76.8, wall=384
2023-05-09 04:19:23 | INFO | train_inner | epoch 003:    798 / 1001 loss=7.389, nll_loss=6.388, intra_distillation_loss=0, ppl=83.78, wps=33213.2, ups=9.2, wpb=3610.6, bsz=133.1, num_updates=2800, lr=0.00035, gnorm=1.138, train_wall=10, gb_free=76.9, wall=395
2023-05-09 04:19:33 | INFO | train_inner | epoch 003:    898 / 1001 loss=7.305, nll_loss=6.293, intra_distillation_loss=0, ppl=78.43, wps=32926.7, ups=9.34, wpb=3525.5, bsz=143.5, num_updates=2900, lr=0.0003625, gnorm=1.214, train_wall=10, gb_free=77.1, wall=406
2023-05-09 04:19:44 | INFO | train_inner | epoch 003:    998 / 1001 loss=7.279, nll_loss=6.263, intra_distillation_loss=0, ppl=76.8, wps=33283.5, ups=9.32, wpb=3569.7, bsz=140.2, num_updates=3000, lr=0.000375, gnorm=1.105, train_wall=10, gb_free=76.8, wall=417
2023-05-09 04:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:19:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:20:28 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.105 | nll_loss 6.008 | intra_distillation_loss 0 | ppl 64.36 | bleu 3.42 | wps 3776.8 | wpb 2727.5 | bsz 105.9 | num_updates 3003 | best_bleu 3.42
2023-05-09 04:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3003 updates
2023-05-09 04:20:28 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:20:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 3 @ 3003 updates, score 3.42) (writing took 1.8274565079482272 seconds)
2023-05-09 04:20:30 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-09 04:20:30 | INFO | train | epoch 003 | loss 7.453 | nll_loss 6.463 | intra_distillation_loss 0 | ppl 88.2 | wps 23375.6 | ups 6.51 | wpb 3588.2 | bsz 139.6 | num_updates 3003 | lr 0.000375375 | gnorm 1.186 | train_wall 98 | gb_free 76.8 | wall 462
2023-05-09 04:20:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:20:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:20:30 | INFO | fairseq.trainer | begin training epoch 4
2023-05-09 04:20:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:20:40 | INFO | train_inner | epoch 004:     97 / 1001 loss=7.212, nll_loss=6.187, intra_distillation_loss=0, ppl=72.85, wps=6367.1, ups=1.78, wpb=3568.1, bsz=129.8, num_updates=3100, lr=0.0003875, gnorm=1.14, train_wall=10, gb_free=77.2, wall=473
2023-05-09 04:20:51 | INFO | train_inner | epoch 004:    197 / 1001 loss=7.169, nll_loss=6.139, intra_distillation_loss=0, ppl=70.49, wps=33275.1, ups=9.23, wpb=3604.1, bsz=133.4, num_updates=3200, lr=0.0004, gnorm=1.063, train_wall=10, gb_free=76.9, wall=484
2023-05-09 04:21:02 | INFO | train_inner | epoch 004:    297 / 1001 loss=7.216, nll_loss=6.189, intra_distillation_loss=0, ppl=72.96, wps=32771.5, ups=9.3, wpb=3523.1, bsz=127.2, num_updates=3300, lr=0.0004125, gnorm=1.165, train_wall=10, gb_free=76.7, wall=494
2023-05-09 04:21:13 | INFO | train_inner | epoch 004:    397 / 1001 loss=7.103, nll_loss=6.062, intra_distillation_loss=0, ppl=66.79, wps=33210.4, ups=9.21, wpb=3604.4, bsz=136.2, num_updates=3400, lr=0.000425, gnorm=1.101, train_wall=10, gb_free=77.1, wall=505
2023-05-09 04:21:23 | INFO | train_inner | epoch 004:    497 / 1001 loss=7.073, nll_loss=6.028, intra_distillation_loss=0, ppl=65.25, wps=33294.2, ups=9.22, wpb=3610.1, bsz=137.8, num_updates=3500, lr=0.0004375, gnorm=1.061, train_wall=10, gb_free=76.9, wall=516
2023-05-09 04:21:34 | INFO | train_inner | epoch 004:    597 / 1001 loss=7.097, nll_loss=6.054, intra_distillation_loss=0, ppl=66.43, wps=32933.8, ups=9.34, wpb=3525.9, bsz=135.8, num_updates=3600, lr=0.00045, gnorm=1.108, train_wall=10, gb_free=76.9, wall=527
2023-05-09 04:21:45 | INFO | train_inner | epoch 004:    697 / 1001 loss=6.958, nll_loss=5.896, intra_distillation_loss=0, ppl=59.56, wps=32745.6, ups=9.06, wpb=3615.9, bsz=156.2, num_updates=3700, lr=0.0004625, gnorm=1.153, train_wall=10, gb_free=76.8, wall=538
2023-05-09 04:21:56 | INFO | train_inner | epoch 004:    797 / 1001 loss=6.945, nll_loss=5.881, intra_distillation_loss=0, ppl=58.92, wps=33452.9, ups=9.25, wpb=3617.8, bsz=151.4, num_updates=3800, lr=0.000475, gnorm=1.181, train_wall=10, gb_free=76.9, wall=549
2023-05-09 04:22:07 | INFO | train_inner | epoch 004:    897 / 1001 loss=7.01, nll_loss=5.955, intra_distillation_loss=0, ppl=62.02, wps=33241.3, ups=9.18, wpb=3620.2, bsz=135.2, num_updates=3900, lr=0.0004875, gnorm=1.039, train_wall=10, gb_free=76.7, wall=560
2023-05-09 04:22:18 | INFO | train_inner | epoch 004:    997 / 1001 loss=6.866, nll_loss=5.791, intra_distillation_loss=0, ppl=55.35, wps=33341.2, ups=9.25, wpb=3603.5, bsz=153.5, num_updates=4000, lr=0.0005, gnorm=1.088, train_wall=10, gb_free=76.9, wall=570
2023-05-09 04:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:22:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:23:01 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.77 | nll_loss 5.637 | intra_distillation_loss 0 | ppl 49.75 | bleu 4.2 | wps 3843.3 | wpb 2727.5 | bsz 105.9 | num_updates 4004 | best_bleu 4.2
2023-05-09 04:23:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4004 updates
2023-05-09 04:23:01 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:23:02 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 4 @ 4004 updates, score 4.2) (writing took 1.8337446560617536 seconds)
2023-05-09 04:23:03 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-09 04:23:03 | INFO | train | epoch 004 | loss 7.064 | nll_loss 6.017 | intra_distillation_loss 0 | ppl 64.77 | wps 23484.4 | ups 6.54 | wpb 3588.2 | bsz 139.6 | num_updates 4004 | lr 0.00049975 | gnorm 1.11 | train_wall 98 | gb_free 77 | wall 615
2023-05-09 04:23:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:23:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:23:03 | INFO | fairseq.trainer | begin training epoch 5
2023-05-09 04:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:23:13 | INFO | train_inner | epoch 005:     96 / 1001 loss=6.746, nll_loss=5.655, intra_distillation_loss=0, ppl=50.38, wps=6570, ups=1.8, wpb=3653.5, bsz=161.6, num_updates=4100, lr=0.000493865, gnorm=1.064, train_wall=10, gb_free=76.9, wall=626
2023-05-09 04:23:24 | INFO | train_inner | epoch 005:    196 / 1001 loss=6.931, nll_loss=5.862, intra_distillation_loss=0, ppl=58.15, wps=32981.7, ups=9.42, wpb=3501.2, bsz=127, num_updates=4200, lr=0.00048795, gnorm=1.096, train_wall=10, gb_free=76.9, wall=637
2023-05-09 04:23:35 | INFO | train_inner | epoch 005:    296 / 1001 loss=6.761, nll_loss=5.67, intra_distillation_loss=0, ppl=50.9, wps=33045.6, ups=9.18, wpb=3601.7, bsz=139.6, num_updates=4300, lr=0.000482243, gnorm=1.018, train_wall=10, gb_free=76.8, wall=647
2023-05-09 04:23:46 | INFO | train_inner | epoch 005:    396 / 1001 loss=6.768, nll_loss=5.677, intra_distillation_loss=0, ppl=51.16, wps=33684.2, ups=9.21, wpb=3656.1, bsz=130.9, num_updates=4400, lr=0.000476731, gnorm=1.059, train_wall=10, gb_free=76.9, wall=658
2023-05-09 04:23:57 | INFO | train_inner | epoch 005:    496 / 1001 loss=6.721, nll_loss=5.623, intra_distillation_loss=0, ppl=49.29, wps=32558, ups=9.13, wpb=3564.1, bsz=134.6, num_updates=4500, lr=0.000471405, gnorm=1.095, train_wall=10, gb_free=76.8, wall=669
2023-05-09 04:24:08 | INFO | train_inner | epoch 005:    596 / 1001 loss=6.564, nll_loss=5.443, intra_distillation_loss=0, ppl=43.5, wps=33002.9, ups=9.21, wpb=3585.2, bsz=150.6, num_updates=4600, lr=0.000466252, gnorm=1.05, train_wall=10, gb_free=76.8, wall=680
2023-05-09 04:24:18 | INFO | train_inner | epoch 005:    696 / 1001 loss=6.598, nll_loss=5.483, intra_distillation_loss=0, ppl=44.72, wps=32497.7, ups=9.15, wpb=3552.9, bsz=139.4, num_updates=4700, lr=0.000461266, gnorm=1.112, train_wall=10, gb_free=76.9, wall=691
2023-05-09 04:24:29 | INFO | train_inner | epoch 005:    796 / 1001 loss=6.558, nll_loss=5.436, intra_distillation_loss=0, ppl=43.29, wps=32947.9, ups=9.25, wpb=3560.6, bsz=141, num_updates=4800, lr=0.000456435, gnorm=1.081, train_wall=10, gb_free=76.8, wall=702
2023-05-09 04:24:40 | INFO | train_inner | epoch 005:    896 / 1001 loss=6.605, nll_loss=5.489, intra_distillation_loss=0, ppl=44.9, wps=33474.8, ups=9.36, wpb=3575.6, bsz=128.8, num_updates=4900, lr=0.000451754, gnorm=1.061, train_wall=10, gb_free=77, wall=713
2023-05-09 04:24:51 | INFO | train_inner | epoch 005:    996 / 1001 loss=6.382, nll_loss=5.235, intra_distillation_loss=0, ppl=37.67, wps=33761.1, ups=9.26, wpb=3647.1, bsz=145.8, num_updates=5000, lr=0.000447214, gnorm=1.114, train_wall=10, gb_free=76.8, wall=723
2023-05-09 04:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:24:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:25:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.213 | nll_loss 4.984 | intra_distillation_loss 0 | ppl 31.65 | bleu 7.44 | wps 3824.4 | wpb 2727.5 | bsz 105.9 | num_updates 5005 | best_bleu 7.44
2023-05-09 04:25:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5005 updates
2023-05-09 04:25:34 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:25:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 5 @ 5005 updates, score 7.44) (writing took 1.8811873979866505 seconds)
2023-05-09 04:25:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-09 04:25:36 | INFO | train | epoch 005 | loss 6.663 | nll_loss 5.557 | intra_distillation_loss 0 | ppl 47.07 | wps 23418.6 | ups 6.53 | wpb 3588.2 | bsz 139.6 | num_updates 5005 | lr 0.00044699 | gnorm 1.075 | train_wall 98 | gb_free 76.8 | wall 769
2023-05-09 04:25:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:25:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:25:36 | INFO | fairseq.trainer | begin training epoch 6
2023-05-09 04:25:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:25:46 | INFO | train_inner | epoch 006:     95 / 1001 loss=6.398, nll_loss=5.254, intra_distillation_loss=0, ppl=38.15, wps=6346.5, ups=1.8, wpb=3529.7, bsz=127.1, num_updates=5100, lr=0.000442807, gnorm=1.078, train_wall=10, gb_free=76.9, wall=779
2023-05-09 04:25:57 | INFO | train_inner | epoch 006:    195 / 1001 loss=6.323, nll_loss=5.167, intra_distillation_loss=0, ppl=35.91, wps=33504.9, ups=9.36, wpb=3579.9, bsz=128.9, num_updates=5200, lr=0.000438529, gnorm=1.136, train_wall=10, gb_free=76.9, wall=790
2023-05-09 04:26:08 | INFO | train_inner | epoch 006:    295 / 1001 loss=6.24, nll_loss=5.072, intra_distillation_loss=0, ppl=33.63, wps=33924.4, ups=9.19, wpb=3689.8, bsz=140.3, num_updates=5300, lr=0.000434372, gnorm=1.122, train_wall=10, gb_free=76.8, wall=801
2023-05-09 04:26:19 | INFO | train_inner | epoch 006:    395 / 1001 loss=6.252, nll_loss=5.083, intra_distillation_loss=0, ppl=33.9, wps=33306.1, ups=9.26, wpb=3596.4, bsz=131, num_updates=5400, lr=0.000430331, gnorm=1.206, train_wall=10, gb_free=76.9, wall=811
2023-05-09 04:26:30 | INFO | train_inner | epoch 006:    495 / 1001 loss=6.224, nll_loss=5.052, intra_distillation_loss=0, ppl=33.16, wps=32516.6, ups=9.22, wpb=3527.3, bsz=148.4, num_updates=5500, lr=0.000426401, gnorm=1.165, train_wall=10, gb_free=76.8, wall=822
2023-05-09 04:26:40 | INFO | train_inner | epoch 006:    595 / 1001 loss=6.125, nll_loss=4.938, intra_distillation_loss=0, ppl=30.65, wps=32591.7, ups=9.25, wpb=3523.7, bsz=143.4, num_updates=5600, lr=0.000422577, gnorm=1.174, train_wall=10, gb_free=77, wall=833
2023-05-09 04:26:51 | INFO | train_inner | epoch 006:    695 / 1001 loss=5.982, nll_loss=4.776, intra_distillation_loss=0, ppl=27.41, wps=32711.8, ups=9.15, wpb=3575.3, bsz=161.7, num_updates=5700, lr=0.000418854, gnorm=1.123, train_wall=10, gb_free=77, wall=844
2023-05-09 04:27:02 | INFO | train_inner | epoch 006:    795 / 1001 loss=6.16, nll_loss=4.977, intra_distillation_loss=0, ppl=31.49, wps=33068.4, ups=9.22, wpb=3585.3, bsz=132.7, num_updates=5800, lr=0.000415227, gnorm=1.195, train_wall=10, gb_free=76.8, wall=855
2023-05-09 04:27:13 | INFO | train_inner | epoch 006:    895 / 1001 loss=6.081, nll_loss=4.886, intra_distillation_loss=0, ppl=29.58, wps=33290, ups=9.15, wpb=3638, bsz=136.7, num_updates=5900, lr=0.000411693, gnorm=1.162, train_wall=10, gb_free=76.8, wall=866
2023-05-09 04:27:24 | INFO | train_inner | epoch 006:    995 / 1001 loss=5.944, nll_loss=4.731, intra_distillation_loss=0, ppl=26.56, wps=33856.9, ups=9.32, wpb=3633.7, bsz=144.1, num_updates=6000, lr=0.000408248, gnorm=1.123, train_wall=10, gb_free=77.1, wall=876
2023-05-09 04:27:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:27:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:28:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.975 | nll_loss 4.66 | intra_distillation_loss 0 | ppl 25.28 | bleu 9.93 | wps 3627.4 | wpb 2727.5 | bsz 105.9 | num_updates 6006 | best_bleu 9.93
2023-05-09 04:28:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6006 updates
2023-05-09 04:28:10 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:28:11 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 6 @ 6006 updates, score 9.93) (writing took 1.859255995019339 seconds)
2023-05-09 04:28:11 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-09 04:28:11 | INFO | train | epoch 006 | loss 6.167 | nll_loss 4.987 | intra_distillation_loss 0 | ppl 31.71 | wps 23110.9 | ups 6.44 | wpb 3588.2 | bsz 139.6 | num_updates 6006 | lr 0.000408044 | gnorm 1.15 | train_wall 97 | gb_free 77.1 | wall 924
2023-05-09 04:28:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:28:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:28:12 | INFO | fairseq.trainer | begin training epoch 7
2023-05-09 04:28:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:28:22 | INFO | train_inner | epoch 007:     94 / 1001 loss=5.865, nll_loss=4.641, intra_distillation_loss=0, ppl=24.94, wps=6264, ups=1.73, wpb=3631.1, bsz=139.8, num_updates=6100, lr=0.000404888, gnorm=1.209, train_wall=10, gb_free=77, wall=934
2023-05-09 04:28:33 | INFO | train_inner | epoch 007:    194 / 1001 loss=5.841, nll_loss=4.614, intra_distillation_loss=0, ppl=24.48, wps=32630.7, ups=9.19, wpb=3551, bsz=139, num_updates=6200, lr=0.00040161, gnorm=1.143, train_wall=10, gb_free=77.1, wall=945
2023-05-09 04:28:43 | INFO | train_inner | epoch 007:    294 / 1001 loss=5.825, nll_loss=4.594, intra_distillation_loss=0, ppl=24.15, wps=33572.1, ups=9.29, wpb=3614.6, bsz=135.7, num_updates=6300, lr=0.00039841, gnorm=1.18, train_wall=10, gb_free=76.8, wall=956
2023-05-09 04:28:54 | INFO | train_inner | epoch 007:    394 / 1001 loss=5.849, nll_loss=4.619, intra_distillation_loss=0, ppl=24.58, wps=33159, ups=9.16, wpb=3619.4, bsz=136.5, num_updates=6400, lr=0.000395285, gnorm=1.163, train_wall=10, gb_free=76.8, wall=967
2023-05-09 04:29:05 | INFO | train_inner | epoch 007:    494 / 1001 loss=5.816, nll_loss=4.582, intra_distillation_loss=0, ppl=23.96, wps=32648.6, ups=9.21, wpb=3543.6, bsz=138.9, num_updates=6500, lr=0.000392232, gnorm=1.179, train_wall=10, gb_free=76.7, wall=978
2023-05-09 04:29:16 | INFO | train_inner | epoch 007:    594 / 1001 loss=5.768, nll_loss=4.527, intra_distillation_loss=0, ppl=23.05, wps=33589.8, ups=9.38, wpb=3582.4, bsz=136.8, num_updates=6600, lr=0.000389249, gnorm=1.163, train_wall=10, gb_free=77.2, wall=988
2023-05-09 04:29:27 | INFO | train_inner | epoch 007:    694 / 1001 loss=5.871, nll_loss=4.645, intra_distillation_loss=0, ppl=25.03, wps=32827.7, ups=9.34, wpb=3512.9, bsz=125.6, num_updates=6700, lr=0.000386334, gnorm=1.168, train_wall=10, gb_free=76.8, wall=999
2023-05-09 04:29:37 | INFO | train_inner | epoch 007:    794 / 1001 loss=5.634, nll_loss=4.375, intra_distillation_loss=0, ppl=20.75, wps=32766.5, ups=9.17, wpb=3572.4, bsz=151.3, num_updates=6800, lr=0.000383482, gnorm=1.148, train_wall=10, gb_free=76.8, wall=1010
2023-05-09 04:29:48 | INFO | train_inner | epoch 007:    894 / 1001 loss=5.69, nll_loss=4.437, intra_distillation_loss=0, ppl=21.67, wps=33524.9, ups=9.2, wpb=3643.6, bsz=136.2, num_updates=6900, lr=0.000380693, gnorm=1.164, train_wall=10, gb_free=76.9, wall=1021
2023-05-09 04:29:59 | INFO | train_inner | epoch 007:    994 / 1001 loss=5.586, nll_loss=4.32, intra_distillation_loss=0, ppl=19.97, wps=33380.1, ups=9.23, wpb=3615, bsz=154.3, num_updates=7000, lr=0.000377964, gnorm=1.152, train_wall=10, gb_free=77.1, wall=1032
2023-05-09 04:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:30:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:30:41 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.512 | nll_loss 4.132 | intra_distillation_loss 0 | ppl 17.53 | bleu 15.38 | wps 3963.7 | wpb 2727.5 | bsz 105.9 | num_updates 7007 | best_bleu 15.38
2023-05-09 04:30:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7007 updates
2023-05-09 04:30:41 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:30:42 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:30:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 7 @ 7007 updates, score 15.38) (writing took 1.878652008017525 seconds)
2023-05-09 04:30:43 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-09 04:30:43 | INFO | train | epoch 007 | loss 5.774 | nll_loss 4.535 | intra_distillation_loss 0 | ppl 23.18 | wps 23677 | ups 6.6 | wpb 3588.2 | bsz 139.6 | num_updates 7007 | lr 0.000377776 | gnorm 1.166 | train_wall 98 | gb_free 76.9 | wall 1076
2023-05-09 04:30:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:30:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:30:43 | INFO | fairseq.trainer | begin training epoch 8
2023-05-09 04:30:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:30:53 | INFO | train_inner | epoch 008:     93 / 1001 loss=5.558, nll_loss=4.288, intra_distillation_loss=0, ppl=19.54, wps=6677.1, ups=1.84, wpb=3624.3, bsz=142.6, num_updates=7100, lr=0.000375293, gnorm=1.201, train_wall=10, gb_free=76.9, wall=1086
2023-05-09 04:31:04 | INFO | train_inner | epoch 008:    193 / 1001 loss=5.453, nll_loss=4.168, intra_distillation_loss=0, ppl=17.98, wps=32801.1, ups=9.14, wpb=3588.2, bsz=156, num_updates=7200, lr=0.000372678, gnorm=1.147, train_wall=10, gb_free=76.9, wall=1097
2023-05-09 04:31:15 | INFO | train_inner | epoch 008:    293 / 1001 loss=5.501, nll_loss=4.222, intra_distillation_loss=0, ppl=18.66, wps=32844.1, ups=9.28, wpb=3539.6, bsz=139.3, num_updates=7300, lr=0.000370117, gnorm=1.169, train_wall=10, gb_free=76.9, wall=1108
2023-05-09 04:31:26 | INFO | train_inner | epoch 008:    393 / 1001 loss=5.481, nll_loss=4.198, intra_distillation_loss=0, ppl=18.35, wps=33641.3, ups=9.34, wpb=3602.4, bsz=134, num_updates=7400, lr=0.000367607, gnorm=1.134, train_wall=10, gb_free=77, wall=1118
2023-05-09 04:31:37 | INFO | train_inner | epoch 008:    493 / 1001 loss=5.478, nll_loss=4.195, intra_distillation_loss=0, ppl=18.31, wps=32951, ups=9.26, wpb=3558.3, bsz=133.9, num_updates=7500, lr=0.000365148, gnorm=1.175, train_wall=10, gb_free=76.9, wall=1129
2023-05-09 04:31:48 | INFO | train_inner | epoch 008:    593 / 1001 loss=5.379, nll_loss=4.082, intra_distillation_loss=0, ppl=16.94, wps=33225, ups=9.18, wpb=3619.7, bsz=150.3, num_updates=7600, lr=0.000362738, gnorm=1.119, train_wall=10, gb_free=76.8, wall=1140
2023-05-09 04:31:58 | INFO | train_inner | epoch 008:    693 / 1001 loss=5.483, nll_loss=4.199, intra_distillation_loss=0, ppl=18.37, wps=33423.2, ups=9.35, wpb=3575.4, bsz=127.4, num_updates=7700, lr=0.000360375, gnorm=1.172, train_wall=10, gb_free=77.1, wall=1151
2023-05-09 04:32:09 | INFO | train_inner | epoch 008:    793 / 1001 loss=5.455, nll_loss=4.168, intra_distillation_loss=0, ppl=17.98, wps=32945.9, ups=9.2, wpb=3579.8, bsz=140.8, num_updates=7800, lr=0.000358057, gnorm=1.187, train_wall=10, gb_free=77, wall=1162
2023-05-09 04:32:20 | INFO | train_inner | epoch 008:    893 / 1001 loss=5.447, nll_loss=4.158, intra_distillation_loss=0, ppl=17.85, wps=33105.2, ups=9.3, wpb=3557.9, bsz=135.4, num_updates=7900, lr=0.000355784, gnorm=1.169, train_wall=10, gb_free=77.1, wall=1172
2023-05-09 04:32:31 | INFO | train_inner | epoch 008:    993 / 1001 loss=5.338, nll_loss=4.036, intra_distillation_loss=0, ppl=16.4, wps=33221.5, ups=9.16, wpb=3628.7, bsz=138.5, num_updates=8000, lr=0.000353553, gnorm=1.107, train_wall=10, gb_free=76.9, wall=1183
2023-05-09 04:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:32:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:33:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.189 | nll_loss 3.749 | intra_distillation_loss 0 | ppl 13.45 | bleu 18.03 | wps 3965 | wpb 2727.5 | bsz 105.9 | num_updates 8008 | best_bleu 18.03
2023-05-09 04:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8008 updates
2023-05-09 04:33:13 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:33:14 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:33:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 8 @ 8008 updates, score 18.03) (writing took 1.799122864031233 seconds)
2023-05-09 04:33:15 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-09 04:33:15 | INFO | train | epoch 008 | loss 5.455 | nll_loss 4.169 | intra_distillation_loss 0 | ppl 17.99 | wps 23671 | ups 6.6 | wpb 3588.2 | bsz 139.6 | num_updates 8008 | lr 0.000353377 | gnorm 1.149 | train_wall 98 | gb_free 77.1 | wall 1228
2023-05-09 04:33:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:33:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:33:15 | INFO | fairseq.trainer | begin training epoch 9
2023-05-09 04:33:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:33:27 | INFO | train_inner | epoch 009:     92 / 1001 loss=5.298, nll_loss=3.995, intra_distillation_loss=0, ppl=15.95, wps=6313.2, ups=1.79, wpb=3528.7, bsz=125.9, num_updates=8100, lr=0.000351364, gnorm=0, train_wall=12, gb_free=76.9, wall=1239
2023-05-09 04:33:40 | INFO | train_inner | epoch 009:    192 / 1001 loss=5.151, nll_loss=3.828, intra_distillation_loss=0, ppl=14.2, wps=27889.4, ups=7.75, wpb=3598.7, bsz=149, num_updates=8200, lr=0.000349215, gnorm=0, train_wall=12, gb_free=76.9, wall=1252
2023-05-09 04:33:53 | INFO | train_inner | epoch 009:    292 / 1001 loss=5.161, nll_loss=3.837, intra_distillation_loss=0, ppl=14.29, wps=28168.1, ups=7.73, wpb=3641.8, bsz=137.6, num_updates=8300, lr=0.000347105, gnorm=0, train_wall=12, gb_free=76.8, wall=1265
2023-05-09 04:34:06 | INFO | train_inner | epoch 009:    392 / 1001 loss=5.015, nll_loss=3.672, intra_distillation_loss=0, ppl=12.74, wps=28253.4, ups=7.71, wpb=3662.9, bsz=159.8, num_updates=8400, lr=0.000345033, gnorm=0, train_wall=12, gb_free=77.1, wall=1278
2023-05-09 04:34:18 | INFO | train_inner | epoch 009:    492 / 1001 loss=5.212, nll_loss=3.894, intra_distillation_loss=0, ppl=14.87, wps=27856.2, ups=7.82, wpb=3560.3, bsz=137.9, num_updates=8500, lr=0.000342997, gnorm=0, train_wall=12, gb_free=76.9, wall=1291
2023-05-09 04:34:31 | INFO | train_inner | epoch 009:    592 / 1001 loss=5.14, nll_loss=3.81, intra_distillation_loss=0, ppl=14.03, wps=28296, ups=7.82, wpb=3616.8, bsz=140.2, num_updates=8600, lr=0.000340997, gnorm=0, train_wall=12, gb_free=77.1, wall=1304
2023-05-09 04:34:44 | INFO | train_inner | epoch 009:    692 / 1001 loss=5.14, nll_loss=3.811, intra_distillation_loss=0, ppl=14.03, wps=28277, ups=7.81, wpb=3619, bsz=139.3, num_updates=8700, lr=0.000339032, gnorm=0, train_wall=12, gb_free=77, wall=1316
2023-05-09 04:34:57 | INFO | train_inner | epoch 009:    792 / 1001 loss=5.161, nll_loss=3.834, intra_distillation_loss=0, ppl=14.26, wps=27659.7, ups=7.87, wpb=3516.2, bsz=138, num_updates=8800, lr=0.0003371, gnorm=0, train_wall=12, gb_free=76.9, wall=1329
2023-05-09 04:35:09 | INFO | train_inner | epoch 009:    892 / 1001 loss=5.181, nll_loss=3.855, intra_distillation_loss=0, ppl=14.47, wps=28397.2, ups=7.91, wpb=3589, bsz=123.1, num_updates=8900, lr=0.000335201, gnorm=0, train_wall=12, gb_free=77, wall=1342
2023-05-09 04:35:22 | INFO | train_inner | epoch 009:    992 / 1001 loss=5.125, nll_loss=3.793, intra_distillation_loss=0, ppl=13.87, wps=27815, ups=7.84, wpb=3545.8, bsz=143, num_updates=9000, lr=0.000333333, gnorm=0, train_wall=12, gb_free=76.9, wall=1355
2023-05-09 04:35:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:35:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:36:03 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.01 | nll_loss 3.527 | intra_distillation_loss 0 | ppl 11.53 | bleu 20.64 | wps 4075.1 | wpb 2727.5 | bsz 105.9 | num_updates 9009 | best_bleu 20.64
2023-05-09 04:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9009 updates
2023-05-09 04:36:03 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:36:04 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 9 @ 9009 updates, score 20.64) (writing took 1.9233791929436848 seconds)
2023-05-09 04:36:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-09 04:36:05 | INFO | train | epoch 009 | loss 5.155 | nll_loss 3.829 | intra_distillation_loss 0 | ppl 14.21 | wps 21089.9 | ups 5.88 | wpb 3588.2 | bsz 139.6 | num_updates 9009 | lr 0.000333167 | gnorm 0 | train_wall 117 | gb_free 77 | wall 1398
2023-05-09 04:36:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:36:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:36:05 | INFO | fairseq.trainer | begin training epoch 10
2023-05-09 04:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:36:17 | INFO | train_inner | epoch 010:     91 / 1001 loss=4.983, nll_loss=3.631, intra_distillation_loss=0, ppl=12.39, wps=6659.1, ups=1.82, wpb=3663.1, bsz=140.3, num_updates=9100, lr=0.000331497, gnorm=0, train_wall=12, gb_free=76.9, wall=1410
2023-05-09 04:36:30 | INFO | train_inner | epoch 010:    191 / 1001 loss=5.025, nll_loss=3.677, intra_distillation_loss=0, ppl=12.79, wps=28279.4, ups=7.78, wpb=3636.4, bsz=136.8, num_updates=9200, lr=0.00032969, gnorm=0, train_wall=12, gb_free=76.8, wall=1422
2023-05-09 04:36:43 | INFO | train_inner | epoch 010:    291 / 1001 loss=5.008, nll_loss=3.657, intra_distillation_loss=0, ppl=12.61, wps=28089.5, ups=7.74, wpb=3629.2, bsz=135.6, num_updates=9300, lr=0.000327913, gnorm=0, train_wall=12, gb_free=76.8, wall=1435
2023-05-09 04:36:56 | INFO | train_inner | epoch 010:    391 / 1001 loss=5.001, nll_loss=3.649, intra_distillation_loss=0, ppl=12.54, wps=27975.1, ups=7.82, wpb=3575.4, bsz=141, num_updates=9400, lr=0.000326164, gnorm=0, train_wall=12, gb_free=76.9, wall=1448
2023-05-09 04:37:08 | INFO | train_inner | epoch 010:    491 / 1001 loss=5.048, nll_loss=3.701, intra_distillation_loss=0, ppl=13, wps=28324, ups=7.93, wpb=3573.2, bsz=132.5, num_updates=9500, lr=0.000324443, gnorm=0, train_wall=12, gb_free=76.9, wall=1461
2023-05-09 04:37:21 | INFO | train_inner | epoch 010:    591 / 1001 loss=4.99, nll_loss=3.636, intra_distillation_loss=0, ppl=12.43, wps=28580.1, ups=7.83, wpb=3648.8, bsz=141.1, num_updates=9600, lr=0.000322749, gnorm=0, train_wall=12, gb_free=77, wall=1474
2023-05-09 04:37:34 | INFO | train_inner | epoch 010:    691 / 1001 loss=5.022, nll_loss=3.672, intra_distillation_loss=0, ppl=12.74, wps=27973.8, ups=7.9, wpb=3540.8, bsz=133.6, num_updates=9700, lr=0.000321081, gnorm=0, train_wall=12, gb_free=77, wall=1486
2023-05-09 04:37:46 | INFO | train_inner | epoch 010:    791 / 1001 loss=4.999, nll_loss=3.647, intra_distillation_loss=0, ppl=12.52, wps=28085.3, ups=7.84, wpb=3582.4, bsz=146.9, num_updates=9800, lr=0.000319438, gnorm=0, train_wall=12, gb_free=76.9, wall=1499
2023-05-09 04:37:59 | INFO | train_inner | epoch 010:    891 / 1001 loss=5.123, nll_loss=3.787, intra_distillation_loss=0, ppl=13.81, wps=27441.8, ups=7.91, wpb=3471.3, bsz=129.5, num_updates=9900, lr=0.000317821, gnorm=0, train_wall=12, gb_free=77.1, wall=1512
2023-05-09 04:38:12 | INFO | train_inner | epoch 010:    991 / 1001 loss=4.955, nll_loss=3.597, intra_distillation_loss=0, ppl=12.1, wps=27831.9, ups=7.77, wpb=3579.9, bsz=159.8, num_updates=10000, lr=0.000316228, gnorm=0, train_wall=12, gb_free=76.8, wall=1524
2023-05-09 04:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:38:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:38:53 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.946 | nll_loss 3.457 | intra_distillation_loss 0 | ppl 10.98 | bleu 20.85 | wps 4132.2 | wpb 2727.5 | bsz 105.9 | num_updates 10010 | best_bleu 20.85
2023-05-09 04:38:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 10010 updates
2023-05-09 04:38:53 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:38:54 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 10 @ 10010 updates, score 20.85) (writing took 1.9836918649962172 seconds)
2023-05-09 04:38:55 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-09 04:38:55 | INFO | train | epoch 010 | loss 5.016 | nll_loss 3.666 | intra_distillation_loss 0 | ppl 12.69 | wps 21187.2 | ups 5.9 | wpb 3588.2 | bsz 139.6 | num_updates 10010 | lr 0.00031607 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 1567
2023-05-09 04:38:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:38:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:38:55 | INFO | fairseq.trainer | begin training epoch 11
2023-05-09 04:38:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:39:06 | INFO | train_inner | epoch 011:     90 / 1001 loss=4.96, nll_loss=3.602, intra_distillation_loss=0, ppl=12.14, wps=6536, ups=1.84, wpb=3559.8, bsz=134.3, num_updates=10100, lr=0.000314658, gnorm=0, train_wall=12, gb_free=76.8, wall=1579
2023-05-09 04:39:19 | INFO | train_inner | epoch 011:    190 / 1001 loss=4.961, nll_loss=3.601, intra_distillation_loss=0, ppl=12.13, wps=28107.1, ups=7.91, wpb=3553, bsz=134.6, num_updates=10200, lr=0.000313112, gnorm=0, train_wall=12, gb_free=76.9, wall=1592
2023-05-09 04:39:32 | INFO | train_inner | epoch 011:    290 / 1001 loss=4.867, nll_loss=3.495, intra_distillation_loss=0, ppl=11.27, wps=28230.6, ups=7.79, wpb=3623.5, bsz=156.6, num_updates=10300, lr=0.000311588, gnorm=0, train_wall=12, gb_free=77.1, wall=1604
2023-05-09 04:39:45 | INFO | train_inner | epoch 011:    390 / 1001 loss=4.911, nll_loss=3.544, intra_distillation_loss=0, ppl=11.66, wps=28043.3, ups=7.8, wpb=3596.8, bsz=142.5, num_updates=10400, lr=0.000310087, gnorm=0, train_wall=12, gb_free=77, wall=1617
2023-05-09 04:39:57 | INFO | train_inner | epoch 011:    490 / 1001 loss=4.95, nll_loss=3.588, intra_distillation_loss=0, ppl=12.03, wps=27843, ups=7.88, wpb=3535.2, bsz=135.6, num_updates=10500, lr=0.000308607, gnorm=0, train_wall=12, gb_free=76.9, wall=1630
2023-05-09 04:40:10 | INFO | train_inner | epoch 011:    590 / 1001 loss=4.852, nll_loss=3.477, intra_distillation_loss=0, ppl=11.13, wps=28452.3, ups=7.84, wpb=3631.4, bsz=148.8, num_updates=10600, lr=0.000307148, gnorm=0, train_wall=12, gb_free=76.9, wall=1643
2023-05-09 04:40:23 | INFO | train_inner | epoch 011:    690 / 1001 loss=4.941, nll_loss=3.579, intra_distillation_loss=0, ppl=11.95, wps=27848.5, ups=7.76, wpb=3587.7, bsz=139.8, num_updates=10700, lr=0.000305709, gnorm=0, train_wall=12, gb_free=76.9, wall=1656
2023-05-09 04:40:36 | INFO | train_inner | epoch 011:    790 / 1001 loss=4.959, nll_loss=3.598, intra_distillation_loss=0, ppl=12.11, wps=28416.8, ups=7.83, wpb=3629.5, bsz=129.2, num_updates=10800, lr=0.00030429, gnorm=0, train_wall=12, gb_free=77, wall=1668
2023-05-09 04:40:49 | INFO | train_inner | epoch 011:    890 / 1001 loss=4.955, nll_loss=3.595, intra_distillation_loss=0, ppl=12.08, wps=27913.5, ups=7.81, wpb=3574.5, bsz=139, num_updates=10900, lr=0.000302891, gnorm=0, train_wall=12, gb_free=76.8, wall=1681
2023-05-09 04:41:01 | INFO | train_inner | epoch 011:    990 / 1001 loss=4.92, nll_loss=3.553, intra_distillation_loss=0, ppl=11.74, wps=27891.5, ups=7.86, wpb=3549.1, bsz=138.1, num_updates=11000, lr=0.000301511, gnorm=0, train_wall=12, gb_free=76.9, wall=1694
2023-05-09 04:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:41:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:41:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.849 | nll_loss 3.341 | intra_distillation_loss 0 | ppl 10.13 | bleu 22.67 | wps 3908.3 | wpb 2727.5 | bsz 105.9 | num_updates 11011 | best_bleu 22.67
2023-05-09 04:41:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 11011 updates
2023-05-09 04:41:45 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:41:46 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 11 @ 11011 updates, score 22.67) (writing took 1.8305690640117973 seconds)
2023-05-09 04:41:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-09 04:41:46 | INFO | train | epoch 011 | loss 4.925 | nll_loss 3.56 | intra_distillation_loss 0 | ppl 11.8 | wps 20923.8 | ups 5.83 | wpb 3588.2 | bsz 139.6 | num_updates 11011 | lr 0.000301361 | gnorm 0 | train_wall 117 | gb_free 76.9 | wall 1739
2023-05-09 04:41:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:41:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:41:46 | INFO | fairseq.trainer | begin training epoch 12
2023-05-09 04:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:41:58 | INFO | train_inner | epoch 012:     89 / 1001 loss=4.882, nll_loss=3.511, intra_distillation_loss=0, ppl=11.4, wps=6347.9, ups=1.77, wpb=3590.8, bsz=120, num_updates=11100, lr=0.00030015, gnorm=0, train_wall=12, gb_free=76.8, wall=1750
2023-05-09 04:42:11 | INFO | train_inner | epoch 012:    189 / 1001 loss=4.794, nll_loss=3.411, intra_distillation_loss=0, ppl=10.64, wps=28151.3, ups=7.83, wpb=3595.4, bsz=151.9, num_updates=11200, lr=0.000298807, gnorm=0, train_wall=12, gb_free=76.9, wall=1763
2023-05-09 04:42:24 | INFO | train_inner | epoch 012:    289 / 1001 loss=4.753, nll_loss=3.364, intra_distillation_loss=0, ppl=10.29, wps=28478.3, ups=7.73, wpb=3682.5, bsz=153.6, num_updates=11300, lr=0.000297482, gnorm=0, train_wall=12, gb_free=76.8, wall=1776
2023-05-09 04:42:36 | INFO | train_inner | epoch 012:    389 / 1001 loss=4.867, nll_loss=3.493, intra_distillation_loss=0, ppl=11.26, wps=28013, ups=7.76, wpb=3611.8, bsz=142.5, num_updates=11400, lr=0.000296174, gnorm=0, train_wall=12, gb_free=77.1, wall=1789
2023-05-09 04:42:49 | INFO | train_inner | epoch 012:    489 / 1001 loss=4.846, nll_loss=3.469, intra_distillation_loss=0, ppl=11.07, wps=27929.6, ups=7.74, wpb=3610.2, bsz=143.3, num_updates=11500, lr=0.000294884, gnorm=0, train_wall=12, gb_free=76.8, wall=1802
2023-05-09 04:43:02 | INFO | train_inner | epoch 012:    589 / 1001 loss=4.881, nll_loss=3.509, intra_distillation_loss=0, ppl=11.38, wps=27952.9, ups=7.84, wpb=3564.8, bsz=138.6, num_updates=11600, lr=0.00029361, gnorm=0, train_wall=12, gb_free=76.9, wall=1815
2023-05-09 04:43:15 | INFO | train_inner | epoch 012:    689 / 1001 loss=4.87, nll_loss=3.496, intra_distillation_loss=0, ppl=11.29, wps=28033.7, ups=7.84, wpb=3575.8, bsz=136, num_updates=11700, lr=0.000292353, gnorm=0, train_wall=12, gb_free=76.8, wall=1827
2023-05-09 04:43:28 | INFO | train_inner | epoch 012:    789 / 1001 loss=4.817, nll_loss=3.436, intra_distillation_loss=0, ppl=10.82, wps=28126.9, ups=7.86, wpb=3580.2, bsz=144.1, num_updates=11800, lr=0.000291111, gnorm=0, train_wall=12, gb_free=77.1, wall=1840
2023-05-09 04:43:40 | INFO | train_inner | epoch 012:    889 / 1001 loss=4.919, nll_loss=3.552, intra_distillation_loss=0, ppl=11.73, wps=28100.1, ups=7.86, wpb=3575.4, bsz=130.5, num_updates=11900, lr=0.000289886, gnorm=0, train_wall=12, gb_free=76.7, wall=1853
2023-05-09 04:43:53 | INFO | train_inner | epoch 012:    989 / 1001 loss=4.859, nll_loss=3.484, intra_distillation_loss=0, ppl=11.19, wps=27836.2, ups=7.83, wpb=3554.4, bsz=135.8, num_updates=12000, lr=0.000288675, gnorm=0, train_wall=12, gb_free=76.9, wall=1866
2023-05-09 04:43:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:43:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:44:32 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.825 | nll_loss 3.313 | intra_distillation_loss 0 | ppl 9.94 | bleu 22.21 | wps 4379.4 | wpb 2727.5 | bsz 105.9 | num_updates 12012 | best_bleu 22.67
2023-05-09 04:44:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 12012 updates
2023-05-09 04:44:32 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt
2023-05-09 04:44:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt (epoch 12 @ 12012 updates, score 22.21) (writing took 1.1056487279711291 seconds)
2023-05-09 04:44:33 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-09 04:44:33 | INFO | train | epoch 012 | loss 4.85 | nll_loss 3.473 | intra_distillation_loss 0 | ppl 11.11 | wps 21550.7 | ups 6.01 | wpb 3588.2 | bsz 139.6 | num_updates 12012 | lr 0.000288531 | gnorm 0 | train_wall 117 | gb_free 77.1 | wall 1906
2023-05-09 04:44:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:44:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:44:33 | INFO | fairseq.trainer | begin training epoch 13
2023-05-09 04:44:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:44:44 | INFO | train_inner | epoch 013:     88 / 1001 loss=4.825, nll_loss=3.446, intra_distillation_loss=0, ppl=10.9, wps=6904.5, ups=1.95, wpb=3541.3, bsz=131, num_updates=12100, lr=0.00028748, gnorm=0, train_wall=12, gb_free=76.9, wall=1917
2023-05-09 04:44:57 | INFO | train_inner | epoch 013:    188 / 1001 loss=4.804, nll_loss=3.421, intra_distillation_loss=0, ppl=10.71, wps=27870.7, ups=7.87, wpb=3539.5, bsz=138, num_updates=12200, lr=0.000286299, gnorm=0, train_wall=12, gb_free=76.9, wall=1930
2023-05-09 04:45:10 | INFO | train_inner | epoch 013:    288 / 1001 loss=4.775, nll_loss=3.387, intra_distillation_loss=0, ppl=10.46, wps=28102.1, ups=7.89, wpb=3563.9, bsz=142, num_updates=12300, lr=0.000285133, gnorm=0, train_wall=12, gb_free=76.9, wall=1942
2023-05-09 04:45:23 | INFO | train_inner | epoch 013:    388 / 1001 loss=4.711, nll_loss=3.315, intra_distillation_loss=0, ppl=9.95, wps=28242.6, ups=7.77, wpb=3636.5, bsz=148, num_updates=12400, lr=0.000283981, gnorm=0, train_wall=12, gb_free=76.8, wall=1955
2023-05-09 04:45:35 | INFO | train_inner | epoch 013:    488 / 1001 loss=4.764, nll_loss=3.374, intra_distillation_loss=0, ppl=10.37, wps=28271.6, ups=7.9, wpb=3577.2, bsz=138.2, num_updates=12500, lr=0.000282843, gnorm=0, train_wall=12, gb_free=76.7, wall=1968
2023-05-09 04:45:48 | INFO | train_inner | epoch 013:    588 / 1001 loss=4.774, nll_loss=3.387, intra_distillation_loss=0, ppl=10.46, wps=28105.9, ups=7.75, wpb=3628.8, bsz=146.1, num_updates=12600, lr=0.000281718, gnorm=0, train_wall=12, gb_free=77, wall=1981
2023-05-09 04:46:01 | INFO | train_inner | epoch 013:    688 / 1001 loss=4.802, nll_loss=3.419, intra_distillation_loss=0, ppl=10.69, wps=27888.7, ups=7.8, wpb=3577.2, bsz=138.6, num_updates=12700, lr=0.000280607, gnorm=0, train_wall=12, gb_free=76.8, wall=1994
2023-05-09 04:46:14 | INFO | train_inner | epoch 013:    788 / 1001 loss=4.784, nll_loss=3.399, intra_distillation_loss=0, ppl=10.55, wps=28012.7, ups=7.83, wpb=3577, bsz=138, num_updates=12800, lr=0.000279508, gnorm=0, train_wall=12, gb_free=76.8, wall=2006
2023-05-09 04:46:26 | INFO | train_inner | epoch 013:    888 / 1001 loss=4.882, nll_loss=3.509, intra_distillation_loss=0, ppl=11.39, wps=27929.3, ups=7.91, wpb=3528.8, bsz=131.4, num_updates=12900, lr=0.000278423, gnorm=0, train_wall=12, gb_free=77.3, wall=2019
2023-05-09 04:46:39 | INFO | train_inner | epoch 013:    988 / 1001 loss=4.778, nll_loss=3.392, intra_distillation_loss=0, ppl=10.5, wps=28470.3, ups=7.77, wpb=3665.8, bsz=145, num_updates=13000, lr=0.00027735, gnorm=0, train_wall=12, gb_free=76.7, wall=2032
2023-05-09 04:46:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:46:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:47:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.762 | nll_loss 3.24 | intra_distillation_loss 0 | ppl 9.45 | bleu 23.26 | wps 4241.7 | wpb 2727.5 | bsz 105.9 | num_updates 13013 | best_bleu 23.26
2023-05-09 04:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 13013 updates
2023-05-09 04:47:20 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:47:21 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:47:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 13 @ 13013 updates, score 23.26) (writing took 1.81033173808828 seconds)
2023-05-09 04:47:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-09 04:47:21 | INFO | train | epoch 013 | loss 4.787 | nll_loss 3.402 | intra_distillation_loss 0 | ppl 10.57 | wps 21337 | ups 5.95 | wpb 3588.2 | bsz 139.6 | num_updates 13013 | lr 0.000277212 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2074
2023-05-09 04:47:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:47:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:47:21 | INFO | fairseq.trainer | begin training epoch 14
2023-05-09 04:47:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:47:33 | INFO | train_inner | epoch 014:     87 / 1001 loss=4.691, nll_loss=3.293, intra_distillation_loss=0, ppl=9.8, wps=6771.8, ups=1.87, wpb=3620.1, bsz=142, num_updates=13100, lr=0.000276289, gnorm=0, train_wall=12, gb_free=77.1, wall=2085
2023-05-09 04:47:45 | INFO | train_inner | epoch 014:    187 / 1001 loss=4.733, nll_loss=3.34, intra_distillation_loss=0, ppl=10.13, wps=27733.6, ups=7.86, wpb=3527.1, bsz=139.1, num_updates=13200, lr=0.000275241, gnorm=0, train_wall=12, gb_free=76.8, wall=2098
2023-05-09 04:47:58 | INFO | train_inner | epoch 014:    287 / 1001 loss=4.785, nll_loss=3.399, intra_distillation_loss=0, ppl=10.55, wps=27942.5, ups=7.76, wpb=3599, bsz=136.2, num_updates=13300, lr=0.000274204, gnorm=0, train_wall=12, gb_free=76.8, wall=2111
2023-05-09 04:48:11 | INFO | train_inner | epoch 014:    387 / 1001 loss=4.796, nll_loss=3.409, intra_distillation_loss=0, ppl=10.62, wps=28330.9, ups=7.93, wpb=3573.4, bsz=113.6, num_updates=13400, lr=0.000273179, gnorm=0, train_wall=12, gb_free=77, wall=2124
2023-05-09 04:48:24 | INFO | train_inner | epoch 014:    487 / 1001 loss=4.7, nll_loss=3.301, intra_distillation_loss=0, ppl=9.86, wps=28206.3, ups=7.85, wpb=3594.3, bsz=144.7, num_updates=13500, lr=0.000272166, gnorm=0, train_wall=12, gb_free=77.1, wall=2136
2023-05-09 04:48:37 | INFO | train_inner | epoch 014:    587 / 1001 loss=4.666, nll_loss=3.264, intra_distillation_loss=0, ppl=9.6, wps=28223.3, ups=7.77, wpb=3631.7, bsz=152.4, num_updates=13600, lr=0.000271163, gnorm=0, train_wall=12, gb_free=76.9, wall=2149
2023-05-09 04:48:49 | INFO | train_inner | epoch 014:    687 / 1001 loss=4.729, nll_loss=3.335, intra_distillation_loss=0, ppl=10.09, wps=27923.7, ups=7.85, wpb=3557.3, bsz=141, num_updates=13700, lr=0.000270172, gnorm=0, train_wall=12, gb_free=77, wall=2162
2023-05-09 04:49:02 | INFO | train_inner | epoch 014:    787 / 1001 loss=4.745, nll_loss=3.353, intra_distillation_loss=0, ppl=10.22, wps=28073.9, ups=7.81, wpb=3593.9, bsz=138.8, num_updates=13800, lr=0.000269191, gnorm=0, train_wall=12, gb_free=76.9, wall=2175
2023-05-09 04:49:15 | INFO | train_inner | epoch 014:    887 / 1001 loss=4.72, nll_loss=3.324, intra_distillation_loss=0, ppl=10.02, wps=28157.5, ups=7.86, wpb=3580.9, bsz=140.2, num_updates=13900, lr=0.000268221, gnorm=0, train_wall=12, gb_free=76.9, wall=2187
2023-05-09 04:49:28 | INFO | train_inner | epoch 014:    987 / 1001 loss=4.698, nll_loss=3.3, intra_distillation_loss=0, ppl=9.85, wps=28311.1, ups=7.8, wpb=3631.1, bsz=147.9, num_updates=14000, lr=0.000267261, gnorm=0, train_wall=12, gb_free=76.9, wall=2200
2023-05-09 04:49:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:49:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:50:08 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.741 | nll_loss 3.209 | intra_distillation_loss 0 | ppl 9.24 | bleu 24.09 | wps 4197.1 | wpb 2727.5 | bsz 105.9 | num_updates 14014 | best_bleu 24.09
2023-05-09 04:50:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 14014 updates
2023-05-09 04:50:08 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:50:09 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 14 @ 14014 updates, score 24.09) (writing took 1.904664009111002 seconds)
2023-05-09 04:50:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-09 04:50:10 | INFO | train | epoch 014 | loss 4.727 | nll_loss 3.333 | intra_distillation_loss 0 | ppl 10.08 | wps 21261.3 | ups 5.93 | wpb 3588.2 | bsz 139.6 | num_updates 14014 | lr 0.000267128 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2243
2023-05-09 04:50:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:50:10 | INFO | fairseq.trainer | begin training epoch 15
2023-05-09 04:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:50:22 | INFO | train_inner | epoch 015:     86 / 1001 loss=4.65, nll_loss=3.246, intra_distillation_loss=0, ppl=9.49, wps=6587.4, ups=1.86, wpb=3548.6, bsz=143.3, num_updates=14100, lr=0.000266312, gnorm=0, train_wall=12, gb_free=76.8, wall=2254
2023-05-09 04:50:34 | INFO | train_inner | epoch 015:    186 / 1001 loss=4.719, nll_loss=3.324, intra_distillation_loss=0, ppl=10.01, wps=27820.9, ups=7.92, wpb=3514.9, bsz=127.6, num_updates=14200, lr=0.000265372, gnorm=0, train_wall=12, gb_free=76.8, wall=2267
2023-05-09 04:50:47 | INFO | train_inner | epoch 015:    286 / 1001 loss=4.686, nll_loss=3.285, intra_distillation_loss=0, ppl=9.75, wps=27750.4, ups=7.9, wpb=3511.1, bsz=138.2, num_updates=14300, lr=0.000264443, gnorm=0, train_wall=12, gb_free=76.8, wall=2279
2023-05-09 04:51:00 | INFO | train_inner | epoch 015:    386 / 1001 loss=4.696, nll_loss=3.296, intra_distillation_loss=0, ppl=9.82, wps=28233.4, ups=7.88, wpb=3584.1, bsz=128.1, num_updates=14400, lr=0.000263523, gnorm=0, train_wall=12, gb_free=76.8, wall=2292
2023-05-09 04:51:12 | INFO | train_inner | epoch 015:    486 / 1001 loss=4.647, nll_loss=3.241, intra_distillation_loss=0, ppl=9.46, wps=28107.5, ups=7.76, wpb=3621.8, bsz=149.5, num_updates=14500, lr=0.000262613, gnorm=0, train_wall=12, gb_free=76.9, wall=2305
2023-05-09 04:51:25 | INFO | train_inner | epoch 015:    586 / 1001 loss=4.723, nll_loss=3.328, intra_distillation_loss=0, ppl=10.04, wps=27808.3, ups=7.84, wpb=3547.5, bsz=132.4, num_updates=14600, lr=0.000261712, gnorm=0, train_wall=12, gb_free=76.8, wall=2318
2023-05-09 04:51:38 | INFO | train_inner | epoch 015:    686 / 1001 loss=4.712, nll_loss=3.315, intra_distillation_loss=0, ppl=9.95, wps=28336.5, ups=7.82, wpb=3623.5, bsz=131, num_updates=14700, lr=0.00026082, gnorm=0, train_wall=12, gb_free=76.9, wall=2331
2023-05-09 04:51:51 | INFO | train_inner | epoch 015:    786 / 1001 loss=4.635, nll_loss=3.228, intra_distillation_loss=0, ppl=9.37, wps=28463.3, ups=7.78, wpb=3658.3, bsz=155.8, num_updates=14800, lr=0.000259938, gnorm=0, train_wall=12, gb_free=76.8, wall=2343
2023-05-09 04:52:04 | INFO | train_inner | epoch 015:    886 / 1001 loss=4.666, nll_loss=3.264, intra_distillation_loss=0, ppl=9.61, wps=28034.5, ups=7.75, wpb=3619.1, bsz=149.8, num_updates=14900, lr=0.000259064, gnorm=0, train_wall=12, gb_free=77.1, wall=2356
2023-05-09 04:52:16 | INFO | train_inner | epoch 015:    986 / 1001 loss=4.687, nll_loss=3.288, intra_distillation_loss=0, ppl=9.77, wps=28291.5, ups=7.84, wpb=3607.5, bsz=140, num_updates=15000, lr=0.000258199, gnorm=0, train_wall=12, gb_free=77.1, wall=2369
2023-05-09 04:52:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:52:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:52:57 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.712 | nll_loss 3.182 | intra_distillation_loss 0 | ppl 9.08 | bleu 24.31 | wps 4250.5 | wpb 2727.5 | bsz 105.9 | num_updates 15015 | best_bleu 24.31
2023-05-09 04:52:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 15015 updates
2023-05-09 04:52:57 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 15 @ 15015 updates, score 24.31) (writing took 1.9906434409786016 seconds)
2023-05-09 04:52:59 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-09 04:52:59 | INFO | train | epoch 015 | loss 4.679 | nll_loss 3.278 | intra_distillation_loss 0 | ppl 9.7 | wps 21304 | ups 5.94 | wpb 3588.2 | bsz 139.6 | num_updates 15015 | lr 0.00025807 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2412
2023-05-09 04:52:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:52:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:52:59 | INFO | fairseq.trainer | begin training epoch 16
2023-05-09 04:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:53:10 | INFO | train_inner | epoch 016:     85 / 1001 loss=4.592, nll_loss=3.179, intra_distillation_loss=0, ppl=9.06, wps=6865.8, ups=1.87, wpb=3669.7, bsz=139.8, num_updates=15100, lr=0.000257343, gnorm=0, train_wall=12, gb_free=76.9, wall=2423
2023-05-09 04:53:23 | INFO | train_inner | epoch 016:    185 / 1001 loss=4.711, nll_loss=3.314, intra_distillation_loss=0, ppl=9.94, wps=28252.2, ups=7.9, wpb=3574.1, bsz=112.3, num_updates=15200, lr=0.000256495, gnorm=0, train_wall=12, gb_free=77.1, wall=2435
2023-05-09 04:53:35 | INFO | train_inner | epoch 016:    285 / 1001 loss=4.624, nll_loss=3.215, intra_distillation_loss=0, ppl=9.29, wps=28218.3, ups=7.84, wpb=3598.3, bsz=138.6, num_updates=15300, lr=0.000255655, gnorm=0, train_wall=12, gb_free=77.1, wall=2448
2023-05-09 04:53:48 | INFO | train_inner | epoch 016:    385 / 1001 loss=4.552, nll_loss=3.134, intra_distillation_loss=0, ppl=8.78, wps=28025.1, ups=7.77, wpb=3608, bsz=155, num_updates=15400, lr=0.000254824, gnorm=0, train_wall=12, gb_free=77, wall=2461
2023-05-09 04:54:01 | INFO | train_inner | epoch 016:    485 / 1001 loss=4.639, nll_loss=3.232, intra_distillation_loss=0, ppl=9.39, wps=28074.4, ups=7.85, wpb=3577.1, bsz=137, num_updates=15500, lr=0.000254, gnorm=0, train_wall=12, gb_free=77.1, wall=2474
2023-05-09 04:54:14 | INFO | train_inner | epoch 016:    585 / 1001 loss=4.607, nll_loss=3.196, intra_distillation_loss=0, ppl=9.16, wps=28162, ups=7.85, wpb=3589.7, bsz=151.4, num_updates=15600, lr=0.000253185, gnorm=0, train_wall=12, gb_free=77.4, wall=2486
2023-05-09 04:54:26 | INFO | train_inner | epoch 016:    685 / 1001 loss=4.7, nll_loss=3.301, intra_distillation_loss=0, ppl=9.86, wps=28085.9, ups=7.85, wpb=3578.7, bsz=128.2, num_updates=15700, lr=0.000252377, gnorm=0, train_wall=12, gb_free=76.9, wall=2499
2023-05-09 04:54:39 | INFO | train_inner | epoch 016:    785 / 1001 loss=4.672, nll_loss=3.27, intra_distillation_loss=0, ppl=9.65, wps=28106.6, ups=7.86, wpb=3577.3, bsz=138.3, num_updates=15800, lr=0.000251577, gnorm=0, train_wall=12, gb_free=77.1, wall=2512
2023-05-09 04:54:52 | INFO | train_inner | epoch 016:    885 / 1001 loss=4.659, nll_loss=3.256, intra_distillation_loss=0, ppl=9.55, wps=27662.3, ups=7.8, wpb=3544.9, bsz=135.8, num_updates=15900, lr=0.000250785, gnorm=0, train_wall=12, gb_free=76.9, wall=2525
2023-05-09 04:55:05 | INFO | train_inner | epoch 016:    985 / 1001 loss=4.6, nll_loss=3.189, intra_distillation_loss=0, ppl=9.12, wps=27715, ups=7.75, wpb=3575.3, bsz=158.2, num_updates=16000, lr=0.00025, gnorm=0, train_wall=12, gb_free=77, wall=2537
2023-05-09 04:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:55:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:55:47 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.67 | nll_loss 3.125 | intra_distillation_loss 0 | ppl 8.72 | bleu 24.91 | wps 4042.4 | wpb 2727.5 | bsz 105.9 | num_updates 16016 | best_bleu 24.91
2023-05-09 04:55:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 16016 updates
2023-05-09 04:55:47 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:55:48 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:55:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 16 @ 16016 updates, score 24.91) (writing took 1.870518608018756 seconds)
2023-05-09 04:55:49 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-09 04:55:49 | INFO | train | epoch 016 | loss 4.635 | nll_loss 3.227 | intra_distillation_loss 0 | ppl 9.37 | wps 21087 | ups 5.88 | wpb 3588.2 | bsz 139.6 | num_updates 16016 | lr 0.000249875 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2582
2023-05-09 04:55:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:55:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:55:49 | INFO | fairseq.trainer | begin training epoch 17
2023-05-09 04:55:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:56:00 | INFO | train_inner | epoch 017:     84 / 1001 loss=4.614, nll_loss=3.204, intra_distillation_loss=0, ppl=9.21, wps=6430.4, ups=1.81, wpb=3554.2, bsz=130.2, num_updates=16100, lr=0.000249222, gnorm=0, train_wall=12, gb_free=76.8, wall=2593
2023-05-09 04:56:13 | INFO | train_inner | epoch 017:    184 / 1001 loss=4.456, nll_loss=3.024, intra_distillation_loss=0, ppl=8.13, wps=28328.1, ups=7.84, wpb=3611.2, bsz=157.2, num_updates=16200, lr=0.000248452, gnorm=0, train_wall=12, gb_free=76.9, wall=2605
2023-05-09 04:56:26 | INFO | train_inner | epoch 017:    284 / 1001 loss=4.535, nll_loss=3.114, intra_distillation_loss=0, ppl=8.66, wps=27533, ups=7.76, wpb=3545.9, bsz=148.9, num_updates=16300, lr=0.000247689, gnorm=0, train_wall=12, gb_free=76.8, wall=2618
2023-05-09 04:56:39 | INFO | train_inner | epoch 017:    384 / 1001 loss=4.571, nll_loss=3.154, intra_distillation_loss=0, ppl=8.9, wps=28199.8, ups=7.76, wpb=3635.9, bsz=143.9, num_updates=16400, lr=0.000246932, gnorm=0, train_wall=12, gb_free=76.8, wall=2631
2023-05-09 04:56:51 | INFO | train_inner | epoch 017:    484 / 1001 loss=4.641, nll_loss=3.234, intra_distillation_loss=0, ppl=9.41, wps=27900.3, ups=7.95, wpb=3510.8, bsz=135.8, num_updates=16500, lr=0.000246183, gnorm=0, train_wall=12, gb_free=76.9, wall=2644
2023-05-09 04:57:04 | INFO | train_inner | epoch 017:    584 / 1001 loss=4.599, nll_loss=3.186, intra_distillation_loss=0, ppl=9.1, wps=28356.9, ups=7.81, wpb=3632.9, bsz=139.8, num_updates=16600, lr=0.00024544, gnorm=0, train_wall=12, gb_free=76.7, wall=2657
2023-05-09 04:57:17 | INFO | train_inner | epoch 017:    684 / 1001 loss=4.59, nll_loss=3.176, intra_distillation_loss=0, ppl=9.04, wps=28386.9, ups=7.83, wpb=3625.1, bsz=138.2, num_updates=16700, lr=0.000244704, gnorm=0, train_wall=12, gb_free=76.7, wall=2669
2023-05-09 04:57:30 | INFO | train_inner | epoch 017:    784 / 1001 loss=4.615, nll_loss=3.205, intra_distillation_loss=0, ppl=9.22, wps=28173.9, ups=7.75, wpb=3636.9, bsz=133.5, num_updates=16800, lr=0.000243975, gnorm=0, train_wall=12, gb_free=76.9, wall=2682
2023-05-09 04:57:43 | INFO | train_inner | epoch 017:    884 / 1001 loss=4.586, nll_loss=3.172, intra_distillation_loss=0, ppl=9.01, wps=28150.7, ups=7.8, wpb=3609.2, bsz=154.6, num_updates=16900, lr=0.000243252, gnorm=0, train_wall=12, gb_free=76.8, wall=2695
2023-05-09 04:57:55 | INFO | train_inner | epoch 017:    984 / 1001 loss=4.708, nll_loss=3.311, intra_distillation_loss=0, ppl=9.92, wps=28084.2, ups=7.92, wpb=3544.1, bsz=116.8, num_updates=17000, lr=0.000242536, gnorm=0, train_wall=12, gb_free=76.8, wall=2708
2023-05-09 04:57:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:57:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:58:37 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.638 | nll_loss 3.091 | intra_distillation_loss 0 | ppl 8.52 | bleu 25.11 | wps 4082 | wpb 2727.5 | bsz 105.9 | num_updates 17017 | best_bleu 25.11
2023-05-09 04:58:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 17017 updates
2023-05-09 04:58:37 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:58:38 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 04:58:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 17 @ 17017 updates, score 25.11) (writing took 1.8096753240097314 seconds)
2023-05-09 04:58:39 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-09 04:58:39 | INFO | train | epoch 017 | loss 4.593 | nll_loss 3.18 | intra_distillation_loss 0 | ppl 9.06 | wps 21134.9 | ups 5.89 | wpb 3588.2 | bsz 139.6 | num_updates 17017 | lr 0.000242414 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 2752
2023-05-09 04:58:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:58:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 04:58:39 | INFO | fairseq.trainer | begin training epoch 18
2023-05-09 04:58:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:58:50 | INFO | train_inner | epoch 018:     83 / 1001 loss=4.629, nll_loss=3.22, intra_distillation_loss=0, ppl=9.32, wps=6430, ups=1.83, wpb=3514.6, bsz=120.2, num_updates=17100, lr=0.000241825, gnorm=0, train_wall=12, gb_free=76.7, wall=2762
2023-05-09 04:59:03 | INFO | train_inner | epoch 018:    183 / 1001 loss=4.56, nll_loss=3.142, intra_distillation_loss=0, ppl=8.83, wps=28171, ups=7.89, wpb=3569.4, bsz=125, num_updates=17200, lr=0.000241121, gnorm=0, train_wall=12, gb_free=77, wall=2775
2023-05-09 04:59:15 | INFO | train_inner | epoch 018:    283 / 1001 loss=4.606, nll_loss=3.195, intra_distillation_loss=0, ppl=9.15, wps=27830, ups=7.89, wpb=3526.7, bsz=131.2, num_updates=17300, lr=0.000240424, gnorm=0, train_wall=12, gb_free=76.8, wall=2788
2023-05-09 04:59:28 | INFO | train_inner | epoch 018:    383 / 1001 loss=4.515, nll_loss=3.09, intra_distillation_loss=0, ppl=8.52, wps=28338.3, ups=7.8, wpb=3632.2, bsz=149.7, num_updates=17400, lr=0.000239732, gnorm=0, train_wall=12, gb_free=76.9, wall=2801
2023-05-09 04:59:41 | INFO | train_inner | epoch 018:    483 / 1001 loss=4.504, nll_loss=3.079, intra_distillation_loss=0, ppl=8.45, wps=28182.9, ups=7.7, wpb=3661.4, bsz=154.7, num_updates=17500, lr=0.000239046, gnorm=0, train_wall=12, gb_free=76.8, wall=2814
2023-05-09 04:59:54 | INFO | train_inner | epoch 018:    583 / 1001 loss=4.536, nll_loss=3.115, intra_distillation_loss=0, ppl=8.66, wps=28376, ups=7.81, wpb=3634, bsz=144.2, num_updates=17600, lr=0.000238366, gnorm=0, train_wall=12, gb_free=76.9, wall=2826
2023-05-09 05:00:07 | INFO | train_inner | epoch 018:    683 / 1001 loss=4.453, nll_loss=3.02, intra_distillation_loss=0, ppl=8.11, wps=28377, ups=7.72, wpb=3675.1, bsz=166.5, num_updates=17700, lr=0.000237691, gnorm=0, train_wall=12, gb_free=77, wall=2839
2023-05-09 05:00:19 | INFO | train_inner | epoch 018:    783 / 1001 loss=4.622, nll_loss=3.213, intra_distillation_loss=0, ppl=9.27, wps=27782.4, ups=7.93, wpb=3503.3, bsz=132.9, num_updates=17800, lr=0.000237023, gnorm=0, train_wall=12, gb_free=76.9, wall=2852
2023-05-09 05:00:32 | INFO | train_inner | epoch 018:    883 / 1001 loss=4.575, nll_loss=3.159, intra_distillation_loss=0, ppl=8.93, wps=27966.6, ups=7.86, wpb=3560.1, bsz=144.2, num_updates=17900, lr=0.00023636, gnorm=0, train_wall=12, gb_free=76.9, wall=2865
2023-05-09 05:00:45 | INFO | train_inner | epoch 018:    983 / 1001 loss=4.621, nll_loss=3.212, intra_distillation_loss=0, ppl=9.26, wps=27589.8, ups=7.79, wpb=3543.7, bsz=125.5, num_updates=18000, lr=0.000235702, gnorm=0, train_wall=12, gb_free=76.9, wall=2878
2023-05-09 05:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 05:00:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:01:28 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.608 | nll_loss 3.057 | intra_distillation_loss 0 | ppl 8.32 | bleu 25.75 | wps 4042.4 | wpb 2727.5 | bsz 105.9 | num_updates 18018 | best_bleu 25.75
2023-05-09 05:01:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 18018 updates
2023-05-09 05:01:28 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 05:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 05:01:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 18 @ 18018 updates, score 25.75) (writing took 1.8563774731010199 seconds)
2023-05-09 05:01:30 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-09 05:01:30 | INFO | train | epoch 018 | loss 4.556 | nll_loss 3.138 | intra_distillation_loss 0 | ppl 8.8 | wps 21083.3 | ups 5.88 | wpb 3588.2 | bsz 139.6 | num_updates 18018 | lr 0.000235584 | gnorm 0 | train_wall 117 | gb_free 76.7 | wall 2922
2023-05-09 05:01:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:01:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 05:01:30 | INFO | fairseq.trainer | begin training epoch 19
2023-05-09 05:01:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 05:01:40 | INFO | train_inner | epoch 019:     82 / 1001 loss=4.449, nll_loss=3.016, intra_distillation_loss=0, ppl=8.09, wps=6689.5, ups=1.81, wpb=3701.7, bsz=138.6, num_updates=18100, lr=0.00023505, gnorm=0, train_wall=12, gb_free=76.8, wall=2933
2023-05-09 05:01:53 | INFO | train_inner | epoch 019:    182 / 1001 loss=4.507, nll_loss=3.082, intra_distillation_loss=0, ppl=8.47, wps=27735.6, ups=7.79, wpb=3559.4, bsz=145.2, num_updates=18200, lr=0.000234404, gnorm=0, train_wall=12, gb_free=76.9, wall=2946
2023-05-09 05:02:06 | INFO | train_inner | epoch 019:    282 / 1001 loss=4.46, nll_loss=3.028, intra_distillation_loss=0, ppl=8.16, wps=28174.5, ups=7.78, wpb=3621.3, bsz=142.6, num_updates=18300, lr=0.000233762, gnorm=0, train_wall=12, gb_free=76.9, wall=2959
2023-05-09 05:02:19 | INFO | train_inner | epoch 019:    382 / 1001 loss=4.549, nll_loss=3.13, intra_distillation_loss=0, ppl=8.75, wps=27482.8, ups=7.88, wpb=3486.9, bsz=137.1, num_updates=18400, lr=0.000233126, gnorm=0, train_wall=12, gb_free=76.9, wall=2971
2023-05-09 05:02:32 | INFO | train_inner | epoch 019:    482 / 1001 loss=4.503, nll_loss=3.078, intra_distillation_loss=0, ppl=8.44, wps=28311, ups=7.78, wpb=3639.2, bsz=147.3, num_updates=18500, lr=0.000232495, gnorm=0, train_wall=12, gb_free=77, wall=2984
2023-05-09 05:02:44 | INFO | train_inner | epoch 019:    582 / 1001 loss=4.556, nll_loss=3.137, intra_distillation_loss=0, ppl=8.8, wps=27932.7, ups=7.93, wpb=3522.9, bsz=134.5, num_updates=18600, lr=0.000231869, gnorm=0, train_wall=12, gb_free=77.1, wall=2997
2023-05-09 05:02:57 | INFO | train_inner | epoch 019:    682 / 1001 loss=4.575, nll_loss=3.158, intra_distillation_loss=0, ppl=8.93, wps=28249.9, ups=7.85, wpb=3597.6, bsz=130.2, num_updates=18700, lr=0.000231249, gnorm=0, train_wall=12, gb_free=76.9, wall=3009
2023-05-09 05:03:10 | INFO | train_inner | epoch 019:    782 / 1001 loss=4.47, nll_loss=3.041, intra_distillation_loss=0, ppl=8.23, wps=27702.6, ups=7.73, wpb=3582.4, bsz=155.5, num_updates=18800, lr=0.000230633, gnorm=0, train_wall=12, gb_free=76.7, wall=3022
2023-05-09 05:03:23 | INFO | train_inner | epoch 019:    882 / 1001 loss=4.504, nll_loss=3.078, intra_distillation_loss=0, ppl=8.45, wps=28107.5, ups=7.83, wpb=3588.1, bsz=145, num_updates=18900, lr=0.000230022, gnorm=0, train_wall=12, gb_free=76.9, wall=3035
2023-05-09 05:03:35 | INFO | train_inner | epoch 019:    982 / 1001 loss=4.601, nll_loss=3.188, intra_distillation_loss=0, ppl=9.12, wps=28605.9, ups=7.87, wpb=3635.8, bsz=121.9, num_updates=19000, lr=0.000229416, gnorm=0, train_wall=12, gb_free=76.8, wall=3048
2023-05-09 05:03:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 05:03:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:04:17 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.593 | nll_loss 3.04 | intra_distillation_loss 0 | ppl 8.22 | bleu 25.72 | wps 4154.4 | wpb 2727.5 | bsz 105.9 | num_updates 19019 | best_bleu 25.75
2023-05-09 05:04:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 19019 updates
2023-05-09 05:04:17 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt
2023-05-09 05:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt
2023-05-09 05:04:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_last.pt (epoch 19 @ 19019 updates, score 25.72) (writing took 0.8902986920438707 seconds)
2023-05-09 05:04:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-09 05:04:18 | INFO | train | epoch 019 | loss 4.521 | nll_loss 3.098 | intra_distillation_loss 0 | ppl 8.56 | wps 21329.6 | ups 5.94 | wpb 3588.2 | bsz 139.6 | num_updates 19019 | lr 0.000229301 | gnorm 0 | train_wall 117 | gb_free 76.8 | wall 3091
2023-05-09 05:04:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:04:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1001
2023-05-09 05:04:18 | INFO | fairseq.trainer | begin training epoch 20
2023-05-09 05:04:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 05:04:28 | INFO | train_inner | epoch 020:     81 / 1001 loss=4.518, nll_loss=3.094, intra_distillation_loss=0, ppl=8.54, wps=6756, ups=1.88, wpb=3595.2, bsz=128.4, num_updates=19100, lr=0.000228814, gnorm=0, train_wall=12, gb_free=76.8, wall=3101
2023-05-09 05:04:41 | INFO | train_inner | epoch 020:    181 / 1001 loss=4.463, nll_loss=3.032, intra_distillation_loss=0, ppl=8.18, wps=27910.1, ups=7.91, wpb=3526.9, bsz=136, num_updates=19200, lr=0.000228218, gnorm=0, train_wall=12, gb_free=76.8, wall=3114
2023-05-09 05:04:54 | INFO | train_inner | epoch 020:    281 / 1001 loss=4.448, nll_loss=3.013, intra_distillation_loss=0, ppl=8.08, wps=28214.8, ups=7.83, wpb=3605, bsz=142.3, num_updates=19300, lr=0.000227626, gnorm=0, train_wall=12, gb_free=77.1, wall=3126
2023-05-09 05:05:07 | INFO | train_inner | epoch 020:    381 / 1001 loss=4.452, nll_loss=3.019, intra_distillation_loss=0, ppl=8.11, wps=27949.4, ups=7.8, wpb=3582.4, bsz=150.6, num_updates=19400, lr=0.000227038, gnorm=0, train_wall=12, gb_free=76.8, wall=3139
2023-05-09 05:05:20 | INFO | train_inner | epoch 020:    481 / 1001 loss=4.532, nll_loss=3.111, intra_distillation_loss=0, ppl=8.64, wps=28054.5, ups=7.77, wpb=3610.6, bsz=139, num_updates=19500, lr=0.000226455, gnorm=0, train_wall=12, gb_free=76.8, wall=3152
2023-05-09 05:05:32 | INFO | train_inner | epoch 020:    581 / 1001 loss=4.461, nll_loss=3.029, intra_distillation_loss=0, ppl=8.16, wps=28862.1, ups=7.83, wpb=3685, bsz=142.6, num_updates=19600, lr=0.000225877, gnorm=0, train_wall=12, gb_free=76.8, wall=3165
2023-05-09 05:05:45 | INFO | train_inner | epoch 020:    681 / 1001 loss=4.497, nll_loss=3.071, intra_distillation_loss=0, ppl=8.4, wps=28128.6, ups=7.78, wpb=3617.3, bsz=134.6, num_updates=19700, lr=0.000225303, gnorm=0, train_wall=12, gb_free=76.8, wall=3178
2023-05-09 05:05:58 | INFO | train_inner | epoch 020:    781 / 1001 loss=4.481, nll_loss=3.052, intra_distillation_loss=0, ppl=8.29, wps=28171.8, ups=7.8, wpb=3613.3, bsz=142.7, num_updates=19800, lr=0.000224733, gnorm=0, train_wall=12, gb_free=77.1, wall=3191
2023-05-09 05:06:11 | INFO | train_inner | epoch 020:    881 / 1001 loss=4.543, nll_loss=3.122, intra_distillation_loss=0, ppl=8.71, wps=27699, ups=7.97, wpb=3474.7, bsz=139.2, num_updates=19900, lr=0.000224168, gnorm=0, train_wall=12, gb_free=77.1, wall=3203
2023-05-09 05:06:23 | INFO | train_inner | epoch 020:    981 / 1001 loss=4.526, nll_loss=3.105, intra_distillation_loss=0, ppl=8.6, wps=27702, ups=7.8, wpb=3553.6, bsz=138.3, num_updates=20000, lr=0.000223607, gnorm=0, train_wall=12, gb_free=76.9, wall=3216
2023-05-09 05:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 05:06:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:07:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.572 | nll_loss 3.015 | intra_distillation_loss 0 | ppl 8.08 | bleu 26.06 | wps 4022.1 | wpb 2727.5 | bsz 105.9 | num_updates 20020 | best_bleu 26.06
2023-05-09 05:07:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 20020 updates
2023-05-09 05:07:07 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 05:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt
2023-05-09 05:07:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_ar/checkpoint_best.pt (epoch 20 @ 20020 updates, score 26.06) (writing took 1.8894129910040647 seconds)
2023-05-09 05:07:09 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-09 05:07:09 | INFO | train | epoch 020 | loss 4.489 | nll_loss 3.061 | intra_distillation_loss 0 | ppl 8.35 | wps 21052.7 | ups 5.87 | wpb 3588.2 | bsz 139.6 | num_updates 20020 | lr 0.000223495 | gnorm 0 | train_wall 117 | gb_free 76.9 | wall 3261
2023-05-09 05:07:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:07:09 | INFO | fairseq_cli.train | done training in 3259.7 seconds
/home/tli104/my_fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
