Direction he to en
2023-05-09 03:17:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
Traceback (most recent call last):
  File "/home/tli104/.conda/envs/fairseqenv/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/tli104/my_fairseq/fairseq_cli/train.py", line 574, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/tli104/my_fairseq/fairseq/distributed/utils.py", line 404, in call_main
    main(cfg, **kwargs)
  File "/home/tli104/my_fairseq/fairseq_cli/train.py", line 58, in main
    assert (
AssertionError: Must specify batch size either with --max-tokens or --batch-size
/cm/local/apps/slurm/var/spool/job15056475/slurm_script: line 28: --weighted-freezing: command not found
/cm/local/apps/slurm/var/spool/job15056475/slurm_script: line 30: --importance-metric: command not found
2023-05-09 03:17:35 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-05-09 03:17:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/scratch4/cs601/tli104/wf_6000/checkpoints_he', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_alpha=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, alpha=5.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/data-bin-he', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, div='X', dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, importance_metric='magnitude', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_updates_train=25000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_iter=2, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch4/cs601/tli104/wf_6000/checkpoints_he', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, smooth_scores=True, source_lang=None, start_freezing=6000, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation_intra_distillation', temperature_p=2, temperature_q=5, tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weighted_freezing=True, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation_intra_distillation', 'data': 'data/data-bin-he', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False, 'alpha': 5.0, 'adaptive_alpha': 0, 'max_updates_train': 25000, 'temperature_q': 5.0, 'temperature_p': 2.0, 'num_iter': 2, 'div': 'X', 'importance_metric': 'magnitude', 'smooth_scores': True, 'weighted_freezing': True, 'start_freezing': 6000}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-09 03:17:37 | INFO | fairseq.tasks.translation | [he] dictionary: 12001 types
2023-05-09 03:17:37 | INFO | fairseq.tasks.translation | [en] dictionary: 12001 types
2023-05-09 03:17:38 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=12001, bias=False)
  )
)
2023-05-09 03:17:38 | INFO | fairseq_cli.train | task: Translation_Intra_Distillation
2023-05-09 03:17:38 | INFO | fairseq_cli.train | model: TransformerModel
2023-05-09 03:17:38 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-09 03:17:38 | INFO | fairseq_cli.train | num. shared model params: 43,832,320 (num. trained: 43,832,320)
2023-05-09 03:17:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-09 03:17:38 | INFO | fairseq.data.data_utils | loaded 6,561 examples from: data/data-bin-he/valid.he-en.he
2023-05-09 03:17:38 | INFO | fairseq.data.data_utils | loaded 6,561 examples from: data/data-bin-he/valid.he-en.en
2023-05-09 03:17:38 | INFO | fairseq.tasks.translation | data/data-bin-he valid he-en 6561 examples
2023-05-09 03:17:39 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-09 03:17:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 03:17:39 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.347 GB ; name = Graphics Device                         
2023-05-09 03:17:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 03:17:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-05-09 03:17:39 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-05-09 03:17:39 | INFO | fairseq.trainer | Preparing to load checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_last.pt
2023-05-09 03:17:39 | INFO | fairseq.trainer | No existing checkpoint found /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_last.pt
2023-05-09 03:17:39 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-09 03:17:39 | INFO | fairseq.data.data_utils | loaded 144,345 examples from: data/data-bin-he/train.he-en.he
2023-05-09 03:17:39 | INFO | fairseq.data.data_utils | loaded 144,345 examples from: data/data-bin-he/train.he-en.en
2023-05-09 03:17:39 | INFO | fairseq.tasks.translation | data/data-bin-he train he-en 144345 examples
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 03:17:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2023-05-09 03:17:39 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 03:17:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 03:17:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:17:41 | INFO | fairseq.trainer | begin training epoch 1
2023-05-09 03:17:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:17:53 | INFO | train_inner | epoch 001:    100 / 986 loss=12.831, nll_loss=12.666, intra_distillation_loss=0, ppl=6498.56, wps=33513.3, ups=9.18, wpb=3651.6, bsz=144.2, num_updates=100, lr=1.25e-05, gnorm=3.807, train_wall=11, gb_free=76.9, wall=13
2023-05-09 03:18:03 | INFO | train_inner | epoch 001:    200 / 986 loss=11.106, nll_loss=10.735, intra_distillation_loss=0, ppl=1704.89, wps=34681.4, ups=9.35, wpb=3710, bsz=137.3, num_updates=200, lr=2.5e-05, gnorm=1.73, train_wall=10, gb_free=77.4, wall=24
2023-05-09 03:18:14 | INFO | train_inner | epoch 001:    300 / 986 loss=10.099, nll_loss=9.574, intra_distillation_loss=0, ppl=762.33, wps=33960.6, ups=9.09, wpb=3735.6, bsz=155.1, num_updates=300, lr=3.75e-05, gnorm=1.678, train_wall=10, gb_free=76.8, wall=35
2023-05-09 03:18:25 | INFO | train_inner | epoch 001:    400 / 986 loss=9.663, nll_loss=9.017, intra_distillation_loss=0, ppl=518.06, wps=33829.6, ups=9.24, wpb=3662.3, bsz=140.1, num_updates=400, lr=5e-05, gnorm=1.58, train_wall=10, gb_free=76.8, wall=46
2023-05-09 03:18:36 | INFO | train_inner | epoch 001:    500 / 986 loss=9.44, nll_loss=8.736, intra_distillation_loss=0, ppl=426.45, wps=33472.1, ups=9.14, wpb=3662.5, bsz=149.8, num_updates=500, lr=6.25e-05, gnorm=1.537, train_wall=10, gb_free=77.2, wall=57
2023-05-09 03:18:47 | INFO | train_inner | epoch 001:    600 / 986 loss=9.205, nll_loss=8.462, intra_distillation_loss=0, ppl=352.71, wps=33348.6, ups=9.16, wpb=3638.8, bsz=163.6, num_updates=600, lr=7.5e-05, gnorm=1.738, train_wall=10, gb_free=76.8, wall=68
2023-05-09 03:18:58 | INFO | train_inner | epoch 001:    700 / 986 loss=9.052, nll_loss=8.288, intra_distillation_loss=0, ppl=312.58, wps=33566.2, ups=9.16, wpb=3662.5, bsz=144.6, num_updates=700, lr=8.75e-05, gnorm=1.587, train_wall=10, gb_free=76.9, wall=79
2023-05-09 03:19:09 | INFO | train_inner | epoch 001:    800 / 986 loss=8.779, nll_loss=7.979, intra_distillation_loss=0, ppl=252.25, wps=34088.7, ups=9.3, wpb=3666.3, bsz=149.5, num_updates=800, lr=0.0001, gnorm=1.655, train_wall=10, gb_free=76.9, wall=90
2023-05-09 03:19:19 | INFO | train_inner | epoch 001:    900 / 986 loss=8.697, nll_loss=7.884, intra_distillation_loss=0, ppl=236.18, wps=33829.8, ups=9.37, wpb=3611.2, bsz=140.5, num_updates=900, lr=0.0001125, gnorm=1.797, train_wall=10, gb_free=76.8, wall=100
2023-05-09 03:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:19:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:20:11 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.292 | nll_loss 7.372 | intra_distillation_loss 0 | ppl 165.7 | bleu 2.02 | wps 3887.2 | wpb 2821.2 | bsz 113.1 | num_updates 986
2023-05-09 03:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 986 updates
2023-05-09 03:20:11 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:20:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 1 @ 986 updates, score 2.02) (writing took 1.7581802579807118 seconds)
2023-05-09 03:20:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-09 03:20:13 | INFO | train | epoch 001 | loss 9.759 | nll_loss 9.126 | intra_distillation_loss 0 | ppl 558.66 | wps 23958.4 | ups 6.53 | wpb 3668.2 | bsz 146.4 | num_updates 986 | lr 0.00012325 | gnorm 1.863 | train_wall 97 | gb_free 76.8 | wall 153
2023-05-09 03:20:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:20:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:20:13 | INFO | fairseq.trainer | begin training epoch 2
2023-05-09 03:20:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:20:14 | INFO | train_inner | epoch 002:     14 / 986 loss=8.492, nll_loss=7.65, intra_distillation_loss=0, ppl=200.86, wps=6725.4, ups=1.82, wpb=3695.2, bsz=140.2, num_updates=1000, lr=0.000125, gnorm=1.473, train_wall=10, gb_free=76.9, wall=155
2023-05-09 03:20:25 | INFO | train_inner | epoch 002:    114 / 986 loss=8.311, nll_loss=7.444, intra_distillation_loss=0, ppl=174.15, wps=34343, ups=9.22, wpb=3726.8, bsz=147.2, num_updates=1100, lr=0.0001375, gnorm=1.606, train_wall=10, gb_free=76.7, wall=166
2023-05-09 03:20:36 | INFO | train_inner | epoch 002:    214 / 986 loss=8.098, nll_loss=7.202, intra_distillation_loss=0, ppl=147.25, wps=34193.3, ups=9.17, wpb=3727.3, bsz=168, num_updates=1200, lr=0.00015, gnorm=1.582, train_wall=10, gb_free=76.8, wall=177
2023-05-09 03:20:47 | INFO | train_inner | epoch 002:    314 / 986 loss=8.231, nll_loss=7.351, intra_distillation_loss=0, ppl=163.25, wps=32843.7, ups=9.2, wpb=3570.3, bsz=144.1, num_updates=1300, lr=0.0001625, gnorm=1.538, train_wall=10, gb_free=76.8, wall=188
2023-05-09 03:20:58 | INFO | train_inner | epoch 002:    414 / 986 loss=8.142, nll_loss=7.249, intra_distillation_loss=0, ppl=152.08, wps=34056.8, ups=9.28, wpb=3669.1, bsz=131.5, num_updates=1400, lr=0.000175, gnorm=1.417, train_wall=10, gb_free=76.8, wall=199
2023-05-09 03:21:09 | INFO | train_inner | epoch 002:    514 / 986 loss=7.982, nll_loss=7.069, intra_distillation_loss=0, ppl=134.23, wps=33537.9, ups=9.14, wpb=3668.4, bsz=152.5, num_updates=1500, lr=0.0001875, gnorm=1.356, train_wall=10, gb_free=76.8, wall=209
2023-05-09 03:21:19 | INFO | train_inner | epoch 002:    614 / 986 loss=7.857, nll_loss=6.925, intra_distillation_loss=0, ppl=121.49, wps=34303.1, ups=9.27, wpb=3699.8, bsz=146, num_updates=1600, lr=0.0002, gnorm=1.31, train_wall=10, gb_free=76.9, wall=220
2023-05-09 03:21:30 | INFO | train_inner | epoch 002:    714 / 986 loss=7.931, nll_loss=7.007, intra_distillation_loss=0, ppl=128.62, wps=33482.2, ups=9.3, wpb=3601.7, bsz=141.9, num_updates=1700, lr=0.0002125, gnorm=1.396, train_wall=10, gb_free=77, wall=231
2023-05-09 03:21:41 | INFO | train_inner | epoch 002:    814 / 986 loss=7.853, nll_loss=6.921, intra_distillation_loss=0, ppl=121.15, wps=33833.9, ups=9.22, wpb=3671.5, bsz=139, num_updates=1800, lr=0.000225, gnorm=1.252, train_wall=10, gb_free=76.8, wall=242
2023-05-09 03:21:52 | INFO | train_inner | epoch 002:    914 / 986 loss=7.704, nll_loss=6.751, intra_distillation_loss=0, ppl=107.68, wps=33652.3, ups=9.15, wpb=3679.4, bsz=155.8, num_updates=1900, lr=0.0002375, gnorm=1.334, train_wall=10, gb_free=76.8, wall=253
2023-05-09 03:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:21:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:22:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.464 | nll_loss 6.43 | intra_distillation_loss 0 | ppl 86.21 | bleu 2.88 | wps 4089.2 | wpb 2821.2 | bsz 113.1 | num_updates 1972 | best_bleu 2.88
2023-05-09 03:22:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1972 updates
2023-05-09 03:22:40 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 2 @ 1972 updates, score 2.88) (writing took 1.9175306420074776 seconds)
2023-05-09 03:22:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-09 03:22:42 | INFO | train | epoch 002 | loss 8 | nll_loss 7.089 | intra_distillation_loss 0 | ppl 136.12 | wps 24271.9 | ups 6.62 | wpb 3668.2 | bsz 146.4 | num_updates 1972 | lr 0.0002465 | gnorm 1.42 | train_wall 96 | gb_free 76.7 | wall 302
2023-05-09 03:22:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:22:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:22:42 | INFO | fairseq.trainer | begin training epoch 3
2023-05-09 03:22:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:22:45 | INFO | train_inner | epoch 003:     28 / 986 loss=7.767, nll_loss=6.822, intra_distillation_loss=0, ppl=113.12, wps=6897.3, ups=1.89, wpb=3645.3, bsz=131.4, num_updates=2000, lr=0.00025, gnorm=1.333, train_wall=10, gb_free=76.8, wall=306
2023-05-09 03:22:56 | INFO | train_inner | epoch 003:    128 / 986 loss=7.609, nll_loss=6.641, intra_distillation_loss=0, ppl=99.83, wps=33760.6, ups=9.13, wpb=3697.7, bsz=143.9, num_updates=2100, lr=0.0002625, gnorm=1.222, train_wall=10, gb_free=76.9, wall=317
2023-05-09 03:23:07 | INFO | train_inner | epoch 003:    228 / 986 loss=7.544, nll_loss=6.568, intra_distillation_loss=0, ppl=94.89, wps=33573.7, ups=9.23, wpb=3637.9, bsz=142.2, num_updates=2200, lr=0.000275, gnorm=1.203, train_wall=10, gb_free=76.9, wall=327
2023-05-09 03:23:17 | INFO | train_inner | epoch 003:    328 / 986 loss=7.568, nll_loss=6.594, intra_distillation_loss=0, ppl=96.61, wps=33694.9, ups=9.24, wpb=3647.1, bsz=142.8, num_updates=2300, lr=0.0002875, gnorm=1.29, train_wall=10, gb_free=77, wall=338
2023-05-09 03:23:28 | INFO | train_inner | epoch 003:    428 / 986 loss=7.395, nll_loss=6.4, intra_distillation_loss=0, ppl=84.43, wps=34110.6, ups=9.28, wpb=3676.1, bsz=141.4, num_updates=2400, lr=0.0003, gnorm=1.213, train_wall=10, gb_free=76.7, wall=349
2023-05-09 03:23:39 | INFO | train_inner | epoch 003:    528 / 986 loss=7.256, nll_loss=6.239, intra_distillation_loss=0, ppl=75.54, wps=33509.9, ups=9.22, wpb=3633.2, bsz=159, num_updates=2500, lr=0.0003125, gnorm=1.34, train_wall=10, gb_free=76.8, wall=360
2023-05-09 03:23:50 | INFO | train_inner | epoch 003:    628 / 986 loss=7.143, nll_loss=6.11, intra_distillation_loss=0, ppl=69.08, wps=34515.2, ups=9.23, wpb=3737.7, bsz=151.8, num_updates=2600, lr=0.000325, gnorm=1.177, train_wall=10, gb_free=76.9, wall=371
2023-05-09 03:24:01 | INFO | train_inner | epoch 003:    728 / 986 loss=7.281, nll_loss=6.268, intra_distillation_loss=0, ppl=77.08, wps=33758.7, ups=9.24, wpb=3654.6, bsz=140.6, num_updates=2700, lr=0.0003375, gnorm=1.267, train_wall=10, gb_free=77, wall=382
2023-05-09 03:24:11 | INFO | train_inner | epoch 003:    828 / 986 loss=7.068, nll_loss=6.026, intra_distillation_loss=0, ppl=65.14, wps=33974.7, ups=9.19, wpb=3696.2, bsz=155.8, num_updates=2800, lr=0.00035, gnorm=1.229, train_wall=10, gb_free=77.1, wall=392
2023-05-09 03:24:22 | INFO | train_inner | epoch 003:    928 / 986 loss=7.064, nll_loss=6.02, intra_distillation_loss=0, ppl=64.9, wps=33933, ups=9.2, wpb=3687.6, bsz=144.2, num_updates=2900, lr=0.0003625, gnorm=1.222, train_wall=10, gb_free=76.9, wall=403
2023-05-09 03:24:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:24:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:25:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.733 | nll_loss 5.566 | intra_distillation_loss 0 | ppl 47.39 | bleu 6.76 | wps 4084.6 | wpb 2821.2 | bsz 113.1 | num_updates 2958 | best_bleu 6.76
2023-05-09 03:25:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2958 updates
2023-05-09 03:25:09 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:25:10 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:25:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 3 @ 2958 updates, score 6.76) (writing took 1.8082955969730392 seconds)
2023-05-09 03:25:11 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-09 03:25:11 | INFO | train | epoch 003 | loss 7.313 | nll_loss 6.304 | intra_distillation_loss 0 | ppl 79.03 | wps 24261.8 | ups 6.61 | wpb 3668.2 | bsz 146.4 | num_updates 2958 | lr 0.00036975 | gnorm 1.251 | train_wall 96 | gb_free 76.9 | wall 452
2023-05-09 03:25:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:25:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:25:11 | INFO | fairseq.trainer | begin training epoch 4
2023-05-09 03:25:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:25:15 | INFO | train_inner | epoch 004:     42 / 986 loss=6.991, nll_loss=5.936, intra_distillation_loss=0, ppl=61.23, wps=6863.4, ups=1.89, wpb=3637.9, bsz=141.6, num_updates=3000, lr=0.000375, gnorm=1.339, train_wall=10, gb_free=76.9, wall=456
2023-05-09 03:25:26 | INFO | train_inner | epoch 004:    142 / 986 loss=6.944, nll_loss=5.882, intra_distillation_loss=0, ppl=58.98, wps=33813, ups=9.4, wpb=3599, bsz=132.6, num_updates=3100, lr=0.0003875, gnorm=1.307, train_wall=10, gb_free=76.9, wall=467
2023-05-09 03:25:37 | INFO | train_inner | epoch 004:    242 / 986 loss=6.808, nll_loss=5.727, intra_distillation_loss=0, ppl=52.95, wps=34212.1, ups=9.23, wpb=3705.7, bsz=133.8, num_updates=3200, lr=0.0004, gnorm=1.213, train_wall=10, gb_free=77.1, wall=478
2023-05-09 03:25:48 | INFO | train_inner | epoch 004:    342 / 986 loss=6.784, nll_loss=5.698, intra_distillation_loss=0, ppl=51.92, wps=34226.4, ups=9.26, wpb=3695.7, bsz=140.4, num_updates=3300, lr=0.0004125, gnorm=1.309, train_wall=10, gb_free=76.8, wall=489
2023-05-09 03:25:58 | INFO | train_inner | epoch 004:    442 / 986 loss=6.797, nll_loss=5.714, intra_distillation_loss=0, ppl=52.48, wps=33114.4, ups=9.24, wpb=3583.1, bsz=131, num_updates=3400, lr=0.000425, gnorm=1.258, train_wall=10, gb_free=76.8, wall=499
2023-05-09 03:26:09 | INFO | train_inner | epoch 004:    542 / 986 loss=6.464, nll_loss=5.333, intra_distillation_loss=0, ppl=40.3, wps=33751.7, ups=9.16, wpb=3683.9, bsz=167.6, num_updates=3500, lr=0.0004375, gnorm=1.283, train_wall=10, gb_free=77.1, wall=510
2023-05-09 03:26:20 | INFO | train_inner | epoch 004:    642 / 986 loss=6.578, nll_loss=5.461, intra_distillation_loss=0, ppl=44.06, wps=32933.5, ups=9.03, wpb=3648, bsz=143.9, num_updates=3600, lr=0.00045, gnorm=1.308, train_wall=10, gb_free=76.8, wall=521
2023-05-09 03:26:31 | INFO | train_inner | epoch 004:    742 / 986 loss=6.374, nll_loss=5.23, intra_distillation_loss=0, ppl=37.54, wps=33769.2, ups=9.14, wpb=3694.5, bsz=166.3, num_updates=3700, lr=0.0004625, gnorm=1.357, train_wall=10, gb_free=76.9, wall=532
2023-05-09 03:26:42 | INFO | train_inner | epoch 004:    842 / 986 loss=6.37, nll_loss=5.224, intra_distillation_loss=0, ppl=37.37, wps=34279.8, ups=9.23, wpb=3714, bsz=148.5, num_updates=3800, lr=0.000475, gnorm=1.184, train_wall=10, gb_free=76.9, wall=543
2023-05-09 03:26:53 | INFO | train_inner | epoch 004:    942 / 986 loss=6.375, nll_loss=5.229, intra_distillation_loss=0, ppl=37.5, wps=33945.4, ups=9.25, wpb=3671.5, bsz=145.2, num_updates=3900, lr=0.0004875, gnorm=1.235, train_wall=10, gb_free=77, wall=554
2023-05-09 03:26:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:26:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:27:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.048 | nll_loss 4.758 | intra_distillation_loss 0 | ppl 27.06 | bleu 12.67 | wps 4278.6 | wpb 2821.2 | bsz 113.1 | num_updates 3944 | best_bleu 12.67
2023-05-09 03:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3944 updates
2023-05-09 03:27:36 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:27:37 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:27:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 4 @ 3944 updates, score 12.67) (writing took 1.7798770950175822 seconds)
2023-05-09 03:27:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-09 03:27:38 | INFO | train | epoch 004 | loss 6.605 | nll_loss 5.493 | intra_distillation_loss 0 | ppl 45.05 | wps 24564.3 | ups 6.7 | wpb 3668.2 | bsz 146.4 | num_updates 3944 | lr 0.000493 | gnorm 1.267 | train_wall 96 | gb_free 76.9 | wall 599
2023-05-09 03:27:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:27:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:27:38 | INFO | fairseq.trainer | begin training epoch 5
2023-05-09 03:27:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:27:44 | INFO | train_inner | epoch 005:     56 / 986 loss=6.127, nll_loss=4.948, intra_distillation_loss=0, ppl=30.88, wps=7207.5, ups=1.95, wpb=3688.7, bsz=173.4, num_updates=4000, lr=0.0005, gnorm=1.25, train_wall=10, gb_free=76.8, wall=605
2023-05-09 03:27:55 | INFO | train_inner | epoch 005:    156 / 986 loss=6.156, nll_loss=4.979, intra_distillation_loss=0, ppl=31.53, wps=33802, ups=9.17, wpb=3684.5, bsz=154.9, num_updates=4100, lr=0.000493865, gnorm=1.238, train_wall=10, gb_free=76.8, wall=616
2023-05-09 03:28:06 | INFO | train_inner | epoch 005:    256 / 986 loss=6.16, nll_loss=4.983, intra_distillation_loss=0, ppl=31.62, wps=33672, ups=9.18, wpb=3669.6, bsz=139.7, num_updates=4200, lr=0.00048795, gnorm=1.196, train_wall=10, gb_free=76.7, wall=627
2023-05-09 03:28:17 | INFO | train_inner | epoch 005:    356 / 986 loss=6.05, nll_loss=4.856, intra_distillation_loss=0, ppl=28.96, wps=33868.5, ups=9.25, wpb=3660.4, bsz=139.6, num_updates=4300, lr=0.000482243, gnorm=1.185, train_wall=10, gb_free=76.9, wall=638
2023-05-09 03:28:28 | INFO | train_inner | epoch 005:    456 / 986 loss=6.04, nll_loss=4.844, intra_distillation_loss=0, ppl=28.73, wps=33728.5, ups=9.18, wpb=3675.7, bsz=140, num_updates=4400, lr=0.000476731, gnorm=1.193, train_wall=10, gb_free=76.9, wall=649
2023-05-09 03:28:38 | INFO | train_inner | epoch 005:    556 / 986 loss=5.899, nll_loss=4.682, intra_distillation_loss=0, ppl=25.67, wps=33788.5, ups=9.45, wpb=3574.7, bsz=142.5, num_updates=4500, lr=0.000471405, gnorm=1.201, train_wall=9, gb_free=76.8, wall=659
2023-05-09 03:28:49 | INFO | train_inner | epoch 005:    656 / 986 loss=5.828, nll_loss=4.602, intra_distillation_loss=0, ppl=24.28, wps=33963.1, ups=9.11, wpb=3726.2, bsz=146, num_updates=4600, lr=0.000466252, gnorm=1.16, train_wall=10, gb_free=76.9, wall=670
2023-05-09 03:29:00 | INFO | train_inner | epoch 005:    756 / 986 loss=5.77, nll_loss=4.536, intra_distillation_loss=0, ppl=23.2, wps=33734.9, ups=9.13, wpb=3694.5, bsz=161, num_updates=4700, lr=0.000461266, gnorm=1.189, train_wall=10, gb_free=76.8, wall=681
2023-05-09 03:29:11 | INFO | train_inner | epoch 005:    856 / 986 loss=5.813, nll_loss=4.583, intra_distillation_loss=0, ppl=23.96, wps=34016, ups=9.41, wpb=3613.3, bsz=128.2, num_updates=4800, lr=0.000456435, gnorm=1.159, train_wall=10, gb_free=76.9, wall=692
2023-05-09 03:29:22 | INFO | train_inner | epoch 005:    956 / 986 loss=5.618, nll_loss=4.361, intra_distillation_loss=0, ppl=20.55, wps=33909.5, ups=9.21, wpb=3682.8, bsz=152.7, num_updates=4900, lr=0.000451754, gnorm=1.158, train_wall=10, gb_free=76.8, wall=703
2023-05-09 03:29:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:29:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:30:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.323 | nll_loss 3.906 | intra_distillation_loss 0 | ppl 14.99 | bleu 18.99 | wps 4048.2 | wpb 2821.2 | bsz 113.1 | num_updates 4930 | best_bleu 18.99
2023-05-09 03:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4930 updates
2023-05-09 03:30:05 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:30:06 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 5 @ 4930 updates, score 18.99) (writing took 1.7586910160025582 seconds)
2023-05-09 03:30:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-09 03:30:07 | INFO | train | epoch 005 | loss 5.928 | nll_loss 4.716 | intra_distillation_loss 0 | ppl 26.29 | wps 24225.1 | ups 6.6 | wpb 3668.2 | bsz 146.4 | num_updates 4930 | lr 0.000450377 | gnorm 1.188 | train_wall 96 | gb_free 76.6 | wall 748
2023-05-09 03:30:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:30:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:30:07 | INFO | fairseq.trainer | begin training epoch 6
2023-05-09 03:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:30:15 | INFO | train_inner | epoch 006:     70 / 986 loss=5.616, nll_loss=4.359, intra_distillation_loss=0, ppl=20.53, wps=6845.8, ups=1.88, wpb=3642.8, bsz=147.9, num_updates=5000, lr=0.000447214, gnorm=1.158, train_wall=10, gb_free=76.8, wall=756
2023-05-09 03:30:26 | INFO | train_inner | epoch 006:    170 / 986 loss=5.571, nll_loss=4.306, intra_distillation_loss=0, ppl=19.78, wps=34477, ups=9.35, wpb=3688.8, bsz=134.4, num_updates=5100, lr=0.000442807, gnorm=1.143, train_wall=10, gb_free=76.8, wall=767
2023-05-09 03:30:37 | INFO | train_inner | epoch 006:    270 / 986 loss=5.479, nll_loss=4.202, intra_distillation_loss=0, ppl=18.4, wps=33687.6, ups=9.18, wpb=3671.7, bsz=144.6, num_updates=5200, lr=0.000438529, gnorm=1.151, train_wall=10, gb_free=76.8, wall=777
2023-05-09 03:30:47 | INFO | train_inner | epoch 006:    370 / 986 loss=5.486, nll_loss=4.207, intra_distillation_loss=0, ppl=18.47, wps=34138.7, ups=9.22, wpb=3703.1, bsz=135.6, num_updates=5300, lr=0.000434372, gnorm=1.178, train_wall=10, gb_free=76.7, wall=788
2023-05-09 03:30:58 | INFO | train_inner | epoch 006:    470 / 986 loss=5.347, nll_loss=4.05, intra_distillation_loss=0, ppl=16.56, wps=34184.7, ups=9.2, wpb=3717.1, bsz=155.3, num_updates=5400, lr=0.000430331, gnorm=1.123, train_wall=10, gb_free=76.9, wall=799
2023-05-09 03:31:09 | INFO | train_inner | epoch 006:    570 / 986 loss=5.266, nll_loss=3.957, intra_distillation_loss=0, ppl=15.53, wps=34017.5, ups=9.14, wpb=3720.3, bsz=162.6, num_updates=5500, lr=0.000426401, gnorm=1.139, train_wall=10, gb_free=76.8, wall=810
2023-05-09 03:31:20 | INFO | train_inner | epoch 006:    670 / 986 loss=5.25, nll_loss=3.939, intra_distillation_loss=0, ppl=15.33, wps=33478.1, ups=9.12, wpb=3669, bsz=162.4, num_updates=5600, lr=0.000422577, gnorm=1.121, train_wall=10, gb_free=76.8, wall=821
2023-05-09 03:31:31 | INFO | train_inner | epoch 006:    770 / 986 loss=5.448, nll_loss=4.162, intra_distillation_loss=0, ppl=17.9, wps=33774.7, ups=9.36, wpb=3609, bsz=130.4, num_updates=5700, lr=0.000418854, gnorm=1.21, train_wall=10, gb_free=76.8, wall=832
2023-05-09 03:31:42 | INFO | train_inner | epoch 006:    870 / 986 loss=5.306, nll_loss=4.002, intra_distillation_loss=0, ppl=16.02, wps=33730.7, ups=9.28, wpb=3633.4, bsz=147.4, num_updates=5800, lr=0.000415227, gnorm=1.124, train_wall=10, gb_free=76.7, wall=843
2023-05-09 03:31:52 | INFO | train_inner | epoch 006:    970 / 986 loss=5.296, nll_loss=3.991, intra_distillation_loss=0, ppl=15.9, wps=33252.8, ups=9.18, wpb=3621.5, bsz=143.9, num_updates=5900, lr=0.000411693, gnorm=1.15, train_wall=10, gb_free=76.9, wall=853
2023-05-09 03:31:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:31:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:32:35 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.041 | nll_loss 3.561 | intra_distillation_loss 0 | ppl 11.8 | bleu 23.06 | wps 4018.1 | wpb 2821.2 | bsz 113.1 | num_updates 5916 | best_bleu 23.06
2023-05-09 03:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5916 updates
2023-05-09 03:32:35 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:32:36 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 6 @ 5916 updates, score 23.06) (writing took 1.8092707250034437 seconds)
2023-05-09 03:32:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-09 03:32:37 | INFO | train | epoch 006 | loss 5.398 | nll_loss 4.108 | intra_distillation_loss 0 | ppl 17.24 | wps 24178.8 | ups 6.59 | wpb 3668.2 | bsz 146.4 | num_updates 5916 | lr 0.000411136 | gnorm 1.151 | train_wall 96 | gb_free 76.9 | wall 898
2023-05-09 03:32:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:32:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:32:37 | INFO | fairseq.trainer | begin training epoch 7
2023-05-09 03:32:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:32:46 | INFO | train_inner | epoch 007:     84 / 986 loss=5.133, nll_loss=3.805, intra_distillation_loss=0, ppl=13.98, wps=6824.4, ups=1.87, wpb=3651.3, bsz=143.1, num_updates=6000, lr=0.000408248, gnorm=1.124, train_wall=10, gb_free=76.8, wall=907
2023-05-09 03:32:59 | INFO | train_inner | epoch 007:    184 / 986 loss=5.097, nll_loss=3.77, intra_distillation_loss=0, ppl=13.64, wps=28840.9, ups=7.85, wpb=3676.1, bsz=144.6, num_updates=6100, lr=0.000404888, gnorm=0, train_wall=12, gb_free=76.9, wall=920
2023-05-09 03:33:11 | INFO | train_inner | epoch 007:    284 / 986 loss=5.051, nll_loss=3.717, intra_distillation_loss=0, ppl=13.15, wps=28920.6, ups=7.84, wpb=3687.6, bsz=143.8, num_updates=6200, lr=0.00040161, gnorm=0, train_wall=12, gb_free=76.9, wall=932
2023-05-09 03:33:24 | INFO | train_inner | epoch 007:    384 / 986 loss=5.017, nll_loss=3.677, intra_distillation_loss=0, ppl=12.79, wps=28749.1, ups=7.78, wpb=3693.1, bsz=149, num_updates=6300, lr=0.00039841, gnorm=0, train_wall=12, gb_free=76.9, wall=945
2023-05-09 03:33:37 | INFO | train_inner | epoch 007:    484 / 986 loss=5.044, nll_loss=3.707, intra_distillation_loss=0, ppl=13.05, wps=28453.6, ups=7.85, wpb=3624.9, bsz=141.8, num_updates=6400, lr=0.000395285, gnorm=0, train_wall=12, gb_free=76.8, wall=958
2023-05-09 03:33:50 | INFO | train_inner | epoch 007:    584 / 986 loss=5.021, nll_loss=3.68, intra_distillation_loss=0, ppl=12.81, wps=28278, ups=7.86, wpb=3595.9, bsz=146.4, num_updates=6500, lr=0.000392232, gnorm=0, train_wall=12, gb_free=77.1, wall=971
2023-05-09 03:34:02 | INFO | train_inner | epoch 007:    684 / 986 loss=5.029, nll_loss=3.687, intra_distillation_loss=0, ppl=12.88, wps=28545, ups=7.94, wpb=3593.3, bsz=132, num_updates=6600, lr=0.000389249, gnorm=0, train_wall=12, gb_free=77, wall=983
2023-05-09 03:34:15 | INFO | train_inner | epoch 007:    784 / 986 loss=4.884, nll_loss=3.522, intra_distillation_loss=0, ppl=11.49, wps=28692.4, ups=7.7, wpb=3725.6, bsz=162, num_updates=6700, lr=0.000386334, gnorm=0, train_wall=12, gb_free=76.9, wall=996
2023-05-09 03:34:28 | INFO | train_inner | epoch 007:    884 / 986 loss=4.952, nll_loss=3.598, intra_distillation_loss=0, ppl=12.11, wps=28749.3, ups=7.65, wpb=3756.3, bsz=143.9, num_updates=6800, lr=0.000383482, gnorm=0, train_wall=12, gb_free=76.7, wall=1009
2023-05-09 03:34:41 | INFO | train_inner | epoch 007:    984 / 986 loss=4.877, nll_loss=3.513, intra_distillation_loss=0, ppl=11.42, wps=29010.9, ups=7.85, wpb=3693.5, bsz=156.9, num_updates=6900, lr=0.000380693, gnorm=0, train_wall=12, gb_free=76.9, wall=1022
2023-05-09 03:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:35:19 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.77 | nll_loss 3.261 | intra_distillation_loss 0 | ppl 9.59 | bleu 25.41 | wps 4333 | wpb 2821.2 | bsz 113.1 | num_updates 6902 | best_bleu 25.41
2023-05-09 03:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6902 updates
2023-05-09 03:35:19 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:35:20 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:35:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 7 @ 6902 updates, score 25.41) (writing took 1.821957492036745 seconds)
2023-05-09 03:35:21 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-09 03:35:21 | INFO | train | epoch 007 | loss 5.007 | nll_loss 3.663 | intra_distillation_loss 0 | ppl 12.67 | wps 22001.3 | ups 6 | wpb 3668.2 | bsz 146.4 | num_updates 6902 | lr 0.000380638 | gnorm 0.096 | train_wall 114 | gb_free 76.8 | wall 1062
2023-05-09 03:35:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:35:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:35:21 | INFO | fairseq.trainer | begin training epoch 8
2023-05-09 03:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:35:34 | INFO | train_inner | epoch 008:     98 / 986 loss=4.867, nll_loss=3.501, intra_distillation_loss=0, ppl=11.32, wps=6985.7, ups=1.9, wpb=3674.5, bsz=132.6, num_updates=7000, lr=0.000377964, gnorm=0, train_wall=12, gb_free=76.9, wall=1075
2023-05-09 03:35:47 | INFO | train_inner | epoch 008:    198 / 986 loss=4.715, nll_loss=3.327, intra_distillation_loss=0, ppl=10.04, wps=28691.4, ups=7.82, wpb=3670.3, bsz=162.3, num_updates=7100, lr=0.000375293, gnorm=0, train_wall=12, gb_free=77.2, wall=1087
2023-05-09 03:36:00 | INFO | train_inner | epoch 008:    298 / 986 loss=4.758, nll_loss=3.376, intra_distillation_loss=0, ppl=10.38, wps=27973.9, ups=7.7, wpb=3632.3, bsz=163, num_updates=7200, lr=0.000372678, gnorm=0, train_wall=12, gb_free=76.7, wall=1100
2023-05-09 03:36:12 | INFO | train_inner | epoch 008:    398 / 986 loss=4.859, nll_loss=3.488, intra_distillation_loss=0, ppl=11.22, wps=28715.8, ups=7.89, wpb=3639.3, bsz=135.5, num_updates=7300, lr=0.000370117, gnorm=0, train_wall=12, gb_free=77, wall=1113
2023-05-09 03:36:25 | INFO | train_inner | epoch 008:    498 / 986 loss=4.842, nll_loss=3.47, intra_distillation_loss=0, ppl=11.08, wps=28950.9, ups=7.82, wpb=3704.4, bsz=148.3, num_updates=7400, lr=0.000367607, gnorm=0, train_wall=12, gb_free=76.8, wall=1126
2023-05-09 03:36:38 | INFO | train_inner | epoch 008:    598 / 986 loss=4.735, nll_loss=3.347, intra_distillation_loss=0, ppl=10.18, wps=29062.2, ups=7.68, wpb=3782.9, bsz=160.6, num_updates=7500, lr=0.000365148, gnorm=0, train_wall=12, gb_free=76.8, wall=1139
2023-05-09 03:36:51 | INFO | train_inner | epoch 008:    698 / 986 loss=4.891, nll_loss=3.524, intra_distillation_loss=0, ppl=11.5, wps=28890.8, ups=7.82, wpb=3696, bsz=135.1, num_updates=7600, lr=0.000362738, gnorm=0, train_wall=12, gb_free=77, wall=1152
2023-05-09 03:37:04 | INFO | train_inner | epoch 008:    798 / 986 loss=4.817, nll_loss=3.441, intra_distillation_loss=0, ppl=10.86, wps=28644.4, ups=7.83, wpb=3658, bsz=144, num_updates=7700, lr=0.000360375, gnorm=0, train_wall=12, gb_free=76.7, wall=1165
2023-05-09 03:37:16 | INFO | train_inner | epoch 008:    898 / 986 loss=4.854, nll_loss=3.482, intra_distillation_loss=0, ppl=11.17, wps=28414.6, ups=7.84, wpb=3623.3, bsz=138, num_updates=7800, lr=0.000358057, gnorm=0, train_wall=12, gb_free=76.8, wall=1177
2023-05-09 03:37:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:37:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:38:05 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.661 | nll_loss 3.132 | intra_distillation_loss 0 | ppl 8.77 | bleu 26.64 | wps 4338.4 | wpb 2821.2 | bsz 113.1 | num_updates 7888 | best_bleu 26.64
2023-05-09 03:38:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7888 updates
2023-05-09 03:38:05 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:38:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 8 @ 7888 updates, score 26.64) (writing took 1.8011202689958736 seconds)
2023-05-09 03:38:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-09 03:38:07 | INFO | train | epoch 008 | loss 4.813 | nll_loss 3.437 | intra_distillation_loss 0 | ppl 10.83 | wps 21806 | ups 5.94 | wpb 3668.2 | bsz 146.4 | num_updates 7888 | lr 0.000356055 | gnorm 0 | train_wall 115 | gb_free 76.9 | wall 1228
2023-05-09 03:38:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:38:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:38:07 | INFO | fairseq.trainer | begin training epoch 9
2023-05-09 03:38:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:38:09 | INFO | train_inner | epoch 009:     12 / 986 loss=4.781, nll_loss=3.4, intra_distillation_loss=0, ppl=10.55, wps=6915.4, ups=1.91, wpb=3624.2, bsz=141.9, num_updates=7900, lr=0.000355784, gnorm=0, train_wall=12, gb_free=76.9, wall=1230
2023-05-09 03:38:22 | INFO | train_inner | epoch 009:    112 / 986 loss=4.679, nll_loss=3.282, intra_distillation_loss=0, ppl=9.73, wps=28325.4, ups=7.81, wpb=3628.5, bsz=146.2, num_updates=8000, lr=0.000353553, gnorm=0, train_wall=12, gb_free=76.8, wall=1242
2023-05-09 03:38:34 | INFO | train_inner | epoch 009:    212 / 986 loss=4.701, nll_loss=3.307, intra_distillation_loss=0, ppl=9.9, wps=28526, ups=7.82, wpb=3647.5, bsz=142.5, num_updates=8100, lr=0.000351364, gnorm=0, train_wall=12, gb_free=76.8, wall=1255
2023-05-09 03:38:47 | INFO | train_inner | epoch 009:    312 / 986 loss=4.647, nll_loss=3.245, intra_distillation_loss=0, ppl=9.48, wps=29020.5, ups=7.76, wpb=3741.8, bsz=152.2, num_updates=8200, lr=0.000349215, gnorm=0, train_wall=12, gb_free=76.7, wall=1268
2023-05-09 03:39:00 | INFO | train_inner | epoch 009:    412 / 986 loss=4.672, nll_loss=3.274, intra_distillation_loss=0, ppl=9.67, wps=28804.7, ups=7.81, wpb=3689, bsz=158.1, num_updates=8300, lr=0.000347105, gnorm=0, train_wall=12, gb_free=76.8, wall=1281
2023-05-09 03:39:13 | INFO | train_inner | epoch 009:    512 / 986 loss=4.802, nll_loss=3.422, intra_distillation_loss=0, ppl=10.72, wps=28608.1, ups=7.89, wpb=3626.4, bsz=131.4, num_updates=8400, lr=0.000345033, gnorm=0, train_wall=12, gb_free=76.8, wall=1294
2023-05-09 03:39:26 | INFO | train_inner | epoch 009:    612 / 986 loss=4.67, nll_loss=3.272, intra_distillation_loss=0, ppl=9.66, wps=28575.8, ups=7.78, wpb=3675.1, bsz=157.5, num_updates=8500, lr=0.000342997, gnorm=0, train_wall=12, gb_free=76.9, wall=1307
2023-05-09 03:39:38 | INFO | train_inner | epoch 009:    712 / 986 loss=4.712, nll_loss=3.319, intra_distillation_loss=0, ppl=9.98, wps=28631.8, ups=7.89, wpb=3630.2, bsz=140.5, num_updates=8600, lr=0.000340997, gnorm=0, train_wall=12, gb_free=76.9, wall=1319
2023-05-09 03:39:51 | INFO | train_inner | epoch 009:    812 / 986 loss=4.74, nll_loss=3.351, intra_distillation_loss=0, ppl=10.2, wps=28762.5, ups=7.89, wpb=3647.5, bsz=138.9, num_updates=8700, lr=0.000339032, gnorm=0, train_wall=12, gb_free=77, wall=1332
2023-05-09 03:40:04 | INFO | train_inner | epoch 009:    912 / 986 loss=4.714, nll_loss=3.321, intra_distillation_loss=0, ppl=10, wps=28824.2, ups=7.76, wpb=3714.8, bsz=141.5, num_updates=8800, lr=0.0003371, gnorm=0, train_wall=12, gb_free=76.7, wall=1345
2023-05-09 03:40:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:40:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:40:53 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.569 | nll_loss 3.01 | intra_distillation_loss 0 | ppl 8.06 | bleu 27.85 | wps 4141.7 | wpb 2821.2 | bsz 113.1 | num_updates 8874 | best_bleu 27.85
2023-05-09 03:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 8874 updates
2023-05-09 03:40:53 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:40:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 9 @ 8874 updates, score 27.85) (writing took 1.8416079379385337 seconds)
2023-05-09 03:40:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-09 03:40:55 | INFO | train | epoch 009 | loss 4.698 | nll_loss 3.304 | intra_distillation_loss 0 | ppl 9.87 | wps 21572 | ups 5.88 | wpb 3668.2 | bsz 146.4 | num_updates 8874 | lr 0.000335691 | gnorm 0 | train_wall 115 | gb_free 76.8 | wall 1396
2023-05-09 03:40:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:40:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:40:55 | INFO | fairseq.trainer | begin training epoch 10
2023-05-09 03:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:40:58 | INFO | train_inner | epoch 010:     26 / 986 loss=4.594, nll_loss=3.186, intra_distillation_loss=0, ppl=9.1, wps=6789.7, ups=1.84, wpb=3690.9, bsz=160.4, num_updates=8900, lr=0.000335201, gnorm=0, train_wall=12, gb_free=76.8, wall=1399
2023-05-09 03:41:11 | INFO | train_inner | epoch 010:    126 / 986 loss=4.566, nll_loss=3.153, intra_distillation_loss=0, ppl=8.9, wps=28901.2, ups=7.85, wpb=3679.9, bsz=134.2, num_updates=9000, lr=0.000333333, gnorm=0, train_wall=12, gb_free=76.7, wall=1412
2023-05-09 03:41:24 | INFO | train_inner | epoch 010:    226 / 986 loss=4.604, nll_loss=3.196, intra_distillation_loss=0, ppl=9.16, wps=28550.5, ups=7.77, wpb=3673.5, bsz=148.6, num_updates=9100, lr=0.000331497, gnorm=0, train_wall=12, gb_free=76.8, wall=1425
2023-05-09 03:41:37 | INFO | train_inner | epoch 010:    326 / 986 loss=4.671, nll_loss=3.271, intra_distillation_loss=0, ppl=9.65, wps=28297.4, ups=7.85, wpb=3607, bsz=135.5, num_updates=9200, lr=0.00032969, gnorm=0, train_wall=12, gb_free=76.9, wall=1437
2023-05-09 03:41:49 | INFO | train_inner | epoch 010:    426 / 986 loss=4.619, nll_loss=3.212, intra_distillation_loss=0, ppl=9.27, wps=28443.8, ups=7.86, wpb=3620.6, bsz=140, num_updates=9300, lr=0.000327913, gnorm=0, train_wall=12, gb_free=76.9, wall=1450
2023-05-09 03:42:02 | INFO | train_inner | epoch 010:    526 / 986 loss=4.63, nll_loss=3.225, intra_distillation_loss=0, ppl=9.35, wps=28743.7, ups=7.78, wpb=3693.6, bsz=141.8, num_updates=9400, lr=0.000326164, gnorm=0, train_wall=12, gb_free=77, wall=1463
2023-05-09 03:42:15 | INFO | train_inner | epoch 010:    626 / 986 loss=4.617, nll_loss=3.211, intra_distillation_loss=0, ppl=9.26, wps=28464.2, ups=7.83, wpb=3636.4, bsz=153.8, num_updates=9500, lr=0.000324443, gnorm=0, train_wall=12, gb_free=77, wall=1476
2023-05-09 03:42:28 | INFO | train_inner | epoch 010:    726 / 986 loss=4.628, nll_loss=3.223, intra_distillation_loss=0, ppl=9.34, wps=28535.6, ups=7.81, wpb=3655.8, bsz=149.5, num_updates=9600, lr=0.000322749, gnorm=0, train_wall=12, gb_free=76.8, wall=1489
2023-05-09 03:42:41 | INFO | train_inner | epoch 010:    826 / 986 loss=4.595, nll_loss=3.185, intra_distillation_loss=0, ppl=9.1, wps=28873, ups=7.82, wpb=3693.5, bsz=145.9, num_updates=9700, lr=0.000321081, gnorm=0, train_wall=12, gb_free=76.9, wall=1501
2023-05-09 03:42:53 | INFO | train_inner | epoch 010:    926 / 986 loss=4.57, nll_loss=3.158, intra_distillation_loss=0, ppl=8.93, wps=28913.5, ups=7.79, wpb=3713.7, bsz=159.8, num_updates=9800, lr=0.000319438, gnorm=0, train_wall=12, gb_free=76.9, wall=1514
2023-05-09 03:43:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:43:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:43:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.516 | nll_loss 2.948 | intra_distillation_loss 0 | ppl 7.72 | bleu 28.64 | wps 4265 | wpb 2821.2 | bsz 113.1 | num_updates 9860 | best_bleu 28.64
2023-05-09 03:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 9860 updates
2023-05-09 03:43:40 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:43:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 10 @ 9860 updates, score 28.64) (writing took 1.8098368589999154 seconds)
2023-05-09 03:43:41 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-09 03:43:41 | INFO | train | epoch 010 | loss 4.606 | nll_loss 3.198 | intra_distillation_loss 0 | ppl 9.18 | wps 21708 | ups 5.92 | wpb 3668.2 | bsz 146.4 | num_updates 9860 | lr 0.000318465 | gnorm 0 | train_wall 115 | gb_free 76.7 | wall 1562
2023-05-09 03:43:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:43:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:43:41 | INFO | fairseq.trainer | begin training epoch 11
2023-05-09 03:43:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:43:47 | INFO | train_inner | epoch 011:     40 / 986 loss=4.559, nll_loss=3.144, intra_distillation_loss=0, ppl=8.84, wps=6889, ups=1.88, wpb=3667.4, bsz=149.7, num_updates=9900, lr=0.000317821, gnorm=0, train_wall=12, gb_free=77.7, wall=1567
2023-05-09 03:43:59 | INFO | train_inner | epoch 011:    140 / 986 loss=4.533, nll_loss=3.114, intra_distillation_loss=0, ppl=8.66, wps=29075.7, ups=7.75, wpb=3750.4, bsz=140.6, num_updates=10000, lr=0.000316228, gnorm=0, train_wall=12, gb_free=76.8, wall=1580
2023-05-09 03:44:12 | INFO | train_inner | epoch 011:    240 / 986 loss=4.523, nll_loss=3.103, intra_distillation_loss=0, ppl=8.59, wps=28330.2, ups=7.75, wpb=3653.5, bsz=156.3, num_updates=10100, lr=0.000314658, gnorm=0, train_wall=12, gb_free=76.9, wall=1593
2023-05-09 03:44:25 | INFO | train_inner | epoch 011:    340 / 986 loss=4.517, nll_loss=3.095, intra_distillation_loss=0, ppl=8.54, wps=28980.2, ups=7.86, wpb=3688.5, bsz=139.6, num_updates=10200, lr=0.000313112, gnorm=0, train_wall=12, gb_free=76.9, wall=1606
2023-05-09 03:44:38 | INFO | train_inner | epoch 011:    440 / 986 loss=4.552, nll_loss=3.136, intra_distillation_loss=0, ppl=8.79, wps=28406.9, ups=7.91, wpb=3590.3, bsz=147.9, num_updates=10300, lr=0.000311588, gnorm=0, train_wall=12, gb_free=76.9, wall=1619
2023-05-09 03:44:51 | INFO | train_inner | epoch 011:    540 / 986 loss=4.466, nll_loss=3.039, intra_distillation_loss=0, ppl=8.22, wps=28536.4, ups=7.78, wpb=3667.9, bsz=167, num_updates=10400, lr=0.000310087, gnorm=0, train_wall=12, gb_free=76.9, wall=1632
2023-05-09 03:45:03 | INFO | train_inner | epoch 011:    640 / 986 loss=4.49, nll_loss=3.064, intra_distillation_loss=0, ppl=8.36, wps=29006.4, ups=7.8, wpb=3718.8, bsz=152.1, num_updates=10500, lr=0.000308607, gnorm=0, train_wall=12, gb_free=76.8, wall=1644
2023-05-09 03:45:16 | INFO | train_inner | epoch 011:    740 / 986 loss=4.578, nll_loss=3.165, intra_distillation_loss=0, ppl=8.97, wps=28831, ups=7.8, wpb=3696.7, bsz=135.7, num_updates=10600, lr=0.000307148, gnorm=0, train_wall=12, gb_free=76.9, wall=1657
2023-05-09 03:45:29 | INFO | train_inner | epoch 011:    840 / 986 loss=4.563, nll_loss=3.147, intra_distillation_loss=0, ppl=8.86, wps=29041.2, ups=7.8, wpb=3722.1, bsz=134.9, num_updates=10700, lr=0.000305709, gnorm=0, train_wall=12, gb_free=76.9, wall=1670
2023-05-09 03:45:42 | INFO | train_inner | epoch 011:    940 / 986 loss=4.579, nll_loss=3.167, intra_distillation_loss=0, ppl=8.98, wps=28000.8, ups=7.83, wpb=3574.4, bsz=143.9, num_updates=10800, lr=0.00030429, gnorm=0, train_wall=12, gb_free=76.9, wall=1683
2023-05-09 03:45:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:45:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:46:26 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.471 | nll_loss 2.898 | intra_distillation_loss 0 | ppl 7.46 | bleu 29.13 | wps 4289.5 | wpb 2821.2 | bsz 113.1 | num_updates 10846 | best_bleu 29.13
2023-05-09 03:46:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 10846 updates
2023-05-09 03:46:26 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:46:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 11 @ 10846 updates, score 29.13) (writing took 1.845098303980194 seconds)
2023-05-09 03:46:28 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-09 03:46:28 | INFO | train | epoch 011 | loss 4.533 | nll_loss 3.114 | intra_distillation_loss 0 | ppl 8.66 | wps 21729 | ups 5.92 | wpb 3668.2 | bsz 146.4 | num_updates 10846 | lr 0.000303644 | gnorm 0 | train_wall 116 | gb_free 77 | wall 1729
2023-05-09 03:46:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:46:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:46:28 | INFO | fairseq.trainer | begin training epoch 12
2023-05-09 03:46:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:46:35 | INFO | train_inner | epoch 012:     54 / 986 loss=4.525, nll_loss=3.104, intra_distillation_loss=0, ppl=8.6, wps=6903.9, ups=1.89, wpb=3659.2, bsz=138, num_updates=10900, lr=0.000302891, gnorm=0, train_wall=12, gb_free=76.8, wall=1736
2023-05-09 03:46:48 | INFO | train_inner | epoch 012:    154 / 986 loss=4.427, nll_loss=2.992, intra_distillation_loss=0, ppl=7.96, wps=28957.9, ups=7.75, wpb=3735.2, bsz=149.8, num_updates=11000, lr=0.000301511, gnorm=0, train_wall=12, gb_free=76.8, wall=1749
2023-05-09 03:47:01 | INFO | train_inner | epoch 012:    254 / 986 loss=4.41, nll_loss=2.974, intra_distillation_loss=0, ppl=7.86, wps=28745.3, ups=7.83, wpb=3669.2, bsz=155.3, num_updates=11100, lr=0.00030015, gnorm=0, train_wall=12, gb_free=76.9, wall=1761
2023-05-09 03:47:13 | INFO | train_inner | epoch 012:    354 / 986 loss=4.448, nll_loss=3.017, intra_distillation_loss=0, ppl=8.09, wps=28806.3, ups=7.82, wpb=3685.4, bsz=148.5, num_updates=11200, lr=0.000298807, gnorm=0, train_wall=12, gb_free=76.9, wall=1774
2023-05-09 03:47:26 | INFO | train_inner | epoch 012:    454 / 986 loss=4.432, nll_loss=2.999, intra_distillation_loss=0, ppl=8, wps=28397.3, ups=7.78, wpb=3649.2, bsz=156.3, num_updates=11300, lr=0.000297482, gnorm=0, train_wall=12, gb_free=76.7, wall=1787
2023-05-09 03:47:39 | INFO | train_inner | epoch 012:    554 / 986 loss=4.509, nll_loss=3.085, intra_distillation_loss=0, ppl=8.49, wps=28008.3, ups=7.81, wpb=3584.1, bsz=141.5, num_updates=11400, lr=0.000296174, gnorm=0, train_wall=12, gb_free=76.8, wall=1800
2023-05-09 03:47:52 | INFO | train_inner | epoch 012:    654 / 986 loss=4.457, nll_loss=3.027, intra_distillation_loss=0, ppl=8.15, wps=28589.8, ups=7.78, wpb=3675.1, bsz=151.7, num_updates=11500, lr=0.000294884, gnorm=0, train_wall=12, gb_free=77.3, wall=1813
2023-05-09 03:48:05 | INFO | train_inner | epoch 012:    754 / 986 loss=4.433, nll_loss=3, intra_distillation_loss=0, ppl=8, wps=28950.7, ups=7.76, wpb=3731.2, bsz=152.7, num_updates=11600, lr=0.00029361, gnorm=0, train_wall=12, gb_free=76.8, wall=1826
2023-05-09 03:48:17 | INFO | train_inner | epoch 012:    854 / 986 loss=4.55, nll_loss=3.133, intra_distillation_loss=0, ppl=8.77, wps=28384.4, ups=7.96, wpb=3567, bsz=133.5, num_updates=11700, lr=0.000292353, gnorm=0, train_wall=12, gb_free=76.8, wall=1838
2023-05-09 03:48:30 | INFO | train_inner | epoch 012:    954 / 986 loss=4.499, nll_loss=3.075, intra_distillation_loss=0, ppl=8.42, wps=28955.4, ups=7.84, wpb=3694, bsz=135.7, num_updates=11800, lr=0.000291111, gnorm=0, train_wall=12, gb_free=76.9, wall=1851
2023-05-09 03:48:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:48:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:49:13 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.428 | nll_loss 2.846 | intra_distillation_loss 0 | ppl 7.19 | bleu 29.85 | wps 4264 | wpb 2821.2 | bsz 113.1 | num_updates 11832 | best_bleu 29.85
2023-05-09 03:49:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 11832 updates
2023-05-09 03:49:13 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:49:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 12 @ 11832 updates, score 29.85) (writing took 1.7662521060556173 seconds)
2023-05-09 03:49:14 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-09 03:49:14 | INFO | train | epoch 012 | loss 4.468 | nll_loss 3.04 | intra_distillation_loss 0 | ppl 8.22 | wps 21723 | ups 5.92 | wpb 3668.2 | bsz 146.4 | num_updates 11832 | lr 0.000290717 | gnorm 0 | train_wall 115 | gb_free 76.9 | wall 1895
2023-05-09 03:49:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:49:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:49:14 | INFO | fairseq.trainer | begin training epoch 13
2023-05-09 03:49:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:49:23 | INFO | train_inner | epoch 013:     68 / 986 loss=4.463, nll_loss=3.035, intra_distillation_loss=0, ppl=8.19, wps=6776.6, ups=1.89, wpb=3592.9, bsz=138.3, num_updates=11900, lr=0.000289886, gnorm=0, train_wall=12, gb_free=76.9, wall=1904
2023-05-09 03:49:36 | INFO | train_inner | epoch 013:    168 / 986 loss=4.346, nll_loss=2.902, intra_distillation_loss=0, ppl=7.47, wps=28491.4, ups=7.76, wpb=3673.5, bsz=161.8, num_updates=12000, lr=0.000288675, gnorm=0, train_wall=12, gb_free=77, wall=1917
2023-05-09 03:49:49 | INFO | train_inner | epoch 013:    268 / 986 loss=4.414, nll_loss=2.977, intra_distillation_loss=0, ppl=7.87, wps=28743.4, ups=7.89, wpb=3644.1, bsz=145.1, num_updates=12100, lr=0.00028748, gnorm=0, train_wall=12, gb_free=76.8, wall=1930
2023-05-09 03:50:01 | INFO | train_inner | epoch 013:    368 / 986 loss=4.381, nll_loss=2.941, intra_distillation_loss=0, ppl=7.68, wps=28654.4, ups=7.78, wpb=3681.7, bsz=151.2, num_updates=12200, lr=0.000286299, gnorm=0, train_wall=12, gb_free=76.9, wall=1942
2023-05-09 03:50:14 | INFO | train_inner | epoch 013:    468 / 986 loss=4.414, nll_loss=2.978, intra_distillation_loss=0, ppl=7.88, wps=28526, ups=7.82, wpb=3649.7, bsz=151.8, num_updates=12300, lr=0.000285133, gnorm=0, train_wall=12, gb_free=76.8, wall=1955
2023-05-09 03:50:27 | INFO | train_inner | epoch 013:    568 / 986 loss=4.424, nll_loss=2.989, intra_distillation_loss=0, ppl=7.94, wps=28752.7, ups=7.81, wpb=3680.3, bsz=143.6, num_updates=12400, lr=0.000283981, gnorm=0, train_wall=12, gb_free=76.9, wall=1968
2023-05-09 03:50:40 | INFO | train_inner | epoch 013:    668 / 986 loss=4.434, nll_loss=3.001, intra_distillation_loss=0, ppl=8, wps=28617.9, ups=7.73, wpb=3701, bsz=143.6, num_updates=12500, lr=0.000282843, gnorm=0, train_wall=12, gb_free=76.9, wall=1981
2023-05-09 03:50:53 | INFO | train_inner | epoch 013:    768 / 986 loss=4.373, nll_loss=2.931, intra_distillation_loss=0, ppl=7.63, wps=28774.2, ups=7.75, wpb=3712.9, bsz=148, num_updates=12600, lr=0.000281718, gnorm=0, train_wall=12, gb_free=76.9, wall=1994
2023-05-09 03:51:06 | INFO | train_inner | epoch 013:    868 / 986 loss=4.479, nll_loss=3.052, intra_distillation_loss=0, ppl=8.29, wps=28535, ups=7.82, wpb=3647.4, bsz=137.9, num_updates=12700, lr=0.000280607, gnorm=0, train_wall=12, gb_free=77, wall=2007
2023-05-09 03:51:19 | INFO | train_inner | epoch 013:    968 / 986 loss=4.448, nll_loss=3.018, intra_distillation_loss=0, ppl=8.1, wps=28502.7, ups=7.79, wpb=3659.3, bsz=146.5, num_updates=12800, lr=0.000279508, gnorm=0, train_wall=12, gb_free=76.8, wall=2019
2023-05-09 03:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:51:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:51:59 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.389 | nll_loss 2.808 | intra_distillation_loss 0 | ppl 7.01 | bleu 30.35 | wps 4298.2 | wpb 2821.2 | bsz 113.1 | num_updates 12818 | best_bleu 30.35
2023-05-09 03:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 12818 updates
2023-05-09 03:51:59 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 13 @ 12818 updates, score 30.35) (writing took 1.7699132229899988 seconds)
2023-05-09 03:52:01 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-09 03:52:01 | INFO | train | epoch 013 | loss 4.413 | nll_loss 2.977 | intra_distillation_loss 0 | ppl 7.87 | wps 21726.9 | ups 5.92 | wpb 3668.2 | bsz 146.4 | num_updates 12818 | lr 0.000279312 | gnorm 0 | train_wall 116 | gb_free 76.8 | wall 2062
2023-05-09 03:52:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:52:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:52:01 | INFO | fairseq.trainer | begin training epoch 14
2023-05-09 03:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:52:11 | INFO | train_inner | epoch 014:     82 / 986 loss=4.346, nll_loss=2.901, intra_distillation_loss=0, ppl=7.47, wps=7048.2, ups=1.89, wpb=3728.8, bsz=143.9, num_updates=12900, lr=0.000278423, gnorm=0, train_wall=12, gb_free=76.9, wall=2072
2023-05-09 03:52:24 | INFO | train_inner | epoch 014:    182 / 986 loss=4.371, nll_loss=2.929, intra_distillation_loss=0, ppl=7.61, wps=28443.3, ups=7.81, wpb=3641.3, bsz=139.4, num_updates=13000, lr=0.00027735, gnorm=0, train_wall=12, gb_free=76.8, wall=2085
2023-05-09 03:52:37 | INFO | train_inner | epoch 014:    282 / 986 loss=4.374, nll_loss=2.932, intra_distillation_loss=0, ppl=7.63, wps=28950.2, ups=7.8, wpb=3713, bsz=145.3, num_updates=13100, lr=0.000276289, gnorm=0, train_wall=12, gb_free=76.8, wall=2098
2023-05-09 03:52:50 | INFO | train_inner | epoch 014:    382 / 986 loss=4.389, nll_loss=2.948, intra_distillation_loss=0, ppl=7.72, wps=28744.1, ups=7.86, wpb=3654.8, bsz=127.5, num_updates=13200, lr=0.000275241, gnorm=0, train_wall=12, gb_free=77, wall=2111
2023-05-09 03:53:03 | INFO | train_inner | epoch 014:    482 / 986 loss=4.344, nll_loss=2.898, intra_distillation_loss=0, ppl=7.45, wps=28625.3, ups=7.77, wpb=3682.8, bsz=151.8, num_updates=13300, lr=0.000274204, gnorm=0, train_wall=12, gb_free=76.9, wall=2124
2023-05-09 03:53:15 | INFO | train_inner | epoch 014:    582 / 986 loss=4.348, nll_loss=2.903, intra_distillation_loss=0, ppl=7.48, wps=28237.7, ups=7.81, wpb=3617.8, bsz=159.3, num_updates=13400, lr=0.000273179, gnorm=0, train_wall=12, gb_free=76.9, wall=2136
2023-05-09 03:53:28 | INFO | train_inner | epoch 014:    682 / 986 loss=4.344, nll_loss=2.899, intra_distillation_loss=0, ppl=7.46, wps=28855.4, ups=7.82, wpb=3691.1, bsz=142.9, num_updates=13500, lr=0.000272166, gnorm=0, train_wall=12, gb_free=77.1, wall=2149
2023-05-09 03:53:41 | INFO | train_inner | epoch 014:    782 / 986 loss=4.366, nll_loss=2.924, intra_distillation_loss=0, ppl=7.59, wps=28802.1, ups=7.93, wpb=3631.9, bsz=146.8, num_updates=13600, lr=0.000271163, gnorm=0, train_wall=12, gb_free=77, wall=2162
2023-05-09 03:53:54 | INFO | train_inner | epoch 014:    882 / 986 loss=4.4, nll_loss=2.963, intra_distillation_loss=0, ppl=7.8, wps=28379.5, ups=7.7, wpb=3686.3, bsz=148.9, num_updates=13700, lr=0.000270172, gnorm=0, train_wall=12, gb_free=76.9, wall=2175
2023-05-09 03:54:07 | INFO | train_inner | epoch 014:    982 / 986 loss=4.353, nll_loss=2.909, intra_distillation_loss=0, ppl=7.51, wps=28698.3, ups=7.82, wpb=3671.9, bsz=159.6, num_updates=13800, lr=0.000269191, gnorm=0, train_wall=12, gb_free=76.7, wall=2188
2023-05-09 03:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:54:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:54:46 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.357 | nll_loss 2.77 | intra_distillation_loss 0 | ppl 6.82 | bleu 30.64 | wps 4247.2 | wpb 2821.2 | bsz 113.1 | num_updates 13804 | best_bleu 30.64
2023-05-09 03:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 13804 updates
2023-05-09 03:54:46 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 14 @ 13804 updates, score 30.64) (writing took 1.8529458460398018 seconds)
2023-05-09 03:54:48 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-09 03:54:48 | INFO | train | epoch 014 | loss 4.364 | nll_loss 2.921 | intra_distillation_loss 0 | ppl 7.57 | wps 21653.1 | ups 5.9 | wpb 3668.2 | bsz 146.4 | num_updates 13804 | lr 0.000269152 | gnorm 0 | train_wall 116 | gb_free 77 | wall 2229
2023-05-09 03:54:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:54:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:54:48 | INFO | fairseq.trainer | begin training epoch 15
2023-05-09 03:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:55:00 | INFO | train_inner | epoch 015:     96 / 986 loss=4.325, nll_loss=2.878, intra_distillation_loss=0, ppl=7.35, wps=6895.5, ups=1.86, wpb=3705.7, bsz=142.1, num_updates=13900, lr=0.000268221, gnorm=0, train_wall=12, gb_free=76.9, wall=2241
2023-05-09 03:55:13 | INFO | train_inner | epoch 015:    196 / 986 loss=4.326, nll_loss=2.877, intra_distillation_loss=0, ppl=7.35, wps=28652.9, ups=7.73, wpb=3707.2, bsz=138.5, num_updates=14000, lr=0.000267261, gnorm=0, train_wall=12, gb_free=76.7, wall=2254
2023-05-09 03:55:26 | INFO | train_inner | epoch 015:    296 / 986 loss=4.296, nll_loss=2.844, intra_distillation_loss=0, ppl=7.18, wps=28592, ups=7.81, wpb=3659.3, bsz=156.2, num_updates=14100, lr=0.000266312, gnorm=0, train_wall=12, gb_free=77, wall=2267
2023-05-09 03:55:39 | INFO | train_inner | epoch 015:    396 / 986 loss=4.329, nll_loss=2.88, intra_distillation_loss=0, ppl=7.36, wps=29078.1, ups=7.91, wpb=3674.8, bsz=134.6, num_updates=14200, lr=0.000265372, gnorm=0, train_wall=12, gb_free=76.8, wall=2280
2023-05-09 03:55:52 | INFO | train_inner | epoch 015:    496 / 986 loss=4.274, nll_loss=2.819, intra_distillation_loss=0, ppl=7.06, wps=28410.2, ups=7.8, wpb=3643, bsz=161.6, num_updates=14300, lr=0.000264443, gnorm=0, train_wall=12, gb_free=77.1, wall=2292
2023-05-09 03:56:04 | INFO | train_inner | epoch 015:    596 / 986 loss=4.34, nll_loss=2.894, intra_distillation_loss=0, ppl=7.43, wps=28745.7, ups=7.8, wpb=3683.3, bsz=139.8, num_updates=14400, lr=0.000263523, gnorm=0, train_wall=12, gb_free=76.9, wall=2305
2023-05-09 03:56:17 | INFO | train_inner | epoch 015:    696 / 986 loss=4.34, nll_loss=2.894, intra_distillation_loss=0, ppl=7.43, wps=28484.9, ups=7.87, wpb=3618.3, bsz=139.9, num_updates=14500, lr=0.000262613, gnorm=0, train_wall=12, gb_free=77, wall=2318
2023-05-09 03:56:30 | INFO | train_inner | epoch 015:    796 / 986 loss=4.319, nll_loss=2.87, intra_distillation_loss=0, ppl=7.31, wps=28442.6, ups=7.8, wpb=3646.2, bsz=151.6, num_updates=14600, lr=0.000261712, gnorm=0, train_wall=12, gb_free=77, wall=2331
2023-05-09 03:56:43 | INFO | train_inner | epoch 015:    896 / 986 loss=4.316, nll_loss=2.867, intra_distillation_loss=0, ppl=7.3, wps=28538.3, ups=7.78, wpb=3668.1, bsz=153.5, num_updates=14700, lr=0.00026082, gnorm=0, train_wall=12, gb_free=76.8, wall=2344
2023-05-09 03:56:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:56:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:57:33 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.324 | nll_loss 2.728 | intra_distillation_loss 0 | ppl 6.63 | bleu 31.3 | wps 4186.7 | wpb 2821.2 | bsz 113.1 | num_updates 14790 | best_bleu 31.3
2023-05-09 03:57:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 14790 updates
2023-05-09 03:57:33 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 03:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 15 @ 14790 updates, score 31.3) (writing took 2.0863757469924167 seconds)
2023-05-09 03:57:35 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-09 03:57:35 | INFO | train | epoch 015 | loss 4.32 | nll_loss 2.871 | intra_distillation_loss 0 | ppl 7.32 | wps 21570.3 | ups 5.88 | wpb 3668.2 | bsz 146.4 | num_updates 14790 | lr 0.000260025 | gnorm 0 | train_wall 116 | gb_free 76.9 | wall 2396
2023-05-09 03:57:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 03:57:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 03:57:36 | INFO | fairseq.trainer | begin training epoch 16
2023-05-09 03:57:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 03:57:37 | INFO | train_inner | epoch 016:     10 / 986 loss=4.343, nll_loss=2.898, intra_distillation_loss=0, ppl=7.45, wps=6773.7, ups=1.84, wpb=3679.8, bsz=141.8, num_updates=14800, lr=0.000259938, gnorm=0, train_wall=12, gb_free=76.8, wall=2398
2023-05-09 03:57:50 | INFO | train_inner | epoch 016:    110 / 986 loss=4.249, nll_loss=2.79, intra_distillation_loss=0, ppl=6.92, wps=28679, ups=7.77, wpb=3689.5, bsz=141, num_updates=14900, lr=0.000259064, gnorm=0, train_wall=12, gb_free=76.9, wall=2411
2023-05-09 03:58:03 | INFO | train_inner | epoch 016:    210 / 986 loss=4.307, nll_loss=2.855, intra_distillation_loss=0, ppl=7.23, wps=28937.4, ups=7.87, wpb=3676.4, bsz=131, num_updates=15000, lr=0.000258199, gnorm=0, train_wall=12, gb_free=77.1, wall=2424
2023-05-09 03:58:15 | INFO | train_inner | epoch 016:    310 / 986 loss=4.277, nll_loss=2.821, intra_distillation_loss=0, ppl=7.07, wps=28794, ups=7.79, wpb=3695.9, bsz=145.9, num_updates=15100, lr=0.000257343, gnorm=0, train_wall=12, gb_free=76.9, wall=2436
2023-05-09 03:58:28 | INFO | train_inner | epoch 016:    410 / 986 loss=4.189, nll_loss=2.723, intra_distillation_loss=0, ppl=6.6, wps=28690.7, ups=7.78, wpb=3688.3, bsz=169.1, num_updates=15200, lr=0.000256495, gnorm=0, train_wall=12, gb_free=77, wall=2449
2023-05-09 03:58:41 | INFO | train_inner | epoch 016:    510 / 986 loss=4.309, nll_loss=2.858, intra_distillation_loss=0, ppl=7.25, wps=28444.2, ups=7.86, wpb=3617.2, bsz=137.1, num_updates=15300, lr=0.000255655, gnorm=0, train_wall=12, gb_free=76.8, wall=2462
2023-05-09 03:58:54 | INFO | train_inner | epoch 016:    610 / 986 loss=4.258, nll_loss=2.801, intra_distillation_loss=0, ppl=6.97, wps=28535.8, ups=7.83, wpb=3646.6, bsz=151.9, num_updates=15400, lr=0.000254824, gnorm=0, train_wall=12, gb_free=76.9, wall=2475
2023-05-09 03:59:07 | INFO | train_inner | epoch 016:    710 / 986 loss=4.33, nll_loss=2.882, intra_distillation_loss=0, ppl=7.37, wps=28720.4, ups=7.79, wpb=3686.8, bsz=133.8, num_updates=15500, lr=0.000254, gnorm=0, train_wall=12, gb_free=76.8, wall=2488
2023-05-09 03:59:19 | INFO | train_inner | epoch 016:    810 / 986 loss=4.285, nll_loss=2.832, intra_distillation_loss=0, ppl=7.12, wps=28454.2, ups=7.83, wpb=3634.9, bsz=156.4, num_updates=15600, lr=0.000253185, gnorm=0, train_wall=12, gb_free=76.8, wall=2500
2023-05-09 03:59:32 | INFO | train_inner | epoch 016:    910 / 986 loss=4.332, nll_loss=2.884, intra_distillation_loss=0, ppl=7.38, wps=28554.4, ups=7.81, wpb=3658.1, bsz=143, num_updates=15700, lr=0.000252377, gnorm=0, train_wall=12, gb_free=76.9, wall=2513
2023-05-09 03:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 03:59:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:00:21 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.305 | nll_loss 2.713 | intra_distillation_loss 0 | ppl 6.56 | bleu 31.71 | wps 4220.1 | wpb 2821.2 | bsz 113.1 | num_updates 15776 | best_bleu 31.71
2023-05-09 04:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 15776 updates
2023-05-09 04:00:21 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:00:22 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:00:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 16 @ 15776 updates, score 31.71) (writing took 1.7664181659929454 seconds)
2023-05-09 04:00:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-09 04:00:23 | INFO | train | epoch 016 | loss 4.28 | nll_loss 2.826 | intra_distillation_loss 0 | ppl 7.09 | wps 21636.2 | ups 5.9 | wpb 3668.2 | bsz 146.4 | num_updates 15776 | lr 0.000251769 | gnorm 0 | train_wall 116 | gb_free 76.9 | wall 2564
2023-05-09 04:00:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:00:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:00:23 | INFO | fairseq.trainer | begin training epoch 17
2023-05-09 04:00:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:00:26 | INFO | train_inner | epoch 017:     24 / 986 loss=4.233, nll_loss=2.773, intra_distillation_loss=0, ppl=6.84, wps=6928.5, ups=1.86, wpb=3716.1, bsz=157.9, num_updates=15800, lr=0.000251577, gnorm=0, train_wall=12, gb_free=76.9, wall=2567
2023-05-09 04:00:39 | INFO | train_inner | epoch 017:    124 / 986 loss=4.183, nll_loss=2.716, intra_distillation_loss=0, ppl=6.57, wps=28156.4, ups=7.82, wpb=3602.2, bsz=160.2, num_updates=15900, lr=0.000250785, gnorm=0, train_wall=12, gb_free=76.9, wall=2580
2023-05-09 04:00:52 | INFO | train_inner | epoch 017:    224 / 986 loss=4.161, nll_loss=2.69, intra_distillation_loss=0, ppl=6.45, wps=28697.4, ups=7.74, wpb=3709.6, bsz=160.3, num_updates=16000, lr=0.00025, gnorm=0, train_wall=12, gb_free=76.8, wall=2593
2023-05-09 04:01:04 | INFO | train_inner | epoch 017:    324 / 986 loss=4.215, nll_loss=2.751, intra_distillation_loss=0, ppl=6.73, wps=28422.9, ups=7.81, wpb=3640.5, bsz=146.9, num_updates=16100, lr=0.000249222, gnorm=0, train_wall=12, gb_free=76.8, wall=2605
2023-05-09 04:01:17 | INFO | train_inner | epoch 017:    424 / 986 loss=4.242, nll_loss=2.781, intra_distillation_loss=0, ppl=6.87, wps=28774.8, ups=7.79, wpb=3692.6, bsz=140.1, num_updates=16200, lr=0.000248452, gnorm=0, train_wall=12, gb_free=76.9, wall=2618
2023-05-09 04:01:30 | INFO | train_inner | epoch 017:    524 / 986 loss=4.276, nll_loss=2.82, intra_distillation_loss=0, ppl=7.06, wps=29303.2, ups=7.79, wpb=3761.7, bsz=141.1, num_updates=16300, lr=0.000247689, gnorm=0, train_wall=12, gb_free=76.9, wall=2631
2023-05-09 04:01:43 | INFO | train_inner | epoch 017:    624 / 986 loss=4.282, nll_loss=2.828, intra_distillation_loss=0, ppl=7.1, wps=28438.1, ups=7.89, wpb=3604.4, bsz=137.7, num_updates=16400, lr=0.000246932, gnorm=0, train_wall=12, gb_free=76.8, wall=2644
2023-05-09 04:01:56 | INFO | train_inner | epoch 017:    724 / 986 loss=4.224, nll_loss=2.762, intra_distillation_loss=0, ppl=6.79, wps=28292.4, ups=7.7, wpb=3676.2, bsz=160.9, num_updates=16500, lr=0.000246183, gnorm=0, train_wall=12, gb_free=76.8, wall=2657
2023-05-09 04:02:09 | INFO | train_inner | epoch 017:    824 / 986 loss=4.256, nll_loss=2.799, intra_distillation_loss=0, ppl=6.96, wps=28373.4, ups=7.81, wpb=3634.7, bsz=145.4, num_updates=16600, lr=0.00024544, gnorm=0, train_wall=12, gb_free=77.1, wall=2669
2023-05-09 04:02:21 | INFO | train_inner | epoch 017:    924 / 986 loss=4.311, nll_loss=2.86, intra_distillation_loss=0, ppl=7.26, wps=29168.7, ups=7.94, wpb=3671.7, bsz=136.7, num_updates=16700, lr=0.000244704, gnorm=0, train_wall=12, gb_free=76.8, wall=2682
2023-05-09 04:02:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:02:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:03:08 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.291 | nll_loss 2.689 | intra_distillation_loss 0 | ppl 6.45 | bleu 31.92 | wps 4236.4 | wpb 2821.2 | bsz 113.1 | num_updates 16762 | best_bleu 31.92
2023-05-09 04:03:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 16762 updates
2023-05-09 04:03:08 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:03:09 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:03:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 17 @ 16762 updates, score 31.92) (writing took 1.7944698020583019 seconds)
2023-05-09 04:03:10 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-09 04:03:10 | INFO | train | epoch 017 | loss 4.242 | nll_loss 2.783 | intra_distillation_loss 0 | ppl 6.88 | wps 21667.4 | ups 5.91 | wpb 3668.2 | bsz 146.4 | num_updates 16762 | lr 0.000244251 | gnorm 0 | train_wall 116 | gb_free 76.9 | wall 2730
2023-05-09 04:03:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:03:10 | INFO | fairseq.trainer | begin training epoch 18
2023-05-09 04:03:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:03:15 | INFO | train_inner | epoch 018:     38 / 986 loss=4.303, nll_loss=2.851, intra_distillation_loss=0, ppl=7.22, wps=6817.7, ups=1.87, wpb=3642.1, bsz=126.8, num_updates=16800, lr=0.000243975, gnorm=0, train_wall=12, gb_free=77, wall=2735
2023-05-09 04:03:27 | INFO | train_inner | epoch 018:    138 / 986 loss=4.214, nll_loss=2.75, intra_distillation_loss=0, ppl=6.73, wps=28511.2, ups=7.79, wpb=3657.9, bsz=135.8, num_updates=16900, lr=0.000243252, gnorm=0, train_wall=12, gb_free=76.9, wall=2748
2023-05-09 04:03:40 | INFO | train_inner | epoch 018:    238 / 986 loss=4.257, nll_loss=2.799, intra_distillation_loss=0, ppl=6.96, wps=28900.2, ups=7.92, wpb=3646.9, bsz=132.8, num_updates=17000, lr=0.000242536, gnorm=0, train_wall=12, gb_free=76.9, wall=2761
2023-05-09 04:03:53 | INFO | train_inner | epoch 018:    338 / 986 loss=4.223, nll_loss=2.76, intra_distillation_loss=0, ppl=6.77, wps=28737.7, ups=7.82, wpb=3673.7, bsz=143.5, num_updates=17100, lr=0.000241825, gnorm=0, train_wall=12, gb_free=76.8, wall=2774
2023-05-09 04:04:06 | INFO | train_inner | epoch 018:    438 / 986 loss=4.106, nll_loss=2.629, intra_distillation_loss=0, ppl=6.19, wps=28760.4, ups=7.67, wpb=3750, bsz=171.4, num_updates=17200, lr=0.000241121, gnorm=0, train_wall=12, gb_free=76.8, wall=2787
2023-05-09 04:04:19 | INFO | train_inner | epoch 018:    538 / 986 loss=4.23, nll_loss=2.769, intra_distillation_loss=0, ppl=6.82, wps=28582.7, ups=7.82, wpb=3657.4, bsz=148, num_updates=17300, lr=0.000240424, gnorm=0, train_wall=12, gb_free=77.1, wall=2800
2023-05-09 04:04:32 | INFO | train_inner | epoch 018:    638 / 986 loss=4.161, nll_loss=2.691, intra_distillation_loss=0, ppl=6.46, wps=28538.9, ups=7.72, wpb=3696.7, bsz=165.7, num_updates=17400, lr=0.000239732, gnorm=0, train_wall=12, gb_free=77, wall=2812
2023-05-09 04:04:44 | INFO | train_inner | epoch 018:    738 / 986 loss=4.233, nll_loss=2.772, intra_distillation_loss=0, ppl=6.83, wps=28842.1, ups=7.87, wpb=3664.1, bsz=138.6, num_updates=17500, lr=0.000239046, gnorm=0, train_wall=12, gb_free=77.1, wall=2825
2023-05-09 04:04:57 | INFO | train_inner | epoch 018:    838 / 986 loss=4.248, nll_loss=2.789, intra_distillation_loss=0, ppl=6.91, wps=28042, ups=7.88, wpb=3559.6, bsz=147.4, num_updates=17600, lr=0.000238366, gnorm=0, train_wall=12, gb_free=77.1, wall=2838
2023-05-09 04:05:10 | INFO | train_inner | epoch 018:    938 / 986 loss=4.222, nll_loss=2.76, intra_distillation_loss=0, ppl=6.78, wps=28815, ups=7.82, wpb=3686, bsz=141.6, num_updates=17700, lr=0.000237691, gnorm=0, train_wall=12, gb_free=77, wall=2851
2023-05-09 04:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:05:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:05:54 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.265 | nll_loss 2.664 | intra_distillation_loss 0 | ppl 6.34 | bleu 32.02 | wps 4301.8 | wpb 2821.2 | bsz 113.1 | num_updates 17748 | best_bleu 32.02
2023-05-09 04:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 17748 updates
2023-05-09 04:05:54 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:05:55 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:05:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 18 @ 17748 updates, score 32.02) (writing took 1.9926495349500328 seconds)
2023-05-09 04:05:56 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-09 04:05:56 | INFO | train | epoch 018 | loss 4.212 | nll_loss 2.748 | intra_distillation_loss 0 | ppl 6.72 | wps 21720.7 | ups 5.92 | wpb 3668.2 | bsz 146.4 | num_updates 17748 | lr 0.00023737 | gnorm 0 | train_wall 116 | gb_free 76.8 | wall 2897
2023-05-09 04:05:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:05:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:05:56 | INFO | fairseq.trainer | begin training epoch 19
2023-05-09 04:05:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:06:03 | INFO | train_inner | epoch 019:     52 / 986 loss=4.16, nll_loss=2.69, intra_distillation_loss=0, ppl=6.45, wps=7066.1, ups=1.88, wpb=3754.1, bsz=149.1, num_updates=17800, lr=0.000237023, gnorm=0, train_wall=12, gb_free=76.9, wall=2904
2023-05-09 04:06:16 | INFO | train_inner | epoch 019:    152 / 986 loss=4.151, nll_loss=2.679, intra_distillation_loss=0, ppl=6.41, wps=28455.2, ups=7.78, wpb=3655.4, bsz=148.6, num_updates=17900, lr=0.00023636, gnorm=0, train_wall=12, gb_free=76.9, wall=2917
2023-05-09 04:06:29 | INFO | train_inner | epoch 019:    252 / 986 loss=4.137, nll_loss=2.663, intra_distillation_loss=0, ppl=6.33, wps=29037.7, ups=7.76, wpb=3743.2, bsz=150, num_updates=18000, lr=0.000235702, gnorm=0, train_wall=12, gb_free=76.9, wall=2930
2023-05-09 04:06:41 | INFO | train_inner | epoch 019:    352 / 986 loss=4.099, nll_loss=2.621, intra_distillation_loss=0, ppl=6.15, wps=28619.1, ups=7.83, wpb=3654.5, bsz=160.6, num_updates=18100, lr=0.00023505, gnorm=0, train_wall=12, gb_free=76.8, wall=2942
2023-05-09 04:06:54 | INFO | train_inner | epoch 019:    452 / 986 loss=4.158, nll_loss=2.687, intra_distillation_loss=0, ppl=6.44, wps=28509.1, ups=7.82, wpb=3645.7, bsz=150.6, num_updates=18200, lr=0.000234404, gnorm=0, train_wall=12, gb_free=76.9, wall=2955
2023-05-09 04:07:07 | INFO | train_inner | epoch 019:    552 / 986 loss=4.238, nll_loss=2.777, intra_distillation_loss=0, ppl=6.86, wps=28232.4, ups=7.73, wpb=3652.6, bsz=137.4, num_updates=18300, lr=0.000233762, gnorm=0, train_wall=12, gb_free=76.7, wall=2968
2023-05-09 04:07:20 | INFO | train_inner | epoch 019:    652 / 986 loss=4.262, nll_loss=2.805, intra_distillation_loss=0, ppl=6.99, wps=27938, ups=7.98, wpb=3499.8, bsz=130.4, num_updates=18400, lr=0.000233126, gnorm=0, train_wall=12, gb_free=77.1, wall=2981
2023-05-09 04:07:33 | INFO | train_inner | epoch 019:    752 / 986 loss=4.14, nll_loss=2.667, intra_distillation_loss=0, ppl=6.35, wps=28888.4, ups=7.69, wpb=3756.3, bsz=162.2, num_updates=18500, lr=0.000232495, gnorm=0, train_wall=12, gb_free=77, wall=2994
2023-05-09 04:07:46 | INFO | train_inner | epoch 019:    852 / 986 loss=4.151, nll_loss=2.679, intra_distillation_loss=0, ppl=6.41, wps=28918.6, ups=7.73, wpb=3740.5, bsz=159, num_updates=18600, lr=0.000231869, gnorm=0, train_wall=12, gb_free=76.9, wall=3007
2023-05-09 04:07:58 | INFO | train_inner | epoch 019:    952 / 986 loss=4.283, nll_loss=2.829, intra_distillation_loss=0, ppl=7.11, wps=28751.3, ups=7.92, wpb=3630.6, bsz=121.8, num_updates=18700, lr=0.000231249, gnorm=0, train_wall=12, gb_free=76.8, wall=3019
2023-05-09 04:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:08:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:08:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.235 | nll_loss 2.629 | intra_distillation_loss 0 | ppl 6.18 | bleu 32.37 | wps 4198.1 | wpb 2821.2 | bsz 113.1 | num_updates 18734 | best_bleu 32.37
2023-05-09 04:08:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 18734 updates
2023-05-09 04:08:42 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:08:43 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_best.pt (epoch 19 @ 18734 updates, score 32.37) (writing took 1.8529364370042458 seconds)
2023-05-09 04:08:43 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-09 04:08:43 | INFO | train | epoch 019 | loss 4.179 | nll_loss 2.711 | intra_distillation_loss 0 | ppl 6.55 | wps 21606.8 | ups 5.89 | wpb 3668.2 | bsz 146.4 | num_updates 18734 | lr 0.000231039 | gnorm 0 | train_wall 116 | gb_free 76.8 | wall 3064
2023-05-09 04:08:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:08:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:08:44 | INFO | fairseq.trainer | begin training epoch 20
2023-05-09 04:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:08:52 | INFO | train_inner | epoch 020:     66 / 986 loss=4.176, nll_loss=2.707, intra_distillation_loss=0, ppl=6.53, wps=6873.4, ups=1.86, wpb=3700.6, bsz=135.7, num_updates=18800, lr=0.000230633, gnorm=0, train_wall=12, gb_free=76.9, wall=3073
2023-05-09 04:09:05 | INFO | train_inner | epoch 020:    166 / 986 loss=4.141, nll_loss=2.668, intra_distillation_loss=0, ppl=6.36, wps=28306.7, ups=7.8, wpb=3627.6, bsz=144.1, num_updates=18900, lr=0.000230022, gnorm=0, train_wall=12, gb_free=76.7, wall=3086
2023-05-09 04:09:18 | INFO | train_inner | epoch 020:    266 / 986 loss=4.119, nll_loss=2.642, intra_distillation_loss=0, ppl=6.24, wps=28744.4, ups=7.86, wpb=3657.9, bsz=147, num_updates=19000, lr=0.000229416, gnorm=0, train_wall=12, gb_free=76.9, wall=3099
2023-05-09 04:09:31 | INFO | train_inner | epoch 020:    366 / 986 loss=4.108, nll_loss=2.63, intra_distillation_loss=0, ppl=6.19, wps=28468.9, ups=7.76, wpb=3669.8, bsz=163.3, num_updates=19100, lr=0.000228814, gnorm=0, train_wall=12, gb_free=76.8, wall=3111
2023-05-09 04:09:43 | INFO | train_inner | epoch 020:    466 / 986 loss=4.164, nll_loss=2.693, intra_distillation_loss=0, ppl=6.46, wps=28668.1, ups=7.79, wpb=3678.2, bsz=141.8, num_updates=19200, lr=0.000228218, gnorm=0, train_wall=12, gb_free=76.8, wall=3124
2023-05-09 04:09:56 | INFO | train_inner | epoch 020:    566 / 986 loss=4.106, nll_loss=2.628, intra_distillation_loss=0, ppl=6.18, wps=28731.3, ups=7.77, wpb=3696.3, bsz=155.7, num_updates=19300, lr=0.000227626, gnorm=0, train_wall=12, gb_free=76.8, wall=3137
2023-05-09 04:10:09 | INFO | train_inner | epoch 020:    666 / 986 loss=4.192, nll_loss=2.725, intra_distillation_loss=0, ppl=6.61, wps=28977.3, ups=7.9, wpb=3668.6, bsz=134.4, num_updates=19400, lr=0.000227038, gnorm=0, train_wall=12, gb_free=76.7, wall=3150
2023-05-09 04:10:22 | INFO | train_inner | epoch 020:    766 / 986 loss=4.156, nll_loss=2.684, intra_distillation_loss=0, ppl=6.42, wps=29189, ups=7.87, wpb=3706.8, bsz=144.2, num_updates=19500, lr=0.000226455, gnorm=0, train_wall=12, gb_free=76.8, wall=3162
2023-05-09 04:10:34 | INFO | train_inner | epoch 020:    866 / 986 loss=4.223, nll_loss=2.761, intra_distillation_loss=0, ppl=6.78, wps=28026, ups=7.82, wpb=3583.6, bsz=141.1, num_updates=19600, lr=0.000225877, gnorm=0, train_wall=12, gb_free=77, wall=3175
2023-05-09 04:10:47 | INFO | train_inner | epoch 020:    966 / 986 loss=4.145, nll_loss=2.673, intra_distillation_loss=0, ppl=6.38, wps=28657.9, ups=7.76, wpb=3691, bsz=153.9, num_updates=19700, lr=0.000225303, gnorm=0, train_wall=12, gb_free=77, wall=3188
2023-05-09 04:10:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:10:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:11:28 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.232 | nll_loss 2.625 | intra_distillation_loss 0 | ppl 6.17 | bleu 32.11 | wps 4317.3 | wpb 2821.2 | bsz 113.1 | num_updates 19720 | best_bleu 32.37
2023-05-09 04:11:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 19720 updates
2023-05-09 04:11:28 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_last.pt
2023-05-09 04:11:29 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_last.pt
2023-05-09 04:11:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_6000/checkpoints_he/checkpoint_last.pt (epoch 20 @ 19720 updates, score 32.11) (writing took 0.9469016310758889 seconds)
2023-05-09 04:11:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-09 04:11:29 | INFO | train | epoch 020 | loss 4.15 | nll_loss 2.677 | intra_distillation_loss 0 | ppl 6.4 | wps 21890.4 | ups 5.97 | wpb 3668.2 | bsz 146.4 | num_updates 19720 | lr 0.000225189 | gnorm 0 | train_wall 116 | gb_free 77.1 | wall 3230
2023-05-09 04:11:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:11:29 | INFO | fairseq_cli.train | done training in 3228.2 seconds
/home/tli104/my_fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-05-09 04:11:41 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-05-09 04:11:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/scratch4/cs601/tli104/wf_8000/checkpoints_he', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_alpha=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, alpha=5.0, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/data-bin-he', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, div='X', dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, importance_metric='magnitude', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_updates_train=25000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_iter=2, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/scratch4/cs601/tli104/wf_8000/checkpoints_he', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, smooth_scores=True, source_lang=None, start_freezing=8000, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation_intra_distillation', temperature_p=2, temperature_q=5, tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weighted_freezing=True, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation_intra_distillation', 'data': 'data/data-bin-he', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False, 'alpha': 5.0, 'adaptive_alpha': 0, 'max_updates_train': 25000, 'temperature_q': 5.0, 'temperature_p': 2.0, 'num_iter': 2, 'div': 'X', 'importance_metric': 'magnitude', 'smooth_scores': True, 'weighted_freezing': True, 'start_freezing': 8000}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-09 04:11:44 | INFO | fairseq.tasks.translation | [he] dictionary: 12001 types
2023-05-09 04:11:44 | INFO | fairseq.tasks.translation | [en] dictionary: 12001 types
2023-05-09 04:11:44 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=12001, bias=False)
  )
)
2023-05-09 04:11:44 | INFO | fairseq_cli.train | task: Translation_Intra_Distillation
2023-05-09 04:11:44 | INFO | fairseq_cli.train | model: TransformerModel
2023-05-09 04:11:44 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-09 04:11:44 | INFO | fairseq_cli.train | num. shared model params: 43,832,320 (num. trained: 43,832,320)
2023-05-09 04:11:44 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-09 04:11:44 | INFO | fairseq.data.data_utils | loaded 6,561 examples from: data/data-bin-he/valid.he-en.he
2023-05-09 04:11:44 | INFO | fairseq.data.data_utils | loaded 6,561 examples from: data/data-bin-he/valid.he-en.en
2023-05-09 04:11:44 | INFO | fairseq.tasks.translation | data/data-bin-he valid he-en 6561 examples
2023-05-09 04:11:45 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-09 04:11:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 04:11:45 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.347 GB ; name = Graphics Device                         
2023-05-09 04:11:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-05-09 04:11:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-05-09 04:11:45 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-05-09 04:11:45 | INFO | fairseq.trainer | Preparing to load checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_last.pt
2023-05-09 04:11:45 | INFO | fairseq.trainer | No existing checkpoint found /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_last.pt
2023-05-09 04:11:45 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-09 04:11:45 | INFO | fairseq.data.data_utils | loaded 144,345 examples from: data/data-bin-he/train.he-en.he
2023-05-09 04:11:45 | INFO | fairseq.data.data_utils | loaded 144,345 examples from: data/data-bin-he/train.he-en.en
2023-05-09 04:11:45 | INFO | fairseq.tasks.translation | data/data-bin-he train he-en 144345 examples
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 04:11:45 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2023-05-09 04:11:45 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-09 04:11:45 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-09 04:11:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:11:47 | INFO | fairseq.trainer | begin training epoch 1
2023-05-09 04:11:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:11:59 | INFO | train_inner | epoch 001:    100 / 986 loss=12.831, nll_loss=12.666, intra_distillation_loss=0, ppl=6498.56, wps=33578.3, ups=9.19, wpb=3651.6, bsz=144.2, num_updates=100, lr=1.25e-05, gnorm=3.807, train_wall=11, gb_free=76.9, wall=14
2023-05-09 04:12:10 | INFO | train_inner | epoch 001:    200 / 986 loss=11.106, nll_loss=10.735, intra_distillation_loss=0, ppl=1704.89, wps=34714.7, ups=9.36, wpb=3710, bsz=137.3, num_updates=200, lr=2.5e-05, gnorm=1.73, train_wall=10, gb_free=77.4, wall=24
2023-05-09 04:12:21 | INFO | train_inner | epoch 001:    300 / 986 loss=10.099, nll_loss=9.574, intra_distillation_loss=0, ppl=762.33, wps=34254.5, ups=9.17, wpb=3735.6, bsz=155.1, num_updates=300, lr=3.75e-05, gnorm=1.678, train_wall=10, gb_free=76.8, wall=35
2023-05-09 04:12:31 | INFO | train_inner | epoch 001:    400 / 986 loss=9.663, nll_loss=9.017, intra_distillation_loss=0, ppl=518.06, wps=33898.8, ups=9.26, wpb=3662.3, bsz=140.1, num_updates=400, lr=5e-05, gnorm=1.58, train_wall=10, gb_free=76.8, wall=46
2023-05-09 04:12:42 | INFO | train_inner | epoch 001:    500 / 986 loss=9.44, nll_loss=8.736, intra_distillation_loss=0, ppl=426.45, wps=33537.7, ups=9.16, wpb=3662.5, bsz=149.8, num_updates=500, lr=6.25e-05, gnorm=1.537, train_wall=10, gb_free=77.2, wall=57
2023-05-09 04:12:53 | INFO | train_inner | epoch 001:    600 / 986 loss=9.205, nll_loss=8.462, intra_distillation_loss=0, ppl=352.71, wps=33395.5, ups=9.18, wpb=3638.8, bsz=163.6, num_updates=600, lr=7.5e-05, gnorm=1.738, train_wall=10, gb_free=76.8, wall=68
2023-05-09 04:13:04 | INFO | train_inner | epoch 001:    700 / 986 loss=9.052, nll_loss=8.288, intra_distillation_loss=0, ppl=312.58, wps=33613.2, ups=9.18, wpb=3662.5, bsz=144.6, num_updates=700, lr=8.75e-05, gnorm=1.587, train_wall=10, gb_free=76.9, wall=79
2023-05-09 04:13:15 | INFO | train_inner | epoch 001:    800 / 986 loss=8.779, nll_loss=7.979, intra_distillation_loss=0, ppl=252.25, wps=34158.9, ups=9.32, wpb=3666.3, bsz=149.5, num_updates=800, lr=0.0001, gnorm=1.655, train_wall=10, gb_free=76.9, wall=89
2023-05-09 04:13:25 | INFO | train_inner | epoch 001:    900 / 986 loss=8.697, nll_loss=7.884, intra_distillation_loss=0, ppl=236.18, wps=33925.1, ups=9.39, wpb=3611.2, bsz=140.5, num_updates=900, lr=0.0001125, gnorm=1.797, train_wall=10, gb_free=76.8, wall=100
2023-05-09 04:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:13:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:14:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.292 | nll_loss 7.372 | intra_distillation_loss 0 | ppl 165.7 | bleu 2.02 | wps 3902.1 | wpb 2821.2 | bsz 113.1 | num_updates 986
2023-05-09 04:14:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 986 updates
2023-05-09 04:14:17 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 1 @ 986 updates, score 2.02) (writing took 1.7903012309689075 seconds)
2023-05-09 04:14:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-09 04:14:19 | INFO | train | epoch 001 | loss 9.759 | nll_loss 9.126 | intra_distillation_loss 0 | ppl 558.66 | wps 24022.8 | ups 6.55 | wpb 3668.2 | bsz 146.4 | num_updates 986 | lr 0.00012325 | gnorm 1.863 | train_wall 97 | gb_free 76.8 | wall 153
2023-05-09 04:14:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:14:19 | INFO | fairseq.trainer | begin training epoch 2
2023-05-09 04:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:14:20 | INFO | train_inner | epoch 002:     14 / 986 loss=8.492, nll_loss=7.65, intra_distillation_loss=0, ppl=200.86, wps=6741.1, ups=1.82, wpb=3695.2, bsz=140.2, num_updates=1000, lr=0.000125, gnorm=1.473, train_wall=10, gb_free=76.9, wall=155
2023-05-09 04:14:31 | INFO | train_inner | epoch 002:    114 / 986 loss=8.311, nll_loss=7.444, intra_distillation_loss=0, ppl=174.15, wps=34375.7, ups=9.22, wpb=3726.8, bsz=147.2, num_updates=1100, lr=0.0001375, gnorm=1.606, train_wall=10, gb_free=76.7, wall=166
2023-05-09 04:14:42 | INFO | train_inner | epoch 002:    214 / 986 loss=8.098, nll_loss=7.202, intra_distillation_loss=0, ppl=147.25, wps=34233.2, ups=9.18, wpb=3727.3, bsz=168, num_updates=1200, lr=0.00015, gnorm=1.582, train_wall=10, gb_free=76.8, wall=177
2023-05-09 04:14:53 | INFO | train_inner | epoch 002:    314 / 986 loss=8.231, nll_loss=7.351, intra_distillation_loss=0, ppl=163.25, wps=32966.2, ups=9.23, wpb=3570.3, bsz=144.1, num_updates=1300, lr=0.0001625, gnorm=1.538, train_wall=10, gb_free=76.8, wall=187
2023-05-09 04:15:04 | INFO | train_inner | epoch 002:    414 / 986 loss=8.142, nll_loss=7.249, intra_distillation_loss=0, ppl=152.08, wps=34108.8, ups=9.3, wpb=3669.1, bsz=131.5, num_updates=1400, lr=0.000175, gnorm=1.417, train_wall=10, gb_free=76.8, wall=198
2023-05-09 04:15:14 | INFO | train_inner | epoch 002:    514 / 986 loss=7.982, nll_loss=7.069, intra_distillation_loss=0, ppl=134.23, wps=33571.8, ups=9.15, wpb=3668.4, bsz=152.5, num_updates=1500, lr=0.0001875, gnorm=1.356, train_wall=10, gb_free=76.8, wall=209
2023-05-09 04:15:25 | INFO | train_inner | epoch 002:    614 / 986 loss=7.857, nll_loss=6.925, intra_distillation_loss=0, ppl=121.49, wps=34157.1, ups=9.23, wpb=3699.8, bsz=146, num_updates=1600, lr=0.0002, gnorm=1.31, train_wall=10, gb_free=76.9, wall=220
2023-05-09 04:15:36 | INFO | train_inner | epoch 002:    714 / 986 loss=7.931, nll_loss=7.007, intra_distillation_loss=0, ppl=128.62, wps=33500.1, ups=9.3, wpb=3601.7, bsz=141.9, num_updates=1700, lr=0.0002125, gnorm=1.396, train_wall=10, gb_free=77, wall=231
2023-05-09 04:15:47 | INFO | train_inner | epoch 002:    814 / 986 loss=7.853, nll_loss=6.921, intra_distillation_loss=0, ppl=121.15, wps=33843.6, ups=9.22, wpb=3671.5, bsz=139, num_updates=1800, lr=0.000225, gnorm=1.252, train_wall=10, gb_free=76.8, wall=242
2023-05-09 04:15:58 | INFO | train_inner | epoch 002:    914 / 986 loss=7.704, nll_loss=6.751, intra_distillation_loss=0, ppl=107.68, wps=33738.9, ups=9.17, wpb=3679.4, bsz=155.8, num_updates=1900, lr=0.0002375, gnorm=1.334, train_wall=10, gb_free=76.8, wall=252
2023-05-09 04:16:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:16:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:16:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.464 | nll_loss 6.43 | intra_distillation_loss 0 | ppl 86.21 | bleu 2.88 | wps 4106.1 | wpb 2821.2 | bsz 113.1 | num_updates 1972 | best_bleu 2.88
2023-05-09 04:16:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1972 updates
2023-05-09 04:16:45 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:16:46 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 2 @ 1972 updates, score 2.88) (writing took 1.8182490109466016 seconds)
2023-05-09 04:16:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-09 04:16:47 | INFO | train | epoch 002 | loss 8 | nll_loss 7.089 | intra_distillation_loss 0 | ppl 136.12 | wps 24326.7 | ups 6.63 | wpb 3668.2 | bsz 146.4 | num_updates 1972 | lr 0.0002465 | gnorm 1.42 | train_wall 96 | gb_free 76.7 | wall 302
2023-05-09 04:16:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:16:47 | INFO | fairseq.trainer | begin training epoch 3
2023-05-09 04:16:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:16:50 | INFO | train_inner | epoch 003:     28 / 986 loss=7.767, nll_loss=6.822, intra_distillation_loss=0, ppl=113.12, wps=6933.2, ups=1.9, wpb=3645.3, bsz=131.4, num_updates=2000, lr=0.00025, gnorm=1.333, train_wall=10, gb_free=76.8, wall=305
2023-05-09 04:17:01 | INFO | train_inner | epoch 003:    128 / 986 loss=7.609, nll_loss=6.641, intra_distillation_loss=0, ppl=99.83, wps=33803.9, ups=9.14, wpb=3697.7, bsz=143.9, num_updates=2100, lr=0.0002625, gnorm=1.222, train_wall=10, gb_free=76.9, wall=316
2023-05-09 04:17:12 | INFO | train_inner | epoch 003:    228 / 986 loss=7.544, nll_loss=6.568, intra_distillation_loss=0, ppl=94.89, wps=33619.4, ups=9.24, wpb=3637.9, bsz=142.2, num_updates=2200, lr=0.000275, gnorm=1.203, train_wall=10, gb_free=76.9, wall=327
2023-05-09 04:17:23 | INFO | train_inner | epoch 003:    328 / 986 loss=7.568, nll_loss=6.594, intra_distillation_loss=0, ppl=96.61, wps=33699.4, ups=9.24, wpb=3647.1, bsz=142.8, num_updates=2300, lr=0.0002875, gnorm=1.29, train_wall=10, gb_free=77, wall=338
2023-05-09 04:17:34 | INFO | train_inner | epoch 003:    428 / 986 loss=7.395, nll_loss=6.4, intra_distillation_loss=0, ppl=84.43, wps=34181.4, ups=9.3, wpb=3676.1, bsz=141.4, num_updates=2400, lr=0.0003, gnorm=1.213, train_wall=10, gb_free=76.7, wall=348
2023-05-09 04:17:45 | INFO | train_inner | epoch 003:    528 / 986 loss=7.256, nll_loss=6.239, intra_distillation_loss=0, ppl=75.54, wps=33616.5, ups=9.25, wpb=3633.2, bsz=159, num_updates=2500, lr=0.0003125, gnorm=1.34, train_wall=10, gb_free=76.8, wall=359
2023-05-09 04:17:55 | INFO | train_inner | epoch 003:    628 / 986 loss=7.143, nll_loss=6.11, intra_distillation_loss=0, ppl=69.08, wps=34574.7, ups=9.25, wpb=3737.7, bsz=151.8, num_updates=2600, lr=0.000325, gnorm=1.177, train_wall=10, gb_free=76.9, wall=370
2023-05-09 04:18:06 | INFO | train_inner | epoch 003:    728 / 986 loss=7.281, nll_loss=6.268, intra_distillation_loss=0, ppl=77.08, wps=33802.9, ups=9.25, wpb=3654.6, bsz=140.6, num_updates=2700, lr=0.0003375, gnorm=1.267, train_wall=10, gb_free=77, wall=381
2023-05-09 04:18:17 | INFO | train_inner | epoch 003:    828 / 986 loss=7.068, nll_loss=6.026, intra_distillation_loss=0, ppl=65.14, wps=34004.6, ups=9.2, wpb=3696.2, bsz=155.8, num_updates=2800, lr=0.00035, gnorm=1.229, train_wall=10, gb_free=77.1, wall=392
2023-05-09 04:18:28 | INFO | train_inner | epoch 003:    928 / 986 loss=7.064, nll_loss=6.02, intra_distillation_loss=0, ppl=64.9, wps=34024.8, ups=9.23, wpb=3687.6, bsz=144.2, num_updates=2900, lr=0.0003625, gnorm=1.222, train_wall=10, gb_free=76.9, wall=402
2023-05-09 04:18:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:18:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:19:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.733 | nll_loss 5.566 | intra_distillation_loss 0 | ppl 47.39 | bleu 6.76 | wps 4109.1 | wpb 2821.2 | bsz 113.1 | num_updates 2958 | best_bleu 6.76
2023-05-09 04:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2958 updates
2023-05-09 04:19:14 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:19:15 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 3 @ 2958 updates, score 6.76) (writing took 1.776175144012086 seconds)
2023-05-09 04:19:16 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-09 04:19:16 | INFO | train | epoch 003 | loss 7.313 | nll_loss 6.304 | intra_distillation_loss 0 | ppl 79.03 | wps 24331.6 | ups 6.63 | wpb 3668.2 | bsz 146.4 | num_updates 2958 | lr 0.00036975 | gnorm 1.251 | train_wall 96 | gb_free 76.9 | wall 451
2023-05-09 04:19:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:19:16 | INFO | fairseq.trainer | begin training epoch 4
2023-05-09 04:19:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:19:21 | INFO | train_inner | epoch 004:     42 / 986 loss=6.991, nll_loss=5.936, intra_distillation_loss=0, ppl=61.23, wps=6900.7, ups=1.9, wpb=3637.9, bsz=141.6, num_updates=3000, lr=0.000375, gnorm=1.339, train_wall=10, gb_free=76.9, wall=455
2023-05-09 04:19:31 | INFO | train_inner | epoch 004:    142 / 986 loss=6.944, nll_loss=5.882, intra_distillation_loss=0, ppl=58.98, wps=33534.4, ups=9.32, wpb=3599, bsz=132.6, num_updates=3100, lr=0.0003875, gnorm=1.307, train_wall=10, gb_free=76.9, wall=466
2023-05-09 04:19:42 | INFO | train_inner | epoch 004:    242 / 986 loss=6.808, nll_loss=5.727, intra_distillation_loss=0, ppl=52.95, wps=34293.8, ups=9.25, wpb=3705.7, bsz=133.8, num_updates=3200, lr=0.0004, gnorm=1.213, train_wall=10, gb_free=77.1, wall=477
2023-05-09 04:19:53 | INFO | train_inner | epoch 004:    342 / 986 loss=6.784, nll_loss=5.698, intra_distillation_loss=0, ppl=51.92, wps=34289, ups=9.28, wpb=3695.7, bsz=140.4, num_updates=3300, lr=0.0004125, gnorm=1.309, train_wall=10, gb_free=76.8, wall=488
2023-05-09 04:20:04 | INFO | train_inner | epoch 004:    442 / 986 loss=6.797, nll_loss=5.714, intra_distillation_loss=0, ppl=52.48, wps=33166, ups=9.26, wpb=3583.1, bsz=131, num_updates=3400, lr=0.000425, gnorm=1.258, train_wall=10, gb_free=76.8, wall=498
2023-05-09 04:20:15 | INFO | train_inner | epoch 004:    542 / 986 loss=6.464, nll_loss=5.333, intra_distillation_loss=0, ppl=40.3, wps=33852.3, ups=9.19, wpb=3683.9, bsz=167.6, num_updates=3500, lr=0.0004375, gnorm=1.283, train_wall=10, gb_free=77.1, wall=509
2023-05-09 04:20:26 | INFO | train_inner | epoch 004:    642 / 986 loss=6.578, nll_loss=5.461, intra_distillation_loss=0, ppl=44.06, wps=33308.8, ups=9.13, wpb=3648, bsz=143.9, num_updates=3600, lr=0.00045, gnorm=1.308, train_wall=10, gb_free=76.8, wall=520
2023-05-09 04:20:36 | INFO | train_inner | epoch 004:    742 / 986 loss=6.374, nll_loss=5.23, intra_distillation_loss=0, ppl=37.54, wps=33893.2, ups=9.17, wpb=3694.5, bsz=166.3, num_updates=3700, lr=0.0004625, gnorm=1.357, train_wall=10, gb_free=76.9, wall=531
2023-05-09 04:20:47 | INFO | train_inner | epoch 004:    842 / 986 loss=6.37, nll_loss=5.224, intra_distillation_loss=0, ppl=37.37, wps=34351.2, ups=9.25, wpb=3714, bsz=148.5, num_updates=3800, lr=0.000475, gnorm=1.184, train_wall=10, gb_free=76.9, wall=542
2023-05-09 04:20:58 | INFO | train_inner | epoch 004:    942 / 986 loss=6.375, nll_loss=5.229, intra_distillation_loss=0, ppl=37.5, wps=34013.2, ups=9.26, wpb=3671.5, bsz=145.2, num_updates=3900, lr=0.0004875, gnorm=1.235, train_wall=10, gb_free=77, wall=553
2023-05-09 04:21:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:21:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:21:41 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.048 | nll_loss 4.758 | intra_distillation_loss 0 | ppl 27.06 | bleu 12.67 | wps 4313.3 | wpb 2821.2 | bsz 113.1 | num_updates 3944 | best_bleu 12.67
2023-05-09 04:21:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3944 updates
2023-05-09 04:21:41 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:21:42 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:21:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 4 @ 3944 updates, score 12.67) (writing took 1.8463747820351273 seconds)
2023-05-09 04:21:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-09 04:21:43 | INFO | train | epoch 004 | loss 6.605 | nll_loss 5.493 | intra_distillation_loss 0 | ppl 45.05 | wps 24625.1 | ups 6.71 | wpb 3668.2 | bsz 146.4 | num_updates 3944 | lr 0.000493 | gnorm 1.267 | train_wall 96 | gb_free 76.9 | wall 597
2023-05-09 04:21:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:21:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:21:43 | INFO | fairseq.trainer | begin training epoch 5
2023-05-09 04:21:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:21:49 | INFO | train_inner | epoch 005:     56 / 986 loss=6.127, nll_loss=4.948, intra_distillation_loss=0, ppl=30.88, wps=7228.6, ups=1.96, wpb=3688.7, bsz=173.4, num_updates=4000, lr=0.0005, gnorm=1.25, train_wall=10, gb_free=76.8, wall=604
2023-05-09 04:22:00 | INFO | train_inner | epoch 005:    156 / 986 loss=6.156, nll_loss=4.979, intra_distillation_loss=0, ppl=31.53, wps=33902.2, ups=9.2, wpb=3684.5, bsz=154.9, num_updates=4100, lr=0.000493865, gnorm=1.238, train_wall=10, gb_free=76.8, wall=615
2023-05-09 04:22:11 | INFO | train_inner | epoch 005:    256 / 986 loss=6.16, nll_loss=4.983, intra_distillation_loss=0, ppl=31.62, wps=33732.5, ups=9.19, wpb=3669.6, bsz=139.7, num_updates=4200, lr=0.00048795, gnorm=1.196, train_wall=10, gb_free=76.7, wall=625
2023-05-09 04:22:22 | INFO | train_inner | epoch 005:    356 / 986 loss=6.05, nll_loss=4.856, intra_distillation_loss=0, ppl=28.96, wps=33597, ups=9.18, wpb=3660.4, bsz=139.6, num_updates=4300, lr=0.000482243, gnorm=1.185, train_wall=10, gb_free=76.9, wall=636
2023-05-09 04:22:33 | INFO | train_inner | epoch 005:    456 / 986 loss=6.04, nll_loss=4.844, intra_distillation_loss=0, ppl=28.73, wps=33822.9, ups=9.2, wpb=3675.7, bsz=140, num_updates=4400, lr=0.000476731, gnorm=1.193, train_wall=10, gb_free=76.9, wall=647
2023-05-09 04:22:43 | INFO | train_inner | epoch 005:    556 / 986 loss=5.899, nll_loss=4.682, intra_distillation_loss=0, ppl=25.67, wps=33859.5, ups=9.47, wpb=3574.7, bsz=142.5, num_updates=4500, lr=0.000471405, gnorm=1.201, train_wall=9, gb_free=76.8, wall=658
2023-05-09 04:22:54 | INFO | train_inner | epoch 005:    656 / 986 loss=5.828, nll_loss=4.602, intra_distillation_loss=0, ppl=24.28, wps=34373.5, ups=9.22, wpb=3726.2, bsz=146, num_updates=4600, lr=0.000466252, gnorm=1.16, train_wall=10, gb_free=76.9, wall=669
2023-05-09 04:23:05 | INFO | train_inner | epoch 005:    756 / 986 loss=5.77, nll_loss=4.536, intra_distillation_loss=0, ppl=23.2, wps=33793.1, ups=9.15, wpb=3694.5, bsz=161, num_updates=4700, lr=0.000461266, gnorm=1.189, train_wall=10, gb_free=76.8, wall=680
2023-05-09 04:23:15 | INFO | train_inner | epoch 005:    856 / 986 loss=5.813, nll_loss=4.583, intra_distillation_loss=0, ppl=23.96, wps=34135.4, ups=9.45, wpb=3613.3, bsz=128.2, num_updates=4800, lr=0.000456435, gnorm=1.159, train_wall=10, gb_free=76.9, wall=690
2023-05-09 04:23:26 | INFO | train_inner | epoch 005:    956 / 986 loss=5.618, nll_loss=4.361, intra_distillation_loss=0, ppl=20.55, wps=33908.1, ups=9.21, wpb=3682.8, bsz=152.7, num_updates=4900, lr=0.000451754, gnorm=1.158, train_wall=10, gb_free=76.8, wall=701
2023-05-09 04:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:24:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.323 | nll_loss 3.906 | intra_distillation_loss 0 | ppl 14.99 | bleu 18.99 | wps 4065.8 | wpb 2821.2 | bsz 113.1 | num_updates 4930 | best_bleu 18.99
2023-05-09 04:24:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4930 updates
2023-05-09 04:24:10 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:24:11 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 5 @ 4930 updates, score 18.99) (writing took 1.7896398649318144 seconds)
2023-05-09 04:24:12 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-09 04:24:12 | INFO | train | epoch 005 | loss 5.928 | nll_loss 4.716 | intra_distillation_loss 0 | ppl 26.29 | wps 24274.4 | ups 6.62 | wpb 3668.2 | bsz 146.4 | num_updates 4930 | lr 0.000450377 | gnorm 1.188 | train_wall 96 | gb_free 76.6 | wall 746
2023-05-09 04:24:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:24:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:24:12 | INFO | fairseq.trainer | begin training epoch 6
2023-05-09 04:24:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:24:19 | INFO | train_inner | epoch 006:     70 / 986 loss=5.616, nll_loss=4.359, intra_distillation_loss=0, ppl=20.53, wps=6856.4, ups=1.88, wpb=3642.8, bsz=147.9, num_updates=5000, lr=0.000447214, gnorm=1.158, train_wall=10, gb_free=76.8, wall=754
2023-05-09 04:24:30 | INFO | train_inner | epoch 006:    170 / 986 loss=5.571, nll_loss=4.306, intra_distillation_loss=0, ppl=19.78, wps=34542.2, ups=9.36, wpb=3688.8, bsz=134.4, num_updates=5100, lr=0.000442807, gnorm=1.143, train_wall=10, gb_free=76.8, wall=765
2023-05-09 04:24:41 | INFO | train_inner | epoch 006:    270 / 986 loss=5.479, nll_loss=4.202, intra_distillation_loss=0, ppl=18.4, wps=33714, ups=9.18, wpb=3671.7, bsz=144.6, num_updates=5200, lr=0.000438529, gnorm=1.151, train_wall=10, gb_free=76.8, wall=776
2023-05-09 04:24:52 | INFO | train_inner | epoch 006:    370 / 986 loss=5.486, nll_loss=4.207, intra_distillation_loss=0, ppl=18.47, wps=34168.7, ups=9.23, wpb=3703.1, bsz=135.6, num_updates=5300, lr=0.000434372, gnorm=1.178, train_wall=10, gb_free=76.7, wall=787
2023-05-09 04:25:03 | INFO | train_inner | epoch 006:    470 / 986 loss=5.347, nll_loss=4.05, intra_distillation_loss=0, ppl=16.56, wps=34241.5, ups=9.21, wpb=3717.1, bsz=155.3, num_updates=5400, lr=0.000430331, gnorm=1.123, train_wall=10, gb_free=76.9, wall=797
2023-05-09 04:25:14 | INFO | train_inner | epoch 006:    570 / 986 loss=5.266, nll_loss=3.957, intra_distillation_loss=0, ppl=15.53, wps=34090.9, ups=9.16, wpb=3720.3, bsz=162.6, num_updates=5500, lr=0.000426401, gnorm=1.139, train_wall=10, gb_free=76.8, wall=808
2023-05-09 04:25:25 | INFO | train_inner | epoch 006:    670 / 986 loss=5.25, nll_loss=3.939, intra_distillation_loss=0, ppl=15.33, wps=32914.7, ups=8.97, wpb=3669, bsz=162.4, num_updates=5600, lr=0.000422577, gnorm=1.121, train_wall=10, gb_free=76.8, wall=819
2023-05-09 04:25:35 | INFO | train_inner | epoch 006:    770 / 986 loss=5.448, nll_loss=4.162, intra_distillation_loss=0, ppl=17.9, wps=33859.6, ups=9.38, wpb=3609, bsz=130.4, num_updates=5700, lr=0.000418854, gnorm=1.21, train_wall=10, gb_free=76.8, wall=830
2023-05-09 04:25:46 | INFO | train_inner | epoch 006:    870 / 986 loss=5.306, nll_loss=4.002, intra_distillation_loss=0, ppl=16.02, wps=33762.4, ups=9.29, wpb=3633.4, bsz=147.4, num_updates=5800, lr=0.000415227, gnorm=1.124, train_wall=10, gb_free=76.7, wall=841
2023-05-09 04:25:57 | INFO | train_inner | epoch 006:    970 / 986 loss=5.296, nll_loss=3.991, intra_distillation_loss=0, ppl=15.9, wps=33713.8, ups=9.31, wpb=3621.5, bsz=143.9, num_updates=5900, lr=0.000411693, gnorm=1.15, train_wall=10, gb_free=76.9, wall=852
2023-05-09 04:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:25:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:26:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.041 | nll_loss 3.561 | intra_distillation_loss 0 | ppl 11.8 | bleu 23.06 | wps 4033.3 | wpb 2821.2 | bsz 113.1 | num_updates 5916 | best_bleu 23.06
2023-05-09 04:26:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5916 updates
2023-05-09 04:26:39 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:26:40 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 6 @ 5916 updates, score 23.06) (writing took 1.7762096440419555 seconds)
2023-05-09 04:26:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-09 04:26:41 | INFO | train | epoch 006 | loss 5.398 | nll_loss 4.108 | intra_distillation_loss 0 | ppl 17.24 | wps 24221.4 | ups 6.6 | wpb 3668.2 | bsz 146.4 | num_updates 5916 | lr 0.000411136 | gnorm 1.151 | train_wall 96 | gb_free 76.9 | wall 896
2023-05-09 04:26:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:26:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:26:41 | INFO | fairseq.trainer | begin training epoch 7
2023-05-09 04:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:26:50 | INFO | train_inner | epoch 007:     84 / 986 loss=5.133, nll_loss=3.805, intra_distillation_loss=0, ppl=13.98, wps=6851.5, ups=1.88, wpb=3651.3, bsz=143.1, num_updates=6000, lr=0.000408248, gnorm=1.124, train_wall=10, gb_free=76.8, wall=905
2023-05-09 04:27:01 | INFO | train_inner | epoch 007:    184 / 986 loss=5.109, nll_loss=3.778, intra_distillation_loss=0, ppl=13.71, wps=33686.5, ups=9.16, wpb=3676.1, bsz=144.6, num_updates=6100, lr=0.000404888, gnorm=1.124, train_wall=10, gb_free=76.9, wall=916
2023-05-09 04:27:12 | INFO | train_inner | epoch 007:    284 / 986 loss=5.126, nll_loss=3.795, intra_distillation_loss=0, ppl=13.88, wps=34282.8, ups=9.3, wpb=3687.6, bsz=143.8, num_updates=6200, lr=0.00040161, gnorm=1.137, train_wall=10, gb_free=76.9, wall=927
2023-05-09 04:27:23 | INFO | train_inner | epoch 007:    384 / 986 loss=5.095, nll_loss=3.759, intra_distillation_loss=0, ppl=13.54, wps=33954.1, ups=9.19, wpb=3693.1, bsz=149, num_updates=6300, lr=0.00039841, gnorm=1.122, train_wall=10, gb_free=76.9, wall=937
2023-05-09 04:27:34 | INFO | train_inner | epoch 007:    484 / 986 loss=5.116, nll_loss=3.783, intra_distillation_loss=0, ppl=13.76, wps=33651.1, ups=9.28, wpb=3624.9, bsz=141.8, num_updates=6400, lr=0.000395285, gnorm=1.132, train_wall=10, gb_free=76.8, wall=948
2023-05-09 04:27:44 | INFO | train_inner | epoch 007:    584 / 986 loss=5.109, nll_loss=3.776, intra_distillation_loss=0, ppl=13.7, wps=33417.8, ups=9.29, wpb=3595.9, bsz=146.4, num_updates=6500, lr=0.000392232, gnorm=1.178, train_wall=10, gb_free=77.1, wall=959
2023-05-09 04:27:55 | INFO | train_inner | epoch 007:    684 / 986 loss=5.11, nll_loss=3.776, intra_distillation_loss=0, ppl=13.7, wps=33800, ups=9.41, wpb=3593.3, bsz=132, num_updates=6600, lr=0.000389249, gnorm=1.161, train_wall=10, gb_free=77, wall=970
2023-05-09 04:28:06 | INFO | train_inner | epoch 007:    784 / 986 loss=4.947, nll_loss=3.591, intra_distillation_loss=0, ppl=12.05, wps=33901.1, ups=9.1, wpb=3725.6, bsz=162, num_updates=6700, lr=0.000386334, gnorm=1.073, train_wall=10, gb_free=76.9, wall=981
2023-05-09 04:28:17 | INFO | train_inner | epoch 007:    884 / 986 loss=5.003, nll_loss=3.655, intra_distillation_loss=0, ppl=12.6, wps=34223.8, ups=9.11, wpb=3756.3, bsz=143.9, num_updates=6800, lr=0.000383482, gnorm=1.065, train_wall=10, gb_free=76.7, wall=992
2023-05-09 04:28:28 | INFO | train_inner | epoch 007:    984 / 986 loss=4.937, nll_loss=3.58, intra_distillation_loss=0, ppl=11.96, wps=34405, ups=9.31, wpb=3693.5, bsz=156.9, num_updates=6900, lr=0.000380693, gnorm=1.113, train_wall=10, gb_free=76.9, wall=1002
2023-05-09 04:28:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:28:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:29:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.81 | nll_loss 3.302 | intra_distillation_loss 0 | ppl 9.86 | bleu 25.14 | wps 4393.5 | wpb 2821.2 | bsz 113.1 | num_updates 6902 | best_bleu 25.14
2023-05-09 04:29:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6902 updates
2023-05-09 04:29:05 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:29:06 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:29:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 7 @ 6902 updates, score 25.14) (writing took 1.8598453750601038 seconds)
2023-05-09 04:29:07 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-09 04:29:07 | INFO | train | epoch 007 | loss 5.066 | nll_loss 3.726 | intra_distillation_loss 0 | ppl 13.23 | wps 24753.9 | ups 6.75 | wpb 3668.2 | bsz 146.4 | num_updates 6902 | lr 0.000380638 | gnorm 1.123 | train_wall 96 | gb_free 76.8 | wall 1042
2023-05-09 04:29:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:29:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:29:07 | INFO | fairseq.trainer | begin training epoch 8
2023-05-09 04:29:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:29:18 | INFO | train_inner | epoch 008:     98 / 986 loss=4.908, nll_loss=3.546, intra_distillation_loss=0, ppl=11.68, wps=7326.6, ups=1.99, wpb=3674.5, bsz=132.6, num_updates=7000, lr=0.000377964, gnorm=1.087, train_wall=10, gb_free=76.9, wall=1052
2023-05-09 04:29:29 | INFO | train_inner | epoch 008:    198 / 986 loss=4.756, nll_loss=3.374, intra_distillation_loss=0, ppl=10.37, wps=33853.1, ups=9.22, wpb=3670.3, bsz=162.3, num_updates=7100, lr=0.000375293, gnorm=1.092, train_wall=10, gb_free=77.2, wall=1063
2023-05-09 04:29:40 | INFO | train_inner | epoch 008:    298 / 986 loss=4.787, nll_loss=3.41, intra_distillation_loss=0, ppl=10.63, wps=33115.7, ups=9.12, wpb=3632.3, bsz=163, num_updates=7200, lr=0.000372678, gnorm=1.092, train_wall=10, gb_free=76.7, wall=1074
2023-05-09 04:29:50 | INFO | train_inner | epoch 008:    398 / 986 loss=4.881, nll_loss=3.514, intra_distillation_loss=0, ppl=11.42, wps=33933.1, ups=9.32, wpb=3639.3, bsz=135.5, num_updates=7300, lr=0.000370117, gnorm=1.088, train_wall=10, gb_free=77, wall=1085
2023-05-09 04:30:01 | INFO | train_inner | epoch 008:    498 / 986 loss=4.868, nll_loss=3.5, intra_distillation_loss=0, ppl=11.31, wps=34171.9, ups=9.22, wpb=3704.4, bsz=148.3, num_updates=7400, lr=0.000367607, gnorm=1.123, train_wall=10, gb_free=76.8, wall=1096
2023-05-09 04:30:12 | INFO | train_inner | epoch 008:    598 / 986 loss=4.742, nll_loss=3.356, intra_distillation_loss=0, ppl=10.24, wps=34300.5, ups=9.07, wpb=3782.9, bsz=160.6, num_updates=7500, lr=0.000365148, gnorm=1.023, train_wall=10, gb_free=76.8, wall=1107
2023-05-09 04:30:23 | INFO | train_inner | epoch 008:    698 / 986 loss=4.896, nll_loss=3.53, intra_distillation_loss=0, ppl=11.56, wps=34109.1, ups=9.23, wpb=3696, bsz=135.1, num_updates=7600, lr=0.000362738, gnorm=1.11, train_wall=10, gb_free=77, wall=1118
2023-05-09 04:30:34 | INFO | train_inner | epoch 008:    798 / 986 loss=4.836, nll_loss=3.464, intra_distillation_loss=0, ppl=11.04, wps=33433.5, ups=9.14, wpb=3658, bsz=144, num_updates=7700, lr=0.000360375, gnorm=1.135, train_wall=10, gb_free=76.7, wall=1129
2023-05-09 04:30:45 | INFO | train_inner | epoch 008:    898 / 986 loss=4.852, nll_loss=3.481, intra_distillation_loss=0, ppl=11.17, wps=33688.4, ups=9.3, wpb=3623.3, bsz=138, num_updates=7800, lr=0.000358057, gnorm=1.124, train_wall=10, gb_free=76.8, wall=1139
2023-05-09 04:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:30:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:31:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.645 | nll_loss 3.127 | intra_distillation_loss 0 | ppl 8.73 | bleu 26.71 | wps 4239.5 | wpb 2821.2 | bsz 113.1 | num_updates 7888 | best_bleu 26.71
2023-05-09 04:31:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7888 updates
2023-05-09 04:31:33 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:31:34 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 8 @ 7888 updates, score 26.71) (writing took 1.7829321400495246 seconds)
2023-05-09 04:31:35 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-09 04:31:35 | INFO | train | epoch 008 | loss 4.833 | nll_loss 3.46 | intra_distillation_loss 0 | ppl 11 | wps 24536.7 | ups 6.69 | wpb 3668.2 | bsz 146.4 | num_updates 7888 | lr 0.000356055 | gnorm 1.1 | train_wall 96 | gb_free 76.9 | wall 1189
2023-05-09 04:31:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:31:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:31:35 | INFO | fairseq.trainer | begin training epoch 9
2023-05-09 04:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:31:36 | INFO | train_inner | epoch 009:     12 / 986 loss=4.779, nll_loss=3.398, intra_distillation_loss=0, ppl=10.54, wps=7065.4, ups=1.95, wpb=3624.2, bsz=141.9, num_updates=7900, lr=0.000355784, gnorm=1.112, train_wall=10, gb_free=76.9, wall=1191
2023-05-09 04:31:47 | INFO | train_inner | epoch 009:    112 / 986 loss=4.655, nll_loss=3.258, intra_distillation_loss=0, ppl=9.57, wps=33544.2, ups=9.24, wpb=3628.5, bsz=146.2, num_updates=8000, lr=0.000353553, gnorm=1.104, train_wall=10, gb_free=76.8, wall=1201
2023-05-09 04:32:00 | INFO | train_inner | epoch 009:    212 / 986 loss=4.662, nll_loss=3.271, intra_distillation_loss=0, ppl=9.66, wps=28662.6, ups=7.86, wpb=3647.5, bsz=142.5, num_updates=8100, lr=0.000351364, gnorm=0, train_wall=12, gb_free=76.8, wall=1214
2023-05-09 04:32:12 | INFO | train_inner | epoch 009:    312 / 986 loss=4.573, nll_loss=3.171, intra_distillation_loss=0, ppl=9, wps=29145.1, ups=7.79, wpb=3741.8, bsz=152.2, num_updates=8200, lr=0.000349215, gnorm=0, train_wall=12, gb_free=76.7, wall=1227
2023-05-09 04:32:25 | INFO | train_inner | epoch 009:    412 / 986 loss=4.569, nll_loss=3.165, intra_distillation_loss=0, ppl=8.97, wps=28933.4, ups=7.84, wpb=3689, bsz=158.1, num_updates=8300, lr=0.000347105, gnorm=0, train_wall=12, gb_free=76.8, wall=1240
2023-05-09 04:32:38 | INFO | train_inner | epoch 009:    512 / 986 loss=4.684, nll_loss=3.295, intra_distillation_loss=0, ppl=9.82, wps=28751.5, ups=7.93, wpb=3626.4, bsz=131.4, num_updates=8400, lr=0.000345033, gnorm=0, train_wall=12, gb_free=76.8, wall=1252
2023-05-09 04:32:51 | INFO | train_inner | epoch 009:    612 / 986 loss=4.554, nll_loss=3.146, intra_distillation_loss=0, ppl=8.85, wps=28698.8, ups=7.81, wpb=3675.1, bsz=157.5, num_updates=8500, lr=0.000342997, gnorm=0, train_wall=12, gb_free=76.9, wall=1265
2023-05-09 04:33:03 | INFO | train_inner | epoch 009:    712 / 986 loss=4.596, nll_loss=3.192, intra_distillation_loss=0, ppl=9.14, wps=28754, ups=7.92, wpb=3630.2, bsz=140.5, num_updates=8600, lr=0.000340997, gnorm=0, train_wall=12, gb_free=76.9, wall=1278
2023-05-09 04:33:16 | INFO | train_inner | epoch 009:    812 / 986 loss=4.623, nll_loss=3.222, intra_distillation_loss=0, ppl=9.33, wps=28903.9, ups=7.92, wpb=3647.5, bsz=138.9, num_updates=8700, lr=0.000339032, gnorm=0, train_wall=12, gb_free=77, wall=1290
2023-05-09 04:33:29 | INFO | train_inner | epoch 009:    912 / 986 loss=4.591, nll_loss=3.186, intra_distillation_loss=0, ppl=9.1, wps=28942.2, ups=7.79, wpb=3714.8, bsz=141.5, num_updates=8800, lr=0.0003371, gnorm=0, train_wall=12, gb_free=76.7, wall=1303
2023-05-09 04:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:33:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:34:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.478 | nll_loss 2.915 | intra_distillation_loss 0 | ppl 7.54 | bleu 28.85 | wps 4125.3 | wpb 2821.2 | bsz 113.1 | num_updates 8874 | best_bleu 28.85
2023-05-09 04:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 8874 updates
2023-05-09 04:34:18 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 9 @ 8874 updates, score 28.85) (writing took 1.8162768159527332 seconds)
2023-05-09 04:34:20 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-09 04:34:20 | INFO | train | epoch 009 | loss 4.605 | nll_loss 3.204 | intra_distillation_loss 0 | ppl 9.21 | wps 21910.8 | ups 5.97 | wpb 3668.2 | bsz 146.4 | num_updates 8874 | lr 0.000335691 | gnorm 0.124 | train_wall 113 | gb_free 76.8 | wall 1354
2023-05-09 04:34:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:34:20 | INFO | fairseq.trainer | begin training epoch 10
2023-05-09 04:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:34:23 | INFO | train_inner | epoch 010:     26 / 986 loss=4.477, nll_loss=3.057, intra_distillation_loss=0, ppl=8.32, wps=6777.6, ups=1.84, wpb=3690.9, bsz=160.4, num_updates=8900, lr=0.000335201, gnorm=0, train_wall=12, gb_free=76.8, wall=1358
2023-05-09 04:34:36 | INFO | train_inner | epoch 010:    126 / 986 loss=4.448, nll_loss=3.023, intra_distillation_loss=0, ppl=8.13, wps=29072.6, ups=7.9, wpb=3679.9, bsz=134.2, num_updates=9000, lr=0.000333333, gnorm=0, train_wall=12, gb_free=76.7, wall=1370
2023-05-09 04:34:49 | INFO | train_inner | epoch 010:    226 / 986 loss=4.48, nll_loss=3.058, intra_distillation_loss=0, ppl=8.33, wps=28651.8, ups=7.8, wpb=3673.5, bsz=148.6, num_updates=9100, lr=0.000331497, gnorm=0, train_wall=12, gb_free=76.8, wall=1383
2023-05-09 04:35:01 | INFO | train_inner | epoch 010:    326 / 986 loss=4.534, nll_loss=3.119, intra_distillation_loss=0, ppl=8.69, wps=28364.2, ups=7.86, wpb=3607, bsz=135.5, num_updates=9200, lr=0.00032969, gnorm=0, train_wall=12, gb_free=76.9, wall=1396
2023-05-09 04:35:14 | INFO | train_inner | epoch 010:    426 / 986 loss=4.496, nll_loss=3.076, intra_distillation_loss=0, ppl=8.43, wps=28624.1, ups=7.91, wpb=3620.6, bsz=140, num_updates=9300, lr=0.000327913, gnorm=0, train_wall=12, gb_free=76.9, wall=1409
2023-05-09 04:35:27 | INFO | train_inner | epoch 010:    526 / 986 loss=4.507, nll_loss=3.088, intra_distillation_loss=0, ppl=8.5, wps=28817, ups=7.8, wpb=3693.6, bsz=141.8, num_updates=9400, lr=0.000326164, gnorm=0, train_wall=12, gb_free=77, wall=1421
2023-05-09 04:35:40 | INFO | train_inner | epoch 010:    626 / 986 loss=4.488, nll_loss=3.066, intra_distillation_loss=0, ppl=8.38, wps=28591.2, ups=7.86, wpb=3636.4, bsz=153.8, num_updates=9500, lr=0.000324443, gnorm=0, train_wall=12, gb_free=77, wall=1434
2023-05-09 04:35:52 | INFO | train_inner | epoch 010:    726 / 986 loss=4.504, nll_loss=3.085, intra_distillation_loss=0, ppl=8.49, wps=28672.5, ups=7.84, wpb=3655.8, bsz=149.5, num_updates=9600, lr=0.000322749, gnorm=0, train_wall=12, gb_free=76.8, wall=1447
2023-05-09 04:36:05 | INFO | train_inner | epoch 010:    826 / 986 loss=4.474, nll_loss=3.051, intra_distillation_loss=0, ppl=8.29, wps=29166.7, ups=7.9, wpb=3693.5, bsz=145.9, num_updates=9700, lr=0.000321081, gnorm=0, train_wall=12, gb_free=76.9, wall=1460
2023-05-09 04:36:18 | INFO | train_inner | epoch 010:    926 / 986 loss=4.446, nll_loss=3.02, intra_distillation_loss=0, ppl=8.11, wps=29033.5, ups=7.82, wpb=3713.7, bsz=159.8, num_updates=9800, lr=0.000319438, gnorm=0, train_wall=12, gb_free=76.9, wall=1472
2023-05-09 04:36:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:36:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:37:04 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.422 | nll_loss 2.85 | intra_distillation_loss 0 | ppl 7.21 | bleu 29.7 | wps 4218.4 | wpb 2821.2 | bsz 113.1 | num_updates 9860 | best_bleu 29.7
2023-05-09 04:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 9860 updates
2023-05-09 04:37:04 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:37:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 10 @ 9860 updates, score 29.7) (writing took 1.7984860009746626 seconds)
2023-05-09 04:37:06 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-09 04:37:06 | INFO | train | epoch 010 | loss 4.482 | nll_loss 3.06 | intra_distillation_loss 0 | ppl 8.34 | wps 21731.5 | ups 5.92 | wpb 3668.2 | bsz 146.4 | num_updates 9860 | lr 0.000318465 | gnorm 0 | train_wall 115 | gb_free 76.7 | wall 1521
2023-05-09 04:37:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:37:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:37:06 | INFO | fairseq.trainer | begin training epoch 11
2023-05-09 04:37:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:37:11 | INFO | train_inner | epoch 011:     40 / 986 loss=4.436, nll_loss=3.008, intra_distillation_loss=0, ppl=8.04, wps=6841.4, ups=1.87, wpb=3667.4, bsz=149.7, num_updates=9900, lr=0.000317821, gnorm=0, train_wall=12, gb_free=77.7, wall=1526
2023-05-09 04:37:24 | INFO | train_inner | epoch 011:    140 / 986 loss=4.408, nll_loss=2.975, intra_distillation_loss=0, ppl=7.86, wps=29169.8, ups=7.78, wpb=3750.4, bsz=140.6, num_updates=10000, lr=0.000316228, gnorm=0, train_wall=12, gb_free=76.8, wall=1539
2023-05-09 04:37:37 | INFO | train_inner | epoch 011:    240 / 986 loss=4.397, nll_loss=2.963, intra_distillation_loss=0, ppl=7.8, wps=28442.4, ups=7.79, wpb=3653.5, bsz=156.3, num_updates=10100, lr=0.000314658, gnorm=0, train_wall=12, gb_free=76.9, wall=1552
2023-05-09 04:37:50 | INFO | train_inner | epoch 011:    340 / 986 loss=4.396, nll_loss=2.96, intra_distillation_loss=0, ppl=7.78, wps=29105.1, ups=7.89, wpb=3688.5, bsz=139.6, num_updates=10200, lr=0.000313112, gnorm=0, train_wall=12, gb_free=76.9, wall=1564
2023-05-09 04:38:02 | INFO | train_inner | epoch 011:    440 / 986 loss=4.431, nll_loss=3.001, intra_distillation_loss=0, ppl=8, wps=28498.1, ups=7.94, wpb=3590.3, bsz=147.9, num_updates=10300, lr=0.000311588, gnorm=0, train_wall=12, gb_free=76.9, wall=1577
2023-05-09 04:38:15 | INFO | train_inner | epoch 011:    540 / 986 loss=4.349, nll_loss=2.908, intra_distillation_loss=0, ppl=7.51, wps=28649, ups=7.81, wpb=3667.9, bsz=167, num_updates=10400, lr=0.000310087, gnorm=0, train_wall=12, gb_free=76.9, wall=1590
2023-05-09 04:38:28 | INFO | train_inner | epoch 011:    640 / 986 loss=4.377, nll_loss=2.939, intra_distillation_loss=0, ppl=7.67, wps=29196.1, ups=7.85, wpb=3718.8, bsz=152.1, num_updates=10500, lr=0.000308607, gnorm=0, train_wall=12, gb_free=76.8, wall=1602
2023-05-09 04:38:41 | INFO | train_inner | epoch 011:    740 / 986 loss=4.46, nll_loss=3.033, intra_distillation_loss=0, ppl=8.19, wps=28986.8, ups=7.84, wpb=3696.7, bsz=135.7, num_updates=10600, lr=0.000307148, gnorm=0, train_wall=12, gb_free=76.9, wall=1615
2023-05-09 04:38:53 | INFO | train_inner | epoch 011:    840 / 986 loss=4.451, nll_loss=3.022, intra_distillation_loss=0, ppl=8.12, wps=29187.5, ups=7.84, wpb=3722.1, bsz=134.9, num_updates=10700, lr=0.000305709, gnorm=0, train_wall=12, gb_free=76.9, wall=1628
2023-05-09 04:39:06 | INFO | train_inner | epoch 011:    940 / 986 loss=4.457, nll_loss=3.031, intra_distillation_loss=0, ppl=8.17, wps=28126.8, ups=7.87, wpb=3574.4, bsz=143.9, num_updates=10800, lr=0.00030429, gnorm=0, train_wall=12, gb_free=76.9, wall=1641
2023-05-09 04:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:39:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:39:50 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.383 | nll_loss 2.802 | intra_distillation_loss 0 | ppl 6.98 | bleu 30.19 | wps 4268.7 | wpb 2821.2 | bsz 113.1 | num_updates 10846 | best_bleu 30.19
2023-05-09 04:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 10846 updates
2023-05-09 04:39:50 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:39:51 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:39:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 11 @ 10846 updates, score 30.19) (writing took 1.7971462820423767 seconds)
2023-05-09 04:39:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-09 04:39:52 | INFO | train | epoch 011 | loss 4.413 | nll_loss 2.98 | intra_distillation_loss 0 | ppl 7.89 | wps 21784.1 | ups 5.94 | wpb 3668.2 | bsz 146.4 | num_updates 10846 | lr 0.000303644 | gnorm 0 | train_wall 115 | gb_free 77 | wall 1687
2023-05-09 04:39:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:39:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:39:52 | INFO | fairseq.trainer | begin training epoch 12
2023-05-09 04:39:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:39:59 | INFO | train_inner | epoch 012:     54 / 986 loss=4.409, nll_loss=2.975, intra_distillation_loss=0, ppl=7.86, wps=6891.2, ups=1.88, wpb=3659.2, bsz=138, num_updates=10900, lr=0.000302891, gnorm=0, train_wall=12, gb_free=76.8, wall=1694
2023-05-09 04:40:12 | INFO | train_inner | epoch 012:    154 / 986 loss=4.313, nll_loss=2.865, intra_distillation_loss=0, ppl=7.28, wps=29082.5, ups=7.79, wpb=3735.2, bsz=149.8, num_updates=11000, lr=0.000301511, gnorm=0, train_wall=12, gb_free=76.8, wall=1707
2023-05-09 04:40:25 | INFO | train_inner | epoch 012:    254 / 986 loss=4.298, nll_loss=2.848, intra_distillation_loss=0, ppl=7.2, wps=28852.2, ups=7.86, wpb=3669.2, bsz=155.3, num_updates=11100, lr=0.00030015, gnorm=0, train_wall=12, gb_free=76.9, wall=1719
2023-05-09 04:40:37 | INFO | train_inner | epoch 012:    354 / 986 loss=4.335, nll_loss=2.89, intra_distillation_loss=0, ppl=7.42, wps=28893.6, ups=7.84, wpb=3685.4, bsz=148.5, num_updates=11200, lr=0.000298807, gnorm=0, train_wall=12, gb_free=76.9, wall=1732
2023-05-09 04:40:50 | INFO | train_inner | epoch 012:    454 / 986 loss=4.319, nll_loss=2.873, intra_distillation_loss=0, ppl=7.32, wps=28535.7, ups=7.82, wpb=3649.2, bsz=156.3, num_updates=11300, lr=0.000297482, gnorm=0, train_wall=12, gb_free=76.7, wall=1745
2023-05-09 04:41:03 | INFO | train_inner | epoch 012:    554 / 986 loss=4.394, nll_loss=2.957, intra_distillation_loss=0, ppl=7.77, wps=28116.5, ups=7.84, wpb=3584.1, bsz=141.5, num_updates=11400, lr=0.000296174, gnorm=0, train_wall=12, gb_free=76.8, wall=1758
2023-05-09 04:41:16 | INFO | train_inner | epoch 012:    654 / 986 loss=4.346, nll_loss=2.903, intra_distillation_loss=0, ppl=7.48, wps=28737.2, ups=7.82, wpb=3675.1, bsz=151.7, num_updates=11500, lr=0.000294884, gnorm=0, train_wall=12, gb_free=77.3, wall=1770
2023-05-09 04:41:29 | INFO | train_inner | epoch 012:    754 / 986 loss=4.327, nll_loss=2.881, intra_distillation_loss=0, ppl=7.37, wps=29060.6, ups=7.79, wpb=3731.2, bsz=152.7, num_updates=11600, lr=0.00029361, gnorm=0, train_wall=12, gb_free=76.8, wall=1783
2023-05-09 04:41:41 | INFO | train_inner | epoch 012:    854 / 986 loss=4.434, nll_loss=3.002, intra_distillation_loss=0, ppl=8.01, wps=28478.3, ups=7.98, wpb=3567, bsz=133.5, num_updates=11700, lr=0.000292353, gnorm=0, train_wall=11, gb_free=76.8, wall=1796
2023-05-09 04:41:54 | INFO | train_inner | epoch 012:    954 / 986 loss=4.388, nll_loss=2.95, intra_distillation_loss=0, ppl=7.73, wps=29017.2, ups=7.86, wpb=3694, bsz=135.7, num_updates=11800, lr=0.000291111, gnorm=0, train_wall=12, gb_free=76.9, wall=1809
2023-05-09 04:41:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:41:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:42:36 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.349 | nll_loss 2.762 | intra_distillation_loss 0 | ppl 6.78 | bleu 30.5 | wps 4306.6 | wpb 2821.2 | bsz 113.1 | num_updates 11832 | best_bleu 30.5
2023-05-09 04:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 11832 updates
2023-05-09 04:42:36 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 12 @ 11832 updates, score 30.5) (writing took 1.896191410953179 seconds)
2023-05-09 04:42:38 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-09 04:42:38 | INFO | train | epoch 012 | loss 4.356 | nll_loss 2.914 | intra_distillation_loss 0 | ppl 7.54 | wps 21819.1 | ups 5.95 | wpb 3668.2 | bsz 146.4 | num_updates 11832 | lr 0.000290717 | gnorm 0 | train_wall 115 | gb_free 76.9 | wall 1853
2023-05-09 04:42:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:42:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:42:38 | INFO | fairseq.trainer | begin training epoch 13
2023-05-09 04:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:42:47 | INFO | train_inner | epoch 013:     68 / 986 loss=4.349, nll_loss=2.907, intra_distillation_loss=0, ppl=7.5, wps=6818.6, ups=1.9, wpb=3592.9, bsz=138.3, num_updates=11900, lr=0.000289886, gnorm=0, train_wall=12, gb_free=76.9, wall=1861
2023-05-09 04:42:59 | INFO | train_inner | epoch 013:    168 / 986 loss=4.237, nll_loss=2.78, intra_distillation_loss=0, ppl=6.87, wps=28622.9, ups=7.79, wpb=3673.5, bsz=161.8, num_updates=12000, lr=0.000288675, gnorm=0, train_wall=12, gb_free=77, wall=1874
2023-05-09 04:43:12 | INFO | train_inner | epoch 013:    268 / 986 loss=4.303, nll_loss=2.853, intra_distillation_loss=0, ppl=7.22, wps=28870, ups=7.92, wpb=3644.1, bsz=145.1, num_updates=12100, lr=0.00028748, gnorm=0, train_wall=12, gb_free=76.8, wall=1887
2023-05-09 04:43:25 | INFO | train_inner | epoch 013:    368 / 986 loss=4.273, nll_loss=2.82, intra_distillation_loss=0, ppl=7.06, wps=28766.6, ups=7.81, wpb=3681.7, bsz=151.2, num_updates=12200, lr=0.000286299, gnorm=0, train_wall=12, gb_free=76.9, wall=1899
2023-05-09 04:43:38 | INFO | train_inner | epoch 013:    468 / 986 loss=4.308, nll_loss=2.859, intra_distillation_loss=0, ppl=7.26, wps=28631.2, ups=7.84, wpb=3649.7, bsz=151.8, num_updates=12300, lr=0.000285133, gnorm=0, train_wall=12, gb_free=76.8, wall=1912
2023-05-09 04:43:50 | INFO | train_inner | epoch 013:    568 / 986 loss=4.319, nll_loss=2.871, intra_distillation_loss=0, ppl=7.32, wps=28886.3, ups=7.85, wpb=3680.3, bsz=143.6, num_updates=12400, lr=0.000283981, gnorm=0, train_wall=12, gb_free=76.9, wall=1925
2023-05-09 04:44:03 | INFO | train_inner | epoch 013:    668 / 986 loss=4.328, nll_loss=2.881, intra_distillation_loss=0, ppl=7.37, wps=28745.6, ups=7.77, wpb=3701, bsz=143.6, num_updates=12500, lr=0.000282843, gnorm=0, train_wall=12, gb_free=76.9, wall=1938
2023-05-09 04:44:16 | INFO | train_inner | epoch 013:    768 / 986 loss=4.268, nll_loss=2.813, intra_distillation_loss=0, ppl=7.03, wps=29182, ups=7.86, wpb=3712.9, bsz=148, num_updates=12600, lr=0.000281718, gnorm=0, train_wall=12, gb_free=76.9, wall=1951
2023-05-09 04:44:29 | INFO | train_inner | epoch 013:    868 / 986 loss=4.37, nll_loss=2.93, intra_distillation_loss=0, ppl=7.62, wps=28641.3, ups=7.85, wpb=3647.4, bsz=137.9, num_updates=12700, lr=0.000280607, gnorm=0, train_wall=12, gb_free=77, wall=1963
2023-05-09 04:44:41 | INFO | train_inner | epoch 013:    968 / 986 loss=4.341, nll_loss=2.897, intra_distillation_loss=0, ppl=7.45, wps=28689.9, ups=7.84, wpb=3659.3, bsz=146.5, num_updates=12800, lr=0.000279508, gnorm=0, train_wall=12, gb_free=76.8, wall=1976
2023-05-09 04:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:44:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:45:22 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.309 | nll_loss 2.723 | intra_distillation_loss 0 | ppl 6.6 | bleu 31.27 | wps 4299.3 | wpb 2821.2 | bsz 113.1 | num_updates 12818 | best_bleu 31.27
2023-05-09 04:45:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 12818 updates
2023-05-09 04:45:22 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 13 @ 12818 updates, score 31.27) (writing took 1.7643963270820677 seconds)
2023-05-09 04:45:24 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-09 04:45:24 | INFO | train | epoch 013 | loss 4.305 | nll_loss 2.856 | intra_distillation_loss 0 | ppl 7.24 | wps 21818.5 | ups 5.95 | wpb 3668.2 | bsz 146.4 | num_updates 12818 | lr 0.000279312 | gnorm 0 | train_wall 115 | gb_free 76.8 | wall 2018
2023-05-09 04:45:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:45:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:45:24 | INFO | fairseq.trainer | begin training epoch 14
2023-05-09 04:45:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:45:34 | INFO | train_inner | epoch 014:     82 / 986 loss=4.243, nll_loss=2.785, intra_distillation_loss=0, ppl=6.89, wps=7054.9, ups=1.89, wpb=3728.8, bsz=143.9, num_updates=12900, lr=0.000278423, gnorm=0, train_wall=12, gb_free=76.9, wall=2029
2023-05-09 04:45:47 | INFO | train_inner | epoch 014:    182 / 986 loss=4.266, nll_loss=2.811, intra_distillation_loss=0, ppl=7.02, wps=28558.6, ups=7.84, wpb=3641.3, bsz=139.4, num_updates=13000, lr=0.00027735, gnorm=0, train_wall=12, gb_free=76.8, wall=2042
2023-05-09 04:46:00 | INFO | train_inner | epoch 014:    282 / 986 loss=4.269, nll_loss=2.814, intra_distillation_loss=0, ppl=7.03, wps=29062, ups=7.83, wpb=3713, bsz=145.3, num_updates=13100, lr=0.000276289, gnorm=0, train_wall=12, gb_free=76.8, wall=2054
2023-05-09 04:46:12 | INFO | train_inner | epoch 014:    382 / 986 loss=4.287, nll_loss=2.834, intra_distillation_loss=0, ppl=7.13, wps=28876.3, ups=7.9, wpb=3654.8, bsz=127.5, num_updates=13200, lr=0.000275241, gnorm=0, train_wall=12, gb_free=77, wall=2067
2023-05-09 04:46:25 | INFO | train_inner | epoch 014:    482 / 986 loss=4.24, nll_loss=2.782, intra_distillation_loss=0, ppl=6.88, wps=28752.5, ups=7.81, wpb=3682.8, bsz=151.8, num_updates=13300, lr=0.000274204, gnorm=0, train_wall=12, gb_free=76.9, wall=2080
2023-05-09 04:46:38 | INFO | train_inner | epoch 014:    582 / 986 loss=4.245, nll_loss=2.788, intra_distillation_loss=0, ppl=6.91, wps=28423.3, ups=7.86, wpb=3617.8, bsz=159.3, num_updates=13400, lr=0.000273179, gnorm=0, train_wall=12, gb_free=76.9, wall=2093
2023-05-09 04:46:51 | INFO | train_inner | epoch 014:    682 / 986 loss=4.249, nll_loss=2.792, intra_distillation_loss=0, ppl=6.92, wps=28999, ups=7.86, wpb=3691.1, bsz=142.9, num_updates=13500, lr=0.000272166, gnorm=0, train_wall=12, gb_free=77.1, wall=2105
2023-05-09 04:47:03 | INFO | train_inner | epoch 014:    782 / 986 loss=4.266, nll_loss=2.812, intra_distillation_loss=0, ppl=7.02, wps=28875.8, ups=7.95, wpb=3631.9, bsz=146.8, num_updates=13600, lr=0.000271163, gnorm=0, train_wall=12, gb_free=77, wall=2118
2023-05-09 04:47:16 | INFO | train_inner | epoch 014:    882 / 986 loss=4.3, nll_loss=2.851, intra_distillation_loss=0, ppl=7.21, wps=28531.2, ups=7.74, wpb=3686.3, bsz=148.9, num_updates=13700, lr=0.000270172, gnorm=0, train_wall=12, gb_free=76.9, wall=2131
2023-05-09 04:47:29 | INFO | train_inner | epoch 014:    982 / 986 loss=4.252, nll_loss=2.797, intra_distillation_loss=0, ppl=6.95, wps=28833.4, ups=7.85, wpb=3671.9, bsz=159.6, num_updates=13800, lr=0.000269191, gnorm=0, train_wall=12, gb_free=76.7, wall=2144
2023-05-09 04:47:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:47:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:48:08 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.284 | nll_loss 2.693 | intra_distillation_loss 0 | ppl 6.47 | bleu 31.34 | wps 4256.5 | wpb 2821.2 | bsz 113.1 | num_updates 13804 | best_bleu 31.34
2023-05-09 04:48:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 13804 updates
2023-05-09 04:48:08 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 14 @ 13804 updates, score 31.34) (writing took 1.8525225180201232 seconds)
2023-05-09 04:48:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-09 04:48:10 | INFO | train | epoch 014 | loss 4.262 | nll_loss 2.807 | intra_distillation_loss 0 | ppl 7 | wps 21753.8 | ups 5.93 | wpb 3668.2 | bsz 146.4 | num_updates 13804 | lr 0.000269152 | gnorm 0 | train_wall 115 | gb_free 77 | wall 2185
2023-05-09 04:48:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:48:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:48:10 | INFO | fairseq.trainer | begin training epoch 15
2023-05-09 04:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:48:22 | INFO | train_inner | epoch 015:     96 / 986 loss=4.227, nll_loss=2.767, intra_distillation_loss=0, ppl=6.81, wps=6930.4, ups=1.87, wpb=3705.7, bsz=142.1, num_updates=13900, lr=0.000268221, gnorm=0, train_wall=12, gb_free=76.9, wall=2197
2023-05-09 04:48:35 | INFO | train_inner | epoch 015:    196 / 986 loss=4.231, nll_loss=2.77, intra_distillation_loss=0, ppl=6.82, wps=28817.8, ups=7.77, wpb=3707.2, bsz=138.5, num_updates=14000, lr=0.000267261, gnorm=0, train_wall=12, gb_free=76.7, wall=2210
2023-05-09 04:48:48 | INFO | train_inner | epoch 015:    296 / 986 loss=4.2, nll_loss=2.737, intra_distillation_loss=0, ppl=6.66, wps=28704.2, ups=7.84, wpb=3659.3, bsz=156.2, num_updates=14100, lr=0.000266312, gnorm=0, train_wall=12, gb_free=77, wall=2223
2023-05-09 04:49:01 | INFO | train_inner | epoch 015:    396 / 986 loss=4.23, nll_loss=2.769, intra_distillation_loss=0, ppl=6.82, wps=29197.4, ups=7.95, wpb=3674.8, bsz=134.6, num_updates=14200, lr=0.000265372, gnorm=0, train_wall=12, gb_free=76.8, wall=2235
2023-05-09 04:49:13 | INFO | train_inner | epoch 015:    496 / 986 loss=4.176, nll_loss=2.709, intra_distillation_loss=0, ppl=6.54, wps=28542.2, ups=7.83, wpb=3643, bsz=161.6, num_updates=14300, lr=0.000264443, gnorm=0, train_wall=12, gb_free=77.1, wall=2248
2023-05-09 04:49:26 | INFO | train_inner | epoch 015:    596 / 986 loss=4.245, nll_loss=2.786, intra_distillation_loss=0, ppl=6.9, wps=28886.9, ups=7.84, wpb=3683.3, bsz=139.8, num_updates=14400, lr=0.000263523, gnorm=0, train_wall=12, gb_free=76.9, wall=2261
2023-05-09 04:49:39 | INFO | train_inner | epoch 015:    696 / 986 loss=4.244, nll_loss=2.785, intra_distillation_loss=0, ppl=6.89, wps=28561.7, ups=7.89, wpb=3618.3, bsz=139.9, num_updates=14500, lr=0.000262613, gnorm=0, train_wall=12, gb_free=77, wall=2273
2023-05-09 04:49:52 | INFO | train_inner | epoch 015:    796 / 986 loss=4.223, nll_loss=2.763, intra_distillation_loss=0, ppl=6.79, wps=28567, ups=7.83, wpb=3646.2, bsz=151.6, num_updates=14600, lr=0.000261712, gnorm=0, train_wall=12, gb_free=77, wall=2286
2023-05-09 04:50:04 | INFO | train_inner | epoch 015:    896 / 986 loss=4.221, nll_loss=2.76, intra_distillation_loss=0, ppl=6.77, wps=28665.3, ups=7.81, wpb=3668.1, bsz=153.5, num_updates=14700, lr=0.00026082, gnorm=0, train_wall=12, gb_free=76.8, wall=2299
2023-05-09 04:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:50:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:50:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.269 | nll_loss 2.669 | intra_distillation_loss 0 | ppl 6.36 | bleu 31.87 | wps 4193.2 | wpb 2821.2 | bsz 113.1 | num_updates 14790 | best_bleu 31.87
2023-05-09 04:50:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 14790 updates
2023-05-09 04:50:55 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:50:56 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 15 @ 14790 updates, score 31.87) (writing took 1.8683621620293707 seconds)
2023-05-09 04:50:57 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-09 04:50:57 | INFO | train | epoch 015 | loss 4.224 | nll_loss 2.763 | intra_distillation_loss 0 | ppl 6.79 | wps 21674.6 | ups 5.91 | wpb 3668.2 | bsz 146.4 | num_updates 14790 | lr 0.000260025 | gnorm 0 | train_wall 115 | gb_free 76.9 | wall 2351
2023-05-09 04:50:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:50:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:50:57 | INFO | fairseq.trainer | begin training epoch 16
2023-05-09 04:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:50:58 | INFO | train_inner | epoch 016:     10 / 986 loss=4.247, nll_loss=2.79, intra_distillation_loss=0, ppl=6.91, wps=6830, ups=1.86, wpb=3679.8, bsz=141.8, num_updates=14800, lr=0.000259938, gnorm=0, train_wall=12, gb_free=76.8, wall=2353
2023-05-09 04:51:11 | INFO | train_inner | epoch 016:    110 / 986 loss=4.154, nll_loss=2.683, intra_distillation_loss=0, ppl=6.42, wps=28840.6, ups=7.82, wpb=3689.5, bsz=141, num_updates=14900, lr=0.000259064, gnorm=0, train_wall=12, gb_free=76.9, wall=2366
2023-05-09 04:51:24 | INFO | train_inner | epoch 016:    210 / 986 loss=4.208, nll_loss=2.744, intra_distillation_loss=0, ppl=6.7, wps=29061.7, ups=7.9, wpb=3676.4, bsz=131, num_updates=15000, lr=0.000258199, gnorm=0, train_wall=12, gb_free=77.1, wall=2378
2023-05-09 04:51:37 | INFO | train_inner | epoch 016:    310 / 986 loss=4.183, nll_loss=2.716, intra_distillation_loss=0, ppl=6.57, wps=28853.3, ups=7.81, wpb=3695.9, bsz=145.9, num_updates=15100, lr=0.000257343, gnorm=0, train_wall=12, gb_free=76.9, wall=2391
2023-05-09 04:51:49 | INFO | train_inner | epoch 016:    410 / 986 loss=4.102, nll_loss=2.626, intra_distillation_loss=0, ppl=6.17, wps=28815, ups=7.81, wpb=3688.3, bsz=169.1, num_updates=15200, lr=0.000256495, gnorm=0, train_wall=12, gb_free=77, wall=2404
2023-05-09 04:52:02 | INFO | train_inner | epoch 016:    510 / 986 loss=4.214, nll_loss=2.751, intra_distillation_loss=0, ppl=6.73, wps=28541.5, ups=7.89, wpb=3617.2, bsz=137.1, num_updates=15300, lr=0.000255655, gnorm=0, train_wall=12, gb_free=76.8, wall=2417
2023-05-09 04:52:15 | INFO | train_inner | epoch 016:    610 / 986 loss=4.164, nll_loss=2.695, intra_distillation_loss=0, ppl=6.48, wps=28857, ups=7.91, wpb=3646.6, bsz=151.9, num_updates=15400, lr=0.000254824, gnorm=0, train_wall=12, gb_free=76.9, wall=2429
2023-05-09 04:52:27 | INFO | train_inner | epoch 016:    710 / 986 loss=4.235, nll_loss=2.776, intra_distillation_loss=0, ppl=6.85, wps=28880.5, ups=7.83, wpb=3686.8, bsz=133.8, num_updates=15500, lr=0.000254, gnorm=0, train_wall=12, gb_free=76.8, wall=2442
2023-05-09 04:52:40 | INFO | train_inner | epoch 016:    810 / 986 loss=4.196, nll_loss=2.732, intra_distillation_loss=0, ppl=6.64, wps=28583.7, ups=7.86, wpb=3634.9, bsz=156.4, num_updates=15600, lr=0.000253185, gnorm=0, train_wall=12, gb_free=76.8, wall=2455
2023-05-09 04:52:53 | INFO | train_inner | epoch 016:    910 / 986 loss=4.24, nll_loss=2.782, intra_distillation_loss=0, ppl=6.88, wps=28671.1, ups=7.84, wpb=3658.1, bsz=143, num_updates=15700, lr=0.000252377, gnorm=0, train_wall=12, gb_free=76.9, wall=2467
2023-05-09 04:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:53:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:53:41 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.242 | nll_loss 2.643 | intra_distillation_loss 0 | ppl 6.25 | bleu 32.27 | wps 4272.2 | wpb 2821.2 | bsz 113.1 | num_updates 15776 | best_bleu 32.27
2023-05-09 04:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 15776 updates
2023-05-09 04:53:41 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:53:42 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 16 @ 15776 updates, score 32.27) (writing took 1.9680284858914092 seconds)
2023-05-09 04:53:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-09 04:53:43 | INFO | train | epoch 016 | loss 4.187 | nll_loss 2.722 | intra_distillation_loss 0 | ppl 6.6 | wps 21767.3 | ups 5.93 | wpb 3668.2 | bsz 146.4 | num_updates 15776 | lr 0.000251769 | gnorm 0 | train_wall 115 | gb_free 76.9 | wall 2518
2023-05-09 04:53:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:53:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:53:43 | INFO | fairseq.trainer | begin training epoch 17
2023-05-09 04:53:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:53:46 | INFO | train_inner | epoch 017:     24 / 986 loss=4.144, nll_loss=2.674, intra_distillation_loss=0, ppl=6.38, wps=6966.2, ups=1.87, wpb=3716.1, bsz=157.9, num_updates=15800, lr=0.000251577, gnorm=0, train_wall=12, gb_free=76.9, wall=2521
2023-05-09 04:53:59 | INFO | train_inner | epoch 017:    124 / 986 loss=4.095, nll_loss=2.617, intra_distillation_loss=0, ppl=6.14, wps=28315.8, ups=7.86, wpb=3602.2, bsz=160.2, num_updates=15900, lr=0.000250785, gnorm=0, train_wall=12, gb_free=76.9, wall=2534
2023-05-09 04:54:12 | INFO | train_inner | epoch 017:    224 / 986 loss=4.07, nll_loss=2.588, intra_distillation_loss=0, ppl=6.01, wps=28867.7, ups=7.78, wpb=3709.6, bsz=160.3, num_updates=16000, lr=0.00025, gnorm=0, train_wall=12, gb_free=76.8, wall=2546
2023-05-09 04:54:25 | INFO | train_inner | epoch 017:    324 / 986 loss=4.125, nll_loss=2.651, intra_distillation_loss=0, ppl=6.28, wps=28573.9, ups=7.85, wpb=3640.5, bsz=146.9, num_updates=16100, lr=0.000249222, gnorm=0, train_wall=12, gb_free=76.8, wall=2559
2023-05-09 04:54:37 | INFO | train_inner | epoch 017:    424 / 986 loss=4.153, nll_loss=2.682, intra_distillation_loss=0, ppl=6.42, wps=28961.6, ups=7.84, wpb=3692.6, bsz=140.1, num_updates=16200, lr=0.000248452, gnorm=0, train_wall=12, gb_free=76.9, wall=2572
2023-05-09 04:54:50 | INFO | train_inner | epoch 017:    524 / 986 loss=4.187, nll_loss=2.721, intra_distillation_loss=0, ppl=6.59, wps=29464.6, ups=7.83, wpb=3761.7, bsz=141.1, num_updates=16300, lr=0.000247689, gnorm=0, train_wall=12, gb_free=76.9, wall=2585
2023-05-09 04:55:03 | INFO | train_inner | epoch 017:    624 / 986 loss=4.194, nll_loss=2.729, intra_distillation_loss=0, ppl=6.63, wps=28563.3, ups=7.92, wpb=3604.4, bsz=137.7, num_updates=16400, lr=0.000246932, gnorm=0, train_wall=12, gb_free=76.8, wall=2597
2023-05-09 04:55:16 | INFO | train_inner | epoch 017:    724 / 986 loss=4.139, nll_loss=2.668, intra_distillation_loss=0, ppl=6.35, wps=28448.1, ups=7.74, wpb=3676.2, bsz=160.9, num_updates=16500, lr=0.000246183, gnorm=0, train_wall=12, gb_free=76.8, wall=2610
2023-05-09 04:55:28 | INFO | train_inner | epoch 017:    824 / 986 loss=4.173, nll_loss=2.705, intra_distillation_loss=0, ppl=6.52, wps=28504.2, ups=7.84, wpb=3634.7, bsz=145.4, num_updates=16600, lr=0.00024544, gnorm=0, train_wall=12, gb_free=77.1, wall=2623
2023-05-09 04:55:41 | INFO | train_inner | epoch 017:    924 / 986 loss=4.223, nll_loss=2.761, intra_distillation_loss=0, ppl=6.78, wps=29281.9, ups=7.98, wpb=3671.7, bsz=136.7, num_updates=16700, lr=0.000244704, gnorm=0, train_wall=12, gb_free=76.8, wall=2636
2023-05-09 04:55:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:55:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:56:27 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.232 | nll_loss 2.626 | intra_distillation_loss 0 | ppl 6.17 | bleu 32.5 | wps 4255.4 | wpb 2821.2 | bsz 113.1 | num_updates 16762 | best_bleu 32.5
2023-05-09 04:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 16762 updates
2023-05-09 04:56:27 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 17 @ 16762 updates, score 32.5) (writing took 1.9644300689687952 seconds)
2023-05-09 04:56:29 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-09 04:56:29 | INFO | train | epoch 017 | loss 4.154 | nll_loss 2.684 | intra_distillation_loss 0 | ppl 6.43 | wps 21749.3 | ups 5.93 | wpb 3668.2 | bsz 146.4 | num_updates 16762 | lr 0.000244251 | gnorm 0 | train_wall 115 | gb_free 76.9 | wall 2684
2023-05-09 04:56:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:56:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:56:29 | INFO | fairseq.trainer | begin training epoch 18
2023-05-09 04:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:56:34 | INFO | train_inner | epoch 018:     38 / 986 loss=4.214, nll_loss=2.752, intra_distillation_loss=0, ppl=6.73, wps=6823.5, ups=1.87, wpb=3642.1, bsz=126.8, num_updates=16800, lr=0.000243975, gnorm=0, train_wall=12, gb_free=77, wall=2689
2023-05-09 04:56:47 | INFO | train_inner | epoch 018:    138 / 986 loss=4.124, nll_loss=2.649, intra_distillation_loss=0, ppl=6.27, wps=28654.3, ups=7.83, wpb=3657.9, bsz=135.8, num_updates=16900, lr=0.000243252, gnorm=0, train_wall=12, gb_free=76.9, wall=2702
2023-05-09 04:57:00 | INFO | train_inner | epoch 018:    238 / 986 loss=4.163, nll_loss=2.693, intra_distillation_loss=0, ppl=6.46, wps=29020.7, ups=7.96, wpb=3646.9, bsz=132.8, num_updates=17000, lr=0.000242536, gnorm=0, train_wall=12, gb_free=76.9, wall=2714
2023-05-09 04:57:12 | INFO | train_inner | epoch 018:    338 / 986 loss=4.131, nll_loss=2.657, intra_distillation_loss=0, ppl=6.31, wps=28873.7, ups=7.86, wpb=3673.7, bsz=143.5, num_updates=17100, lr=0.000241825, gnorm=0, train_wall=12, gb_free=76.8, wall=2727
2023-05-09 04:57:25 | INFO | train_inner | epoch 018:    438 / 986 loss=4.027, nll_loss=2.54, intra_distillation_loss=0, ppl=5.82, wps=28983.3, ups=7.73, wpb=3750, bsz=171.4, num_updates=17200, lr=0.000241121, gnorm=0, train_wall=12, gb_free=76.8, wall=2740
2023-05-09 04:57:38 | INFO | train_inner | epoch 018:    538 / 986 loss=4.144, nll_loss=2.672, intra_distillation_loss=0, ppl=6.37, wps=28639.4, ups=7.83, wpb=3657.4, bsz=148, num_updates=17300, lr=0.000240424, gnorm=0, train_wall=12, gb_free=77.1, wall=2753
2023-05-09 04:57:51 | INFO | train_inner | epoch 018:    638 / 986 loss=4.074, nll_loss=2.593, intra_distillation_loss=0, ppl=6.03, wps=28699.6, ups=7.76, wpb=3696.7, bsz=165.7, num_updates=17400, lr=0.000239732, gnorm=0, train_wall=12, gb_free=77, wall=2766
2023-05-09 04:58:04 | INFO | train_inner | epoch 018:    738 / 986 loss=4.147, nll_loss=2.675, intra_distillation_loss=0, ppl=6.39, wps=28944.5, ups=7.9, wpb=3664.1, bsz=138.6, num_updates=17500, lr=0.000239046, gnorm=0, train_wall=12, gb_free=77.1, wall=2778
2023-05-09 04:58:16 | INFO | train_inner | epoch 018:    838 / 986 loss=4.163, nll_loss=2.694, intra_distillation_loss=0, ppl=6.47, wps=28184, ups=7.92, wpb=3559.6, bsz=147.4, num_updates=17600, lr=0.000238366, gnorm=0, train_wall=12, gb_free=77.1, wall=2791
2023-05-09 04:58:29 | INFO | train_inner | epoch 018:    938 / 986 loss=4.138, nll_loss=2.665, intra_distillation_loss=0, ppl=6.34, wps=28929.5, ups=7.85, wpb=3686, bsz=141.6, num_updates=17700, lr=0.000237691, gnorm=0, train_wall=12, gb_free=77, wall=2804
2023-05-09 04:58:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 04:58:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:59:13 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.208 | nll_loss 2.603 | intra_distillation_loss 0 | ppl 6.08 | bleu 32.56 | wps 4318 | wpb 2821.2 | bsz 113.1 | num_updates 17748 | best_bleu 32.56
2023-05-09 04:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 17748 updates
2023-05-09 04:59:13 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:59:14 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 04:59:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 18 @ 17748 updates, score 32.56) (writing took 1.7568947770632803 seconds)
2023-05-09 04:59:15 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-09 04:59:15 | INFO | train | epoch 018 | loss 4.126 | nll_loss 2.651 | intra_distillation_loss 0 | ppl 6.28 | wps 21852.7 | ups 5.96 | wpb 3668.2 | bsz 146.4 | num_updates 17748 | lr 0.00023737 | gnorm 0 | train_wall 115 | gb_free 76.8 | wall 2849
2023-05-09 04:59:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 04:59:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 04:59:15 | INFO | fairseq.trainer | begin training epoch 19
2023-05-09 04:59:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 04:59:22 | INFO | train_inner | epoch 019:     52 / 986 loss=4.081, nll_loss=2.601, intra_distillation_loss=0, ppl=6.07, wps=7132.1, ups=1.9, wpb=3754.1, bsz=149.1, num_updates=17800, lr=0.000237023, gnorm=0, train_wall=12, gb_free=76.9, wall=2856
2023-05-09 04:59:34 | INFO | train_inner | epoch 019:    152 / 986 loss=4.068, nll_loss=2.586, intra_distillation_loss=0, ppl=6, wps=28494.5, ups=7.8, wpb=3655.4, bsz=148.6, num_updates=17900, lr=0.00023636, gnorm=0, train_wall=12, gb_free=76.9, wall=2869
2023-05-09 04:59:47 | INFO | train_inner | epoch 019:    252 / 986 loss=4.055, nll_loss=2.57, intra_distillation_loss=0, ppl=5.94, wps=29114, ups=7.78, wpb=3743.2, bsz=150, num_updates=18000, lr=0.000235702, gnorm=0, train_wall=12, gb_free=76.9, wall=2882
2023-05-09 05:00:00 | INFO | train_inner | epoch 019:    352 / 986 loss=4.02, nll_loss=2.532, intra_distillation_loss=0, ppl=5.78, wps=28727.5, ups=7.86, wpb=3654.5, bsz=160.6, num_updates=18100, lr=0.00023505, gnorm=0, train_wall=12, gb_free=76.8, wall=2895
2023-05-09 05:00:13 | INFO | train_inner | epoch 019:    452 / 986 loss=4.076, nll_loss=2.595, intra_distillation_loss=0, ppl=6.04, wps=28613.5, ups=7.85, wpb=3645.7, bsz=150.6, num_updates=18200, lr=0.000234404, gnorm=0, train_wall=12, gb_free=76.9, wall=2907
2023-05-09 05:00:26 | INFO | train_inner | epoch 019:    552 / 986 loss=4.153, nll_loss=2.682, intra_distillation_loss=0, ppl=6.42, wps=28520.4, ups=7.81, wpb=3652.6, bsz=137.4, num_updates=18300, lr=0.000233762, gnorm=0, train_wall=12, gb_free=76.7, wall=2920
2023-05-09 05:00:38 | INFO | train_inner | epoch 019:    652 / 986 loss=4.173, nll_loss=2.704, intra_distillation_loss=0, ppl=6.52, wps=28102.4, ups=8.03, wpb=3499.8, bsz=130.4, num_updates=18400, lr=0.000233126, gnorm=0, train_wall=11, gb_free=77.1, wall=2933
2023-05-09 05:00:51 | INFO | train_inner | epoch 019:    752 / 986 loss=4.065, nll_loss=2.583, intra_distillation_loss=0, ppl=5.99, wps=29017.1, ups=7.72, wpb=3756.3, bsz=162.2, num_updates=18500, lr=0.000232495, gnorm=0, train_wall=12, gb_free=77, wall=2946
2023-05-09 05:01:04 | INFO | train_inner | epoch 019:    852 / 986 loss=4.069, nll_loss=2.588, intra_distillation_loss=0, ppl=6.01, wps=29031.6, ups=7.76, wpb=3740.5, bsz=159, num_updates=18600, lr=0.000231869, gnorm=0, train_wall=12, gb_free=76.9, wall=2958
2023-05-09 05:01:16 | INFO | train_inner | epoch 019:    952 / 986 loss=4.199, nll_loss=2.735, intra_distillation_loss=0, ppl=6.66, wps=28872, ups=7.95, wpb=3630.6, bsz=121.8, num_updates=18700, lr=0.000231249, gnorm=0, train_wall=12, gb_free=76.8, wall=2971
2023-05-09 05:01:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 05:01:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:01:59 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.187 | nll_loss 2.578 | intra_distillation_loss 0 | ppl 5.97 | bleu 32.98 | wps 4236.6 | wpb 2821.2 | bsz 113.1 | num_updates 18734 | best_bleu 32.98
2023-05-09 05:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 18734 updates
2023-05-09 05:01:59 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 05:02:00 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt
2023-05-09 05:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_best.pt (epoch 19 @ 18734 updates, score 32.98) (writing took 1.8323057340458035 seconds)
2023-05-09 05:02:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-09 05:02:01 | INFO | train | epoch 019 | loss 4.097 | nll_loss 2.618 | intra_distillation_loss 0 | ppl 6.14 | wps 21736.1 | ups 5.93 | wpb 3668.2 | bsz 146.4 | num_updates 18734 | lr 0.000231039 | gnorm 0 | train_wall 115 | gb_free 76.8 | wall 3016
2023-05-09 05:02:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:02:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 986
2023-05-09 05:02:01 | INFO | fairseq.trainer | begin training epoch 20
2023-05-09 05:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-09 05:02:10 | INFO | train_inner | epoch 020:     66 / 986 loss=4.092, nll_loss=2.612, intra_distillation_loss=0, ppl=6.11, wps=6935.3, ups=1.87, wpb=3700.6, bsz=135.7, num_updates=18800, lr=0.000230633, gnorm=0, train_wall=12, gb_free=76.9, wall=3024
2023-05-09 05:02:23 | INFO | train_inner | epoch 020:    166 / 986 loss=4.056, nll_loss=2.572, intra_distillation_loss=0, ppl=5.94, wps=28419, ups=7.83, wpb=3627.6, bsz=144.1, num_updates=18900, lr=0.000230022, gnorm=0, train_wall=12, gb_free=76.7, wall=3037
2023-05-09 05:02:35 | INFO | train_inner | epoch 020:    266 / 986 loss=4.039, nll_loss=2.552, intra_distillation_loss=0, ppl=5.86, wps=28907.9, ups=7.9, wpb=3657.9, bsz=147, num_updates=19000, lr=0.000229416, gnorm=0, train_wall=12, gb_free=76.9, wall=3050
2023-05-09 05:02:48 | INFO | train_inner | epoch 020:    366 / 986 loss=4.033, nll_loss=2.546, intra_distillation_loss=0, ppl=5.84, wps=28592.1, ups=7.79, wpb=3669.8, bsz=163.3, num_updates=19100, lr=0.000228814, gnorm=0, train_wall=12, gb_free=76.8, wall=3063
2023-05-09 05:03:01 | INFO | train_inner | epoch 020:    466 / 986 loss=4.085, nll_loss=2.604, intra_distillation_loss=0, ppl=6.08, wps=28729.1, ups=7.81, wpb=3678.2, bsz=141.8, num_updates=19200, lr=0.000228218, gnorm=0, train_wall=12, gb_free=76.8, wall=3075
2023-05-09 05:03:14 | INFO | train_inner | epoch 020:    566 / 986 loss=4.031, nll_loss=2.544, intra_distillation_loss=0, ppl=5.83, wps=28872.6, ups=7.81, wpb=3696.3, bsz=155.7, num_updates=19300, lr=0.000227626, gnorm=0, train_wall=12, gb_free=76.8, wall=3088
2023-05-09 05:03:26 | INFO | train_inner | epoch 020:    666 / 986 loss=4.111, nll_loss=2.635, intra_distillation_loss=0, ppl=6.21, wps=29096.2, ups=7.93, wpb=3668.6, bsz=134.4, num_updates=19400, lr=0.000227038, gnorm=0, train_wall=12, gb_free=76.7, wall=3101
2023-05-09 05:03:39 | INFO | train_inner | epoch 020:    766 / 986 loss=4.08, nll_loss=2.598, intra_distillation_loss=0, ppl=6.06, wps=29238.9, ups=7.89, wpb=3706.8, bsz=144.2, num_updates=19500, lr=0.000226455, gnorm=0, train_wall=12, gb_free=76.8, wall=3114
2023-05-09 05:03:52 | INFO | train_inner | epoch 020:    866 / 986 loss=4.138, nll_loss=2.666, intra_distillation_loss=0, ppl=6.35, wps=28124.6, ups=7.85, wpb=3583.6, bsz=141.1, num_updates=19600, lr=0.000225877, gnorm=0, train_wall=12, gb_free=77, wall=3126
2023-05-09 05:04:04 | INFO | train_inner | epoch 020:    966 / 986 loss=4.068, nll_loss=2.587, intra_distillation_loss=0, ppl=6.01, wps=28744.8, ups=7.79, wpb=3691, bsz=153.9, num_updates=19700, lr=0.000225303, gnorm=0, train_wall=12, gb_free=77, wall=3139
2023-05-09 05:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-09 05:04:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:04:46 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.179 | nll_loss 2.565 | intra_distillation_loss 0 | ppl 5.92 | bleu 32.81 | wps 4247.1 | wpb 2821.2 | bsz 113.1 | num_updates 19720 | best_bleu 32.98
2023-05-09 05:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 19720 updates
2023-05-09 05:04:46 | INFO | fairseq.trainer | Saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_last.pt
2023-05-09 05:04:47 | INFO | fairseq.trainer | Finished saving checkpoint to /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_last.pt
2023-05-09 05:04:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /scratch4/cs601/tli104/wf_8000/checkpoints_he/checkpoint_last.pt (epoch 20 @ 19720 updates, score 32.81) (writing took 0.9405343399848789 seconds)
2023-05-09 05:04:47 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-09 05:04:47 | INFO | train | epoch 020 | loss 4.07 | nll_loss 2.588 | intra_distillation_loss 0 | ppl 6.01 | wps 21862.5 | ups 5.96 | wpb 3668.2 | bsz 146.4 | num_updates 19720 | lr 0.000225189 | gnorm 0 | train_wall 115 | gb_free 77.1 | wall 3181
2023-05-09 05:04:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-09 05:04:47 | INFO | fairseq_cli.train | done training in 3179.7 seconds
/home/tli104/my_fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
